# Summarizing Data

Summarizing data is what it sounds like. You're reducing the number of rows in your dataset based on some predetermined method. Taking the average of a group of numbers is summarizing the data. Many numbers have been condensed to one: the average. In this chapter we'll go over summarizing data, and some aesthetic changes we can make for publication ready tables. 

## Data to play with

We'll take a look at the 2018 hourly mean NO~2~ concentrations for the Atlantic provinces (New Brunswick, Prince Edward Island, Nova Scotia, and Newfoundland). The dataset is available in the [*R4EnvChem* Project Template](https://github.com/DavidRossHall/R4EnvChem-ProjectTemplate) repository. Also if you're keen, you can download any number of atmospheric datasets from *Environment and Climate Change Canada*'s (ECCC) National Airborne Pollution Program's (NAPS) website [here](https://data.ec.gc.ca/data/air/monitor/national-air-pollution-surveillance-naps-program/Data-Donnees/?lang=en) 

Since ECCC stores their NAPS data in a matrix layout, we need to briefly tidy it up:

```{r, message = FALSE}

atlNO2 <- read_csv("data/2018hourlyNO2_Atl.csv", skip = 7, na =c("-999")) %>%
  rename_with(~tolower(gsub("/.*", "", .x))) %>%
  pivot_longer(cols = starts_with("h"), 
               names_prefix = "h", 
               names_to = "hour", 
               names_transform = list(hour = as.numeric),
               values_to = "conc", 
               values_transform = list(conc = as.numeric),
               values_drop_na = TRUE) 

# First 50 rows of dataset
DT::datatable(atlNO2[1:50, ])
```

Note in our dataset that both Halifax NS and Saint John NB have three NAPS stations each. It won't matter for our aggregation, but if we were exploring this data in more depth this is something we would want to take into account. 


```{r, echo = FALSE, message = FALSE, warning = FALSE}
naps <- atlNO2 %>%
  group_by(p, city) %>%
  summarise(napsID = n_distinct(`naps id`))


ggplot(data = naps, 
       aes(x=fct_reorder(city, p), 
           y = napsID,
           colour = p)) +
  geom_segment( aes(x=fct_reorder(city, p), 
                    xend=fct_reorder(city, p),
                    y=0, yend=napsID,
                    colour = p)) +
  geom_point( aes(color = p), size=4, alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme_classic() + 
  labs(x = "Cities in dataset", 
       y = "Number of NAPS stations")
```


## Summarizing data by group

While we can readily summarize an entire dataset, we often want to summarize *groups* within our dataset. In our case, it's [NO~2~] in each city. To this end, we need to combine the`group_by()` and `summarize()` functions. `summarise()` also works for the Americans. This approach allows us to (1) specify which groups we want summarized, and (2) how we want them summarized. We'll talk more about point (2) later on, for now, let's look at point (1)

Let's calculate the mean hourly NO~2~ concentrations in the `r length(unique(atlNO2$p))` provinces in our dataset:

```{r, message = FALSE}

sumAtl <- atlNO2 %>%
  group_by(p) %>%
  summarize(mean = mean(conc))

sumAtl

```

That's it. `r length(atlNO2$conc)` unique rows summarized like that. Note that `summarize` produces a *new* data frame, so you'll want to double check on the outputted data types. Let's break down what our code does: 

  - We're creating a new data frame, so we store it in `sumAtl`.
  - We then take our `atlNO2` dataset and  group our dataset by province using `group_by(p)`.
  - We then summarize our grouped data by summarizing the NO~2~ concentration with `summarize(mean = mean(conc)`. 
    - Note that since we're creating a new data set, we need to create new columns. This is what `mean = mean(conc)` does. We're creating a column *called mean*, which contains the *numerical mean* 1-hr NO~2~ values which were calculated using the `mean()` function. Simple...
    
    
Let's dig a little deeper. The [*Canadian Ambient Air Quality Standards*](https://ccme.ca/en/air-quality-report) stipulates that the annual mean of 1-hour means for NO~2~ cannot exceed 17.0 ppb in 2020, and 12.0 ppb in 2025. Let's see if any city in our dataset violated these standards in 2018. 

To do this, we'll group by province (`p`) and city (`city`). This will retain our provinces column that we might want to use later on. 

```{r, message = FALSE}
sumAtl <- atlNO2 %>%
  group_by(p, city) %>%
  summarize(mean = mean(conc))

sumAtl
```

Looks like there aren't any offenders. For tips on visualizing these results please see the [Visualizations] chapter.



### Further summarize operations

There are other options we can use to summarize out data. A handy list is provided on the [`summarize()` help page](https://dplyr.tidyverse.org/reference/summarise.html). The most common ones you'll need are:

  - `mean()` which calculates the arithmetic mean, a.k.a. the average. 
  - `median()` which calculates the sample median, the value separating the higher 50% of data from the lower 50% of a data sample.
  - `sd()` which calculates the sample standard deviation. 
  - `min()` and `max()` which returns the smallest and largest value in the dataset. 
  - `n()` which provides the number of entries in a group. Note you don't specify a variable for n. 

Let's seem them in action: 

```{r, message = FALSE}
sumAtl <- atlNO2 %>%
  group_by(p, city) %>%
  summarize(mean = mean(conc),
            sd = sd(conc),
            min = min(conc), 
            max = max(conc), 
            n = n())

sumAtl
```

Note that the functions we pass to summarize adhere to rules of missing values. That is to say, if even one value in a group is an `NA`, the entire group defaults to `NA`. Consequently, if your confident this isn't an issue, you can pass the argument `na.rm = TRUE` to any of the summarize functions, which would look like `mean = mean(conc, na.rm = TRUE)`. This will ignore any `NA` values and return a numeric value like you probably expect. 


## Pretty tables with flextable

While the summarize function does an excellent job of summarizing our data, the outputted dataset isn't really fit for publication. This is doubly so if you used summarize as the last step of your chemical quantification and you want a nice and pretty table of mean sample concentration with standard deviations. 

To this end we'll use the 'flextable' package. Please refer to [flextable R package](https://davidgohel.github.io/flextable/index.html). There are other packages to make tables, but we're using `flextable` as it's consistent between HTML and PDF outputs. 

```{r, message = FALSE, warning = FALSE}
library(flextable)

flextable(sumAtl)
```

Perhaps that isn't pretty enough for you. Doubtlessly your instructor will tell you to combine the mean and standard deviation into one value (i.e. $mean \pm sd$). We'll do this in two steps. 
  
  - Step 1: Use `unite()` to merge the `mean` and `sd` values together rowise; values will be separated by ±. 
    - ± is a legit symbol, try `Alt+241`or copy and paste it from this book. 
  - Step 2: Pretty up our table to significant digits, and perform some aesthetic changes. 


### Uniting columns 

Firstly, our `mean` and `sd` columns contain way too many decimal places. We'll need to round them down before we use `unite()` to paste together the two columns into one. During our `unite()` call, we'll use `sep = " ± "` to separate the `mean` from `sd` values (otherwise they'd be pasted as one long number). 

```{r}

prettySumAtl <- sumAtl %>%
  mutate(mean = sprintf("%.1f", mean), 
         sd = sprintf("%.1f", sd)) %>%
  unite("mean ± sd", mean:sd, sep = " ± " ) %>%
  select(-n) # removing n column

prettySumAtl
```

Note to round the numbers we used `sprintf()` to round out numbers. This is because in the final publication it's important to keep trailing zeros (i.e. `1.0` and not `1`), but R's `round()` will drop these. `mean = sprintf("%.1f", mean)` takes the existing values in the `mean` column, rounds them to one digit, that's what `"%.1f"` means ("%.2f" would be two digits and so on), and paste them back into the `mean` column. Same situation for the `sd` column. 


### Pretty tables 

Now we'll want to make a pretty table. Despite the emphasis on visualizations in this book, tables are an under appreciate means to convey information. Often when you're only plotting a handful of numbers, a table would better serve the reader. So don't overlook this point of your report. If you've distilled hours of your work to a handful of numbers, you best serve them up on a silver platter. 


```{r}


ft <- flextable(prettySumAtl) 
  
ft <- set_header_labels(ft, 
                        p = "province")

ft <- set_table_properties(ft, layout = "autofit")
ft <- align(ft, j = "mean ± sd", align = "right", part = "all")
ft
```


```{r child='end-of-chapter-exercise-temp.Rmd'}
```


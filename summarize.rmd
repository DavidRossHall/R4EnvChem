# Summarizing data

In the `tidyverse`, summarizing your data means to reduce it 

## Data to play with

We'll take a look at the 2018 hourly mean NO~2~ concentrations for the Atlantic provinces (New Brunswick, Prince Edward Island, Nova Scotia, and Newfoundlan). The dataset is available in the [*R4EnvChem* Project Template](https://github.com/DavidRossHall/R4EnvChem-ProjectTemplate) repository. Also if you're keen you can download any number of atmospheric datasets from *Environment and Climate Change Canada*'s (ECCC) National Airborn Pollution Program's (NAPS) website [here](https://data.ec.gc.ca/data/air/monitor/national-air-pollution-surveillance-naps-program/Data-Donnees/?lang=en) 

Since ECCC stores their NAPS data in a matrix layout, we need to briefly tidy it up:

```{r}
library(tidyverse)

atlNO2 <- read_csv("data/2018hourlyNO2_Atl.csv", skip = 7, na =c("-999")) %>%
  rename_with(~tolower(gsub("/.*", "", .x))) %>%
  pivot_longer(cols = starts_with("h"), 
               names_prefix = "h", 
               names_to = "hour", 
               names_transform = list(hour = as.numeric),
               values_to = "conc", 
               values_transform = list(conc = as.numeric),
               values_drop_na = TRUE) 

# First 50 rows of dataset
DT::datatable(atlNO2[1:50, ])
```

Note in our dataset that both Halifax NS and Saint John NB have three NAPS stations each. It won't matter for our aggregation, but if we were xploring this data in more depth this is something we would want to take into account. 


```{r, echo = FALSE, message = FALSE, warning = FALSE}
naps <- atlNO2 %>%
  group_by(p, city) %>%
  summarise(napsID = n_distinct(`naps id`))


ggplot(data = naps, 
       aes(x=fct_reorder(city, p), 
           y = napsID,
           colour = p)) +
  geom_segment( aes(x=fct_reorder(city, p), 
                    xend=fct_reorder(city, p),
                    y=0, yend=napsID,
                    colour = p)) +
  geom_point( aes(color = p), size=4, alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme_classic() + 
  labs(x = "Cities in dataset", 
       y = "Number of NAPS stations")
```


## Summarizing data by group

We need to combine the`group_by()` and `summarize()` functions. `summarise()` also works... What this does is (1) allows us to specify which groups we want summarize, and (2) how we want it summarized. We'll talk more about point (2) later on, for now, let's look at point (1)

Let's look at an example where we calculate the mean hourly NO~2~ concentrations in the `r length(unique(atlNO2$p))` provinces in our dataset:

```{r, message = FALSE}

sumAtl <- atlNO2 %>%
  group_by(p) %>%
  summarize(mean = mean(conc))

sumAtl

```

That's it. `r length(atlNO2$conc)` unique rows summarized like that. Note that `summarize` produces a *new* data frame, so you'll want to double check on the outputted data types. Let's break down what our code does: 

  - We're creating a new dataframe, so we're going to store it in `sumAtl`.
  - We then take out `atlNO2` dataset and  we group our datasets by province (`group_by(p)`)
  - We then summarize our groupped data by summarizing the NO~2~ concentration with `sumaraize(mean = mean(conc)). 
    - Note that since we're creating a new data set, we need to create new columns. This is what `mean = mean(conc)` does. We're creating a column *called mean*, which contains the *numerical mean* 1-hr NO~2~ values which were calculated using the `mean()` function. Simple...
    
    
Let's dig a little deeper. The [*Canadian Ambient Air Quality Standards*](https://ccme.ca/en/air-quality-report) stipulate that the annual mean of 1-hour means for NO~2~ cannot exceed 17.0 ppb in 2020, and 12.0 ppb in 2025. Let's see if any city in our dataset violated these standards in 2018. 

To do this, we'll group by province (`p`) and city (`city`). This will retain our provinces column that we might want to use later on. 

```{r, message = FALSE}
sumAtl <- atlNO2 %>%
  group_by(p, city) %>%
  summarize(mean = mean(conc))

sumAtl
```


### Further sumarize operations

There are other options we can use to sumamrize out data. A handy list is proviced in the [summarize help page](https://dplyr.tidyverse.org/reference/summarise.html). The most common ones you'll need are :

  - `mean()` which calcualted the arithmetic mean, a.k.a. the average. 
  - `median()` which calcualted the sample media, the value separating the higher half from the lower half of a data sample.
  - `sd()` which calcualted the sample standard deviation. 
  - `min()` and `max()` which returnt he smallest and largest value in the dataset. 
  - `n()` which provides the number of entires in a group. Note you don't specify a variable for n. 

Let's seem them in action: 

```{r, message = FALSE}
sumAtl <- atlNO2 %>%
  group_by(p, city) %>%
  summarize(mean = mean(conc),
            sd = sd(conc),
            min = min(conc), 
            max = max(conc), 
            n = n())

sumAtl
```

Note that the functions we pass to summarize adhere to rules of missing values. That is to say, if even one value in a group is an `NA`, the entire group defaults to `NA`. Consequently, if your confident this isn't an issue, you can pass the argument `na.rm = TRUE` to any of the summarize functions, which would look like `mean = mean(conc, na.rm = TRUE)`. This will ignore any `NA` values and return a numeric value like you probably expect. 


## Pretty tables with kable

While the summarize function does an excellent job of summarizing our data, the outputted dataset isn't really fit for publication. This is double so if you used summarize as the last step of your chemuical quantification and you want a nice and pretty table of means sample concentration with standard deviations. 

To this end we'll use the 'flextable' pacakage. Please refer to [flextable R package](https://davidgohel.github.io/flextable/index.html). There are other packages to make tables, but `flextable` is consistent between HTML and PDF outputs. 

```{r, message = FALSE}
library(flextable)

flextable(sumAtl)
```

Perhaps that isn't pretty enough for you. Doubtlessly your instructor will tell you to combine the mean and sd into one value (i.e. $ mean \pm sd$). We'll do this in two steps. 
  
  - Step 1: Use `unite()` to merge the `mean` and `sd` values together rowise; values will be separated by ±. 
    - ± is a legit symbol, try `Alt+241`or copy and paste it from this book. 
  - Step 2: Pretty up our kable to significant digits, and perform some aesthetic changes. 


### Uniting columns 

Firstly, our `mean` and `sd` columns contain way too many decimal places. We'll need to round them down before we use `unite()` to paste together the two columns into one. During our `unite()` call, we'll use `sep = " ± "` to seprate the `mean` from `sd` values (otherwise they'd be pasted as one long number). 

```{r}

prettySumAtl <- sumAtl %>%
  mutate(mean = sprintf("%.1f", mean), 
         sd = sprintf("%.1f", sd)) %>%
  unite("mean ± sd", mean:sd, sep = " ± " ) %>%
  select(-n) # removing n column

prettySumAtl
```

Note to round the numbers we used `sprintf()` to round out numbers. This is because in the final publication it's important to keep trailing zeros (i.e. `1.0` and not `1`), but R's `round()` will drop these. `mean = sprintf("%.1f", mean)` takes the existing values in the `mean` column, rounds thems to one difit, that's what `"%.1f"` means ("%.2f" would be two digits and so on), and paste them back into the `mean` column. Same situation for the `sd` column. 


### Pretty kables 

Now we'll want to make a pretty table. Despite this being a book on visualziations in R, tables are often underappreciate means to convey information. Often when you're only plotting a handful of numbers, a table would better serve the reader. So don't overlook this point of your report. If you've distilled hours of your work to a handful of numbers, you best serve them up on a silver plater. 


```{r}


ft <- flextable(prettySumAtl) 
  
ft <- set_header_labels(ft, 
                        p = "province")

ft <- set_table_properties(ft, layout = "autofit")
ft <- align(ft, j = "mean ± sd", align = "right")
ft
```




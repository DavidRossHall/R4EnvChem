# Transform: Data Manipulation

Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. We've already touched upon some useful transformation functions in previous example code snippets, such as the `mutate` function for adding columns. This section will explore some of the most useful functionalities of the `dplyr` package, explicitly introduce the pipe operator `%>%`, and showcase how you can leverage these tools to quickly manipulate your data.

The essential `dplyr` functions are :

  - `mutate()` to create new columns/variables from existing data
  - `arrange()` to reorder rows 
  - `filter()` to refine observations by their values (in other words by row)
  - `select()` to pick variables by name (in other words by column)
  - `summarize()` to collapse many values down to a single summary. 
  
We'll go through each of these functions, for more details you can read [Chapter 3: Data Transformation](https://r4ds.had.co.nz/transform.html) from *R for Data Science* which provides a more comprehensive breakdown of these functions. Note that the information here is based on a `tidyverse` approach, but this is only one way of doing things. See the [Further reading](#further_reading_chapter12) section for links to other suitable approaches to data transformation. 

Let's explore the functionality of `dplyr` using some flame absorption/emission spectroscopy (FAES) data from a *CHM317* lab. This data represents the emission signal of five sodium (Na) standards measured in triplicate:

```{r, message = FALSE}
FAES <- read_csv(file = "data/FAES.csv")
head(FAES)
```

In this dataset you can see that two important aspects of the data, sample type (sample, blank or standard) and concentration are grouped in one column.
We can use the `separate()` function we learned about in [Separating columns] to separate these values into two columns to facilitate further analysis.

```{r, message = FALSE}
FAES <- separate(
  FAES,
  col = std_Na_conc,
  into = c("type", "conc_Na", "units"),
  sep = " ",
  convert = TRUE
)

DT::datatable(FAES)
```

## Selecting by row or value

`filter()` allows up to subset our data based on observation (row) values. 

```{r}
filter(FAES, conc_Na == 0)
```

Note how we need to pass logical operations to `filter()` to specify which rows we want to select. In the above code, we used `filter()` to get all rows where the concentration of sodium is equal to 0 (`== 0`). Note the presence of two equal signs (`==`). In R one equal sign (`=`) is used to pass an argument, two equal signs (`==`) is the logical operation "is equal" and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use `=` instead of `==` when testing for equality. 

### Logical operators

`filter()` can use other *relational* and *logical* operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators... let's just list what they are and how we can use them:

```{r, echo = FALSE}
ops <- data.frame(
  Operator = c(">", "<","<=",">=","==","!=", "&","!","|", "is.na()"),
  Type = c("relational", "relational", "relational", "relational", "relational", "relational", "logical", "logical", "logical", "function"), 
  Description = c("Less than", "Greater than", "Less than or equal to", "Greater than or equal to", "Equal to", "Not equal to", "AND", "NOT", "OR", "Checks for missing values, TRUE if NA")
)

knitr::kable(ops)
```

- Selecting all signals below a threshold value:

  ```{r}
  filter(FAES, signal < 4450)
  ```

- Selecting signals between values:

  ```{r}
  filter(FAES, signal >= 4450 & signal < 8150)
  ```

- Selecting all other replicates other than replicate `2`:

  ```{r}
  filter(FAES, replicate != 2)
  ```

- Selecting the first standard replicate OR any of the blanks:

  ```{r}
  filter(FAES, (type == "standard" & replicate == 1) | (type == "blank"))
  ```

- Removing any rows with missing signal values (`NA`) using `is.na()`. Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (`!`) we would have selected all rows *with* missing values. 

  ```{r}
  filter(FAES, !is.na(signal))
  ```

These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, it's up to you do figure out which works best for you. 

## Arranging rows

`arrange()` reorders the rows based on the value you passed to it. By default it arranges the specified values into ascending order. Let's arrange our data our data by increasing order of signal value:

```{r}
arrange(FAES, signal)
```

Since our original `FAES` data is already arranged by increasing `conc_Na` and `replicate`, let's inverse that order by arranging `conc_Na` into descending order using the `desc()` function WHILE arranging the `signal` values in ascending order:

```{r}
# Note the order of precedence (left-to-right)
arrange(FAES, desc(conc_Na), signal)
```

Just note with `arrange()` that `NA` values will always be placed at the bottom, whether you use `desc()` or not. 

## Selecting column name

`select()` allows you to readily select columns by name. Note however that it will always return a tibble, even if you only select one variable/column.

```{r}
select(FAES, signal)
```
You can also select multiple columns using the same operators and helper functions described in [Tidying Your Data]:. 

```{r}
select(FAES, conc_Na:replicate)
```
```{r}
# Getting columns containing the character "p"
select(FAES, contains("p"))
```


## Deleting Columns or Rows
While the process of selecting and filtering data is pivotal in data analysis, there are instances when you may need to remove specific columns or rows entirely. This is useful especially when you're dealing with redundant or irrelevant data that might clutter your analysis.

### Deleting columns
To delete a column, you can use the `select()` function with the `-` sign before the column name you want to remove:

```{r}
# This will remove the 'signal' column from the FAES dataset
head(select(FAES, -signal))
```

Multiple columns can be deleted by providing more column names after the `-` sign:

```{r}
# Deleting both 'signal' and 'replicate' columns from the FAES dataset
head(select(FAES, -c(signal, replicate)))
```


### Deleting rows

To delete rows, the `filter()` function can be used in conjunction with relational or logical conditions that define the rows you wish to exclude:

```{r}
# This will remove rows where 'signal' values are less than 20000
filter(FAES, !(signal < 20000))
```
The key here is the use of the `!` (NOT) operator which excludes rows that meet the specified condition.


##  Adding new variables

`mutate()` allows you to add new variables (read columns) to your existing data set. It'll probably be the workhorse function you'll use during your data transformation as you can readily pass other functions and mathematical operators to it to transform your data. let's suppose that our standards were diluted by a factor of 10, we can add a new column `dil_fct` for this:

```{r}
mutate(FAES, dil_fct = 10)
```

We can also create multiple columns in the same `mutate()` call: 

```{r}
mutate(FAES, 
       dil_fct = 10, 
       adj_signal = signal * dil_fct)
```


A couple of things to note: 

1. Quotation marks are generally optional when creating a new variable in `mutate()`, but they become necessary if the variable name contains spaces, special characters, or starts with a number. For example, `"dil_fct"`, `dil_fct`, and `dil_fct1` are all valid, but if you had a variable name like `"dil fct"`, `"dil-fct"`, or `"2nd_fct"`, the quotes would be required.
  2. The variables we're referencing do not need to be in quotation marks; hence `signal` because this variable already exists. 
  3. Note the order of precedence: `dil_fct` is created first so we can reference in the second column being added, we would get an error if we swapped the order. 
  
### Mutate with a condition

In data analysis, there are often scenarios where we want to categorize or re-label values based on certain conditions. The `case_when()` function offers a versatile and readable solution for handling these multiple conditions.

The syntax for `case_when()` is straightforward: for each condition, you specify the logical test followed by the tilde (~) operator, and then the value or expression to return if the condition is `TRUE`.
A `.default` value can be provided for cases when none of the conditions are `TRUE`.

With our FAES data, say you want to label each `conc_Na` as "Low", "Medium", or "High" based on its value. You can use `case_when()` within `mutate()` as follows:

```{r}
mutate(FAES, 
       conc_Na_level = case_when(
         conc_Na < 0.2 ~ "Low",
         conc_Na < 0.4 ~ "Medium",
         .default = "High"))
```
  
For those interested in exploring further, there's a similar function called `ifelse()` which provides conditional transformations in R. You can learn more about it in R documentation found [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse).
  
  
### Useful mutate function

There are a myriad of functions you can make use of with `mutate`. Here are some of the mathematical operators available in R: 

```{r, echo = FALSE}
funs <- data.frame(
  "Operator or Function" = c("+", "-", "*", "/", "^", "log()"),
  Definition = c("addition", "subtraction", "multiplication", "division", "exponent; to the power of...", "returns the specified base-log; see also log10() and log2()")
)

knitr::kable(funs)
```

## Group and summarize data

`summarize` effectively summarizes your data based on functions you've passed to it. Looking at our `FAES` data we'd might want the mean and standard deviation of the triplicate signals. Let's see what happens when we apply the summarize function straight up: 

```{r}
summarise(FAES, mean = mean(signal), stdDev = sd(signal))
```

This doesn't look like what we wanted. What we got was the mean and standard deviation of *all* of the signals, regardless of the concentration of the standard. Also note how we've lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to **group** our observations by a variable. We can do this by using the `group_by()` function.

```{r}
groupedFAES <- group_by(FAES, type, conc_Na)
summarise(groupedFAES, mean = mean(signal), stdDev = sd(signal))
```

Here we've created a new data set, `groupedFAES`, that we grouped by the variables `type` and `conc_Na` so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. Depending on your dataset and the analysis you're performing, you'll need to decide how to group your data: the more variables you use, the smaller each group will be.

### Useful summarize functions

We've used the `mean()` and `sd()` functions above, but there are a host of other useful functions you can use in conjunction with summarize. See **Useful Functions** in the `summarise()` documentation (enter `?summarise`) in the console. This is also discussed in more depth in the [Summarizing Data] chapter. 

## The Pipe: Chaining Functions Together

Piping is a concept that allows you to chain functions together in a way that simplifies and clarifies your code. At its core, piping is similar to function composition in mathematics, where the output of one function becomes the input to the next. This helps you build complex operations in a readable and logical sequence.

### Function Composition Example

Consider a mathematical example of function composition:

\[ f(g(x)) \]

Here, the function \( g(x) \) is applied first, and its output is then passed as the input to the function \( f(x) \). In programming, this concept can be translated to chaining functions together.

### Abstract Example of Piping

The **pipe operator** `%>%`, an incredibly useful tool for writing more legible and understandable code. The pipe basically changes how you read code to emphasize the functions you're working with by passing the intermediate steps to hidden processes in the background.

Now, consider the following abstract example of piping in R:

```{r,eval=FALSE}
result <- data %>%
  step1() %>%
  step2() %>%
  step3()
```

Here’s what’s happening:

- **`step1()`**: Takes `data` as its input.
- **`step2()`**: Takes the result of `step1()` as its input.
- **`step3()`**: Takes the result of `step2()` as its input.

This sequence of operations could be written without pipes by nesting function calls, but the use of pipes makes the flow of data more explicit and easier to read.

### Simple Examples of Piping

Let’s start with a simple, single use of piping:

```{r,eval=FALSE}
FAES %>% nrow()
```

In this case, the `%>%` operator pipes the `FAES` dataset directly into the `nrow()` function, which returns the number of rows in the dataset. This is functionally equivalent to:

```{r,eval=FALSE}
nrow(FAES)
```

Now, let's consider an example where the function takes an additional argument:

```{r,eval=FALSE}
meanBlank <- FAES %>%
  filter(type == "blank")
```

This is functionally equivalent to:

```{r,eval=FALSE}
meanBlank <- filter(FAES, type == "blank")
```

In both cases, piping might seem redundant because it only removes the need to specify the first argument explicitly.

### Piping in Practice

With the tools presented in this chapter without using pipe operator we could do a decent job analyzing our `FAES` data. Let's say we wanted to subtract the mean of the `blank` from each `standard` signal and then summarize those results. It would look something like this:

```{r}
blank <- filter(FAES, type == "blank")
meanBlank <- summarize(blank, mean(signal))
meanBlank <- as.numeric(meanBlank)

paste("The mean signal from the blank triplicate is:", meanBlank)

stds_1 <- filter(FAES, type == "standard")
stds_2 <- mutate(stds_1, cor_sig = signal - meanBlank)
stds_3 <- group_by(stds_2, conc_Na)
stds_4 <- summarize(stds_3, mean = mean(cor_sig), stdDev = sd(cor_sig))
stds_4
```

If we use pipes, we can make this code much more legible and easier to understand. The code with pipes would look like this:

```{r}
meanBlank <- FAES %>%
  filter(type == "blank") %>%
  summarise(mean(signal)) %>%
  as.numeric()

paste("The mean signal from the blank triplicate is:", meanBlank)

stds <- FAES %>%
  filter(type == "standard") %>%
  mutate(cor_sig = signal - meanBlank) %>% 
  group_by(conc_Na) %>%
  summarize(mean = mean(cor_sig), stdDev = sd(cor_sig))

stds
```

While the initial code did its job, it's certainly wasn't easy to type and certainly not easy to read. At every step of the way we've saved our updated data outputs to a new variable (`stds_1`, `stds_2`, etc.). However, most of these intermediates aren't important, and moreover the repetitive names clutter our code. As the code above is written, we've had to pay special attention to the variable suffix to make sure we're calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. `stds_1 <- mutate(stds_1, ...)`), but that doesn't solve the issue of readability as there's still redundant assigning.

Things may look a bit different, but our underlying code hasn't changed much. What's happening is the pipe operator passes the output to the first argument of the next function. So the output of `filter...` is passed to the first argument of `sumamrise...`, and the argument we specified in `summarise` is actually the *second* argument it receives. You're probably wondering how hiding stuff makes your code more legible, but think of `%>%` as being equivalent to "then". We can read our code as:

>"Take the `FAES` dataset, *then* filter for `type == "blank"` *then* collapse the dataset to the mean `signal` value and *then* convert to numeric value *then* pass this final output to the new variable `meanBlank`."

Not only is the pipe less typing, but the emphasis is on the functions so you can better understand what you're doing vs. where all the intermediate values are going.

### Notes on piping

The pipe is great, but it does have some limitations: 

  - You can't easily extract intermediate steps. So you'll need to break up your pipping chain to output any intermediate steps you can. 
  - The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of what's going on. Keep the piping short and thematically similar. 
  - Pipes are linear, if you have multiple inputs or outputs you should consider an alternative approach. 


## Further reading {#further_reading_chapter12}

  - [Chapter 5: Data Transformation](https://r4ds.had.co.nz/transform.html) of *R for Data Science* for a deeper breakdown of `dplyr` and its functionality. 
  - [Chapter 18: Pipes](https://r4ds.had.co.nz/pipes.html) of *R for Data Science* for more information on pipes. 
  - [Syntax equivalents: base R vs Tidyverse](https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html) by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but there's no denying that certain base-R can be more elegant. Check out this write up to get a better idea. 
  
```{r child='src/common/end-of-chapter-exercise.Rmd'}
```

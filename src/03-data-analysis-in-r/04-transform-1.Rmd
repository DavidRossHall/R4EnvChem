# Transform: Data Manipulation

Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. We've already touched upon some useful transformation functions in previous example code snippets, such as the `mutate` function for adding columns. This section will explore some of the most useful functionalities of the `dplyr` package, explicitly introduce the pipe operator `%>%`, and showcase how you can leverage these tools to quickly manipulate your data.

The essential `dplyr` functions are :

  - `mutate()` to create new columns/variables from existing data
  - `arrange()` to reorder rows 
  - `filter()` to refine observations by their values (in other words by row)
  - `select()` to pick variables by name (in other words by column)
  - `summarize()` to collapse many values down to a single summary. 
  
We'll go through each of these functions, for more details you can read [Chapter 3: Data Transformation](https://r4ds.had.co.nz/transform.html) from *R for Data Science* which provides a more comprehensive breakdown of these functions. Note that the information here is based on a `tidyverse` approach, but this is only one way of doing things. See the [Further reading](#further_reading_chapter12) section for links to other suitable approaches to data transformation. 

Let's explore the functionality of `dplyr` using some flame absorption/emission spectroscopy (FAES) data from a *CHM317* lab. This data represents the emission signal of five sodium (Na)
standards measured in triplicate:

standards measured in triplicate:


```{r, message = FALSE}
# Importing using tips from Import chapter
FAES <- read_csv(file = "data/FAES.csv")
head(FAES)

```

In this dataset you can see that two important aspects of the data, sample type (sample, blank or standard) and concentration are grouped in one column.  We can use the separate function to separate these into two columns to facilitate further analysis.

```{r, message = FALSE}
# Importing using tips from Import chapter
FAES <- read_csv(file = "data/FAES.csv") %>% # see section on Pipe
  separate(col = std_Na_conc,
           into = c("type", "conc_Na", "units"),
           sep = " ",
           convert = TRUE)

DT::datatable(FAES)
```

**Note** the use of `convert = TRUE` in the `separate()` call. This runs a type convert on new columns. If we didn't include this, the `conc_Na` column would be of type character because the numbers originated from a string. `convert()` ensures they're converted to numeric. **Always use `convert = TRUE`** when you separate columns. 

## Selecting by row or value

`filter()` allows up to subset our data based on observation (row) values. 

```{r}
filter(FAES, conc_Na == 0)
```

Note how we need to pass logical operations to `filter()` to specify which rows we want to select. In the above code, we used `filter()` to get all rows where the concentration of sodium is equal to 0 (`== 0`). Note the presence of two equal signs (`==`). In R one equal sign (`=`) is used to pass an argument, two equal signs (`==`) is the logical operation "is equal" and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use `=` instead of `==` when testing for equality. 

### Logical operators

`filter()` can use other *relational* and *logical* operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators... let's just list what they are and how we can use them:

```{r, echo = FALSE}

ops <- data.frame("Operator" = c(">", "<","<=",">=","==","!=", "&","!","|", "is.na()"),
                  "Type" = c("relational", "relational", "relational", "relational", "relational", "relational", "logical", "logical", "logical", "function"), 
                  Description = c("Less than", "Greater than", "Less than or equal to", "Greater than or equal to", "Equal to", "Not equal to", "AND", "NOT", "OR", "Checks for missing values, TRUE if NA" ))


knitr::kable(ops)
```

- Selecting all signals below a threshold value

```{r}
filter(FAES, signal < 4450)
```

- Selecting signals between values 

```{r}
filter(FAES, signal >= 4450 & signal < 8150)
```

- Selecting all other replicates other than replicate `2`

```{r}
filter(FAES, replicate != 2)
```

- selecting the first standard replicate OR any of the blanks.

```{r}
filter(FAES, (type == "standard" & replicate == 1) | (type == "blank"))
```
- Removing any missing values (`NA`) using `is.na()`. Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (`!`) we would have selected all rows *with* missing values. 

```{r}
filter(FAES, !is.na(signal))
```



These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, it's up to you do figure out which works best for you. 

## Arranging rows

`arrange()` reorders the rows based on the value you passed to it. By default it arranges the specified values into ascending order. Let's arrange our data our data by increasing order of signal value:

```{r}
arrange( FAES, signal)
```

Since our original `FAES` data is already arranged by increasing `conc_Na` and `replicate`, let's inverse that order by arranging `conc_Na` into descending order using the `desc()` function WHILE arranging the `signal` values in ascending order:

```{r}
# Note the order of precedence (left-to-right)
arrange(FAES, desc(conc_Na), signal)
```

Just note with `arrange()` that `NA` values will always be placed at the bottom, whether you use `desc()` or not. 

## Selecting column name

`select()` allows you to readily select columns by name. Note however that it will always return a tibble, even if you only select one variable/column.

```{r}
select(FAES, signal)
```
You can also select multiple columns using the same operators and helper functions described in [Tidying Your Data]:. 

```{r}
select(FAES, conc_Na:replicate)
```
```{r}
# Getting columns containing the character "p"
select(FAES, contains("p"))
```


## Deleting Columns or Rows
While the process of selecting and filtering data is pivotal in data analysis, there are instances when you may need to remove specific columns or rows entirely. This is useful especially when you're dealing with redundant or irrelevant data that might clutter your analysis.

### Deleting columns
To delete a column, you can use the `select()` function with the `-` sign before the column name you want to remove:

```{r}
# This will remove the 'signal' column from the FAES dataset
head(select(FAES, -signal))
```

Multiple columns can be deleted by providing more column names after the `-` sign:

```{r}
# Deleting both 'signal' and 'replicate' columns from the FAES dataset
head(select(FAES, -c(signal, replicate)))
```


### Deleting rows

To delete rows, the `filter()` function can be used in conjunction with relational or logical conditions that define the rows you wish to exclude:

```{r}
# This will remove rows where 'signal' values are less than 20000
filter(FAES, !(signal < 20000))
```
The key here is the use of the `!` (NOT) operator which excludes rows that meet the specified condition.


##  Adding new variables

`mutate()` allows you to add new variables (read columns) to your existing data set. It'll probably be the workhorse function you'll use during your data transformation as you can readily pass other functions and mathematical operators to it to transform your data. let's suppose that our standards were diluted by a factor of 10, we can add a new column for this:

```{r}
mutate(FAES, "dil_fct" = 10)
```

We can also create multiple columns in the same `mutate()` call: 

```{r}
mutate(FAES, 
       dil_fct = 10, 
       adj_signal = signal * dil_fct)
```


A Couple of things to note: 

  1. The variable we're creating needs to be in quotation marks, hence `dil_fct` for our dilution factor variable
  2. The variables we're referencing do not need to be in quotation marks; hence `signal` because this variable already exists. 
  3. Note the order of precedence: `dil_fct` is created first so we can reference in the second argument, we would get an error if we swapped the order. 
  
### Mutate with a condition

In data analysis, there are often scenarios where we want to categorize or re-label values based on certain conditions. The `case_when()` function, provided by the `dplyr` package, which was introduced in the [Piping conditional statements] section, offers a versatile and readable solution for handling these multiple conditions.

The syntax for `case_when()` is straightforward: for each condition, you specify the logical test followed by the tilde (~) operator, and then the value or expression to return if the condition is `TRUE`.

With our FAES data, say you want to label each `conc_Na` as "Low", "Medium", or "High" based on its value. You can use `case_when()` within `mutate()` as follows:

```{r}
mutate(FAES, 
       conc_Na_level = case_when(
         conc_Na < 0.2 ~ "Low",
         conc_Na < 0.4 ~ "Medium",
         TRUE ~ "High"))
```
  
For those interested in exploring further, there's a similar function called `ifelse()` which provides conditional transformations in R. You can learn more about it in the [Conditional Statements] section in this resource or in the documentation for the ifelse statement that can be found [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse).
  
  
### Useful mutate function

There are a myriad of functions you can make use of with the mutate function. Here are some of the mathematical operators available in R: 

```{r, echo = FALSE}

funs <- data.frame("Operator/Function" = c("+", "-", "*", "/", "^", "log()"),
                   "Definition" = c("additon", "subtraction", "multiplication", "division", "exponent; to the power of...", "returns the specified base-log; see also log10() and log2()"))

knitr::kable(funs)


```

## Group and summarize data

`summarize` effectively summarizes your data based on functions you've passed to it. Looking at our `FAES` data we'd might want the mean and standard deviation of the triplicate signals. Let's see what happens when we apply the summarize function straight up: 

```{r}
summarise(FAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

This doesn't look like what we wanted. What we got was the mean and standard deviation of *all* of the signals, regardless of the concentration of the standard. Also note how we've lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to **group** our observations by a variable. We can do this by using the `group_by()` function.

```{r}
groupedFAES <- group_by(FAES, type, conc_Na)
summarise(groupedFAES, "mean" = mean(signal), "stdDev" = sd(signal))
```

Here we've created a new data set, `groupedFAES`, that we grouped by the variables `type` and `conc_Na` so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. For this data set we could have omitted the `type` variable, but in larger datasets you may have multiple groupings (i.e. from different location), so you can group by multiple variables to get smaller groups. 

### Useful summarize functions

We've used the `mean()` and `sd()` functions above, but there are a host of other useful functions you can use in conjunction with summarize. See **Useful Functions** in the `summarise()` documentation (enter `?summarise`) in the console. This is also discussed in more depth in the [Summarizing Data] chapter. 

## The Pipe: Chaining Functions Together 

With the tools presented here we could do a decent job analyzing our `FAES` data. Let's say we wanted to subtract the mean of the `blank` from each `standard` signal and then summarize those results. It would look something like this: 

```{r}
blank <- filter(FAES, type == "blank")
meanBlank <- summarize(blank, mean(signal))
meanBlank <- as.numeric(meanBlank)

paste("The mean signal from the blank triplicate is:", meanBlank)

stds_1 <- filter(FAES, type == "standard")
stds_2 <- mutate(stds_1, "cor_sig" = signal - meanBlank)
stds_3 <- group_by(stds_2, conc_Na)
stds_4 <- summarize(stds_3, "mean" = mean(cor_sig), "stdDev" = sd(cor_sig))
stds_4
```

While the code above did its job, it's certainly wasn't easy to type and certainly not easy to read. At every step of the way we've saved our updated data outputs to a new variable (`stds_1`, `stds_2`, etc.). However, most of these intermediates aren't important, and moreover the repetitive names clutter our code. As the code above is written, we've had to pay special attention to the variable suffix to make sure we're calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. `stds_1 <- mutate(stds_1, ...)`), but that doesn't solve the issue of readability as there's still redundant assigning. 

A solution for this is the pipe operator `%>%` ( pronounced as "then"), an incredibly useful tool for writing more legible and understandable code. The pipe basically changes how you read code to emphasize the functions you're working with by passing the intermediate steps to hidden processes in the background. Re-writing the code above, we'd get something like: 

```{r}
meanBlank <- FAES %>%
  filter(type =="blank") %>%
  summarise(mean(signal)) %>%
  as.numeric()

paste("The mean signal from the blank triplicate is:", meanBlank)
```

Things may look a bit different, but our underlying code hasn't changed much. What's happening is the pipe operator passes the output to the first argument of the next function. So the output of `filter...` is passed to the first argument of `sumamrise...`, and the argument we specified in `summarise` is actually the *second* argument it receives. You're probably wondering how hiding stuff makes your code more legible, but think of `%>%` as being equivalent to "then". We can read our code as:

>"Take the `FAES` dataset, *then* filter for `type == "blank"` *then* collapse the dataset to the mean `signal` value and *then* convert to numeric value *then* pass this final output to the new variable `meanBlank`." 

Not only is the pipe less typing, but the emphasis is on the functions so you can better understand what you're doing vs. where all the intermediates are going. Extending our piping to the second batch of code we get:  


```{r}
stds <- FAES %>%
  filter(type == "standard") %>%
  mutate("cor_sig" = signal - meanBlank) %>% 
  group_by(conc_Na) %>%
  summarize("mean" = mean(cor_sig), "stdDev" = sd(cor_sig))

stds
```

Same thing. The underlying code hasn't changed much, but it's much more legible and we can clearly see we're subtracting the `meanBlank` value from each measured signal then summarizing the corrected signals. 

### Notes on piping

The pipe is great and especially useful with *tidyverse* packages, but it does have some limitations: 

  - You can't easily extract intermediate steps. So you'll need to break up your pipping chain to output any intermediate steps you can. 
  - The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of what's going on. Keep the piping short and thematically similar. 
  - Pipes are linear, if you have multiple inputs or outputs you should consider an alternative approach. 


## Further reading {#further_reading_chapter12}

  - [Chapter 5: Data Transformation](https://r4ds.had.co.nz/transform.html) of *R for Data Science* for a deeper breakdown of `dplyr` and it's functionality. 
  - [Chapter 18: Pipes](https://r4ds.had.co.nz/pipes.html) of *R for Data Science* for more information on pipes. 
  - [Syntax equivalents: base R vs Tidyverse](https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html) by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but there's no denying that certain base-R can be more elegant. Check out this write up to get a better idea. 
  
```{r child='src/common/end-of-chapter-exercise.Rmd'}
```

# (PART\*) Part 5: Modelling in R {-}

# Modelling: Linear Regression

Modelling is basically math used to describe some type of system, and they are a forte of R, a language tailor made for statistical computing... Every model has assumptions, limitations, and all around tricky bits to working. There is no shortage of modelling in a myriad of context, but in this chapter we'll discuss and break down the most common model you'll encounter, the *linear regression model*, in the most common context, the *linear calibration model*, using the most common function, `lm()`. 

You have probably encountered the linear regression model under the pseudonym "trend-lines", most likely generated by *Excel*'s "add trend-line option" (as in CHM135). While the models we'll be constructing with `lm()` work much the same *mathematically*, unlike *Excel*, R returns *alllll* of the model outputs. Correspondingly, it's easy to get lost between juggling R code, the seemingly endless model outputs, and keeping yourself grounded in the real systems you're attempting to model. 

To this end, this chapter is broken into the following parts: 

  - [Modelling theory] where we briefly touch upon *what* model is being calculated.
  - [Modelling in R] where we provide a boilerplate template for *how* to calculate models in R. 
  - [Visualizing models] where we explore our model results. 
  - [Calculating Concentrations] where we use our model to calculate the concentration.

## Modelling Theory

The *linear calibration model* relates the response of an instrument to the value of the *measurand*. The measurand is simply what we're measuring, often the concentration of an analyte. So we use the measurand, which we can control via preparation of standards from reference material as the *independent* variable, with the instrument output being the *dependent* variable (as instrument response varies with concentration). Altogether we're:

  1. Measuring the instrument response of standards of known concentration and samples of unknown concentration. 
  2. Calculating the linear calibration model (i.e. line of best fit) through our standards. 
  3. Using the measurement model to calculate the concentration in our unknown from their respective instrument response. 

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
library(tidyverse)
df <- data.frame("conc" = c(0, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8),
                 "abs" = c(0, 0.057, 0.119, 0.261, 0.353, 0.599, 0.730))

df <- df %>%
  nest() %>%
  mutate(fit = map(data, ~lm(abs ~ conc, data = .x)),
         tidied = map(fit, broom::tidy),
         glanced = map(fit, broom::glance),
         augmented = map(fit, broom::augment)
         ) %>%
  unnest(augmented)
```

```{r reg-plot, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Linear calibration model; figure modified from Hibbert and Gooding (2006)."}



ggplot(df, 
       aes( x = conc, y = abs)) +
  geom_point() +
  
  # for residuals
  geom_smooth(method = lm, se = FALSE) +
  
  # actual measurement
  geom_segment(aes(xend = conc, yend = .fitted)) +
  
  
  # y4 measurement
  geom_segment(aes( x = 0, y = 0.261 , xend = 0.20, yend = 0.261),
               alpha = 0.6, linetype = "dashed") +
  annotate("text", x = 0.025, y = 0.281, label = expression(y[4] )) +

  # y4 fitted
  geom_segment(aes( x = 0, y = 0.20391462 , xend = 0.20, yend = 0.20391462),
               alpha = 0.6, linetype = "dashed") +
    annotate("text", x = 0.025, y = 0.23, label = expression(widehat(y)[4])) +

  # residual annotation
  annotate("text", x = 0.25, y = 0.1, label = "This is a\n residual", hjust = 1) +
  geom_curve(aes( x = 0.24, y = 0.15, xend = 0.205, yend = 0.245),
              curvature = 0.5, arrow = arrow(length = unit(0.03, "npc"))) +

  # measured point annotation
  annotate("text", x = 0.5, y = 0.3, label = "This is the \n measurement \n of a standard", hjust = 0) +
  geom_curve(aes(x =0.5, y = 0.3, xend = 0.4, yend = 0.33),
              curvature = -0.7, arrow = arrow(length = unit(0.03, "npc"))) +

  # fitted line annotation
  annotate("text", x = 0.65, y = 0.45, label = "This is the \n calibration line") +
  geom_curve(aes(x = 0.65, y = 0.5, xend = 0.65, yend = 0.6),
              curvature = 0, arrow = arrow(length = unit(0.03, "npc"))) +
  # unknown inst. response annotation
  geom_point(aes(x = 0, y = 0.35), colour = "red", shape = "square") +
  geom_curve(aes(x =0, y = 0.35, xend = 0.359, yend = 0.35),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash", colour = "red") +
  annotate("text", x = 0.05, y = 0.5, label = "Response of\n unknown", hjust = 0.25) +
  geom_curve(aes(x = 0.05, y = 0.45, xend = 0.005, yend = 0.36), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # unknown calculated conc
  geom_curve(aes(x =0.359, y = 0.35, xend = 0.359, yend = 0),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash",
            colour = "red") + 
  annotate("text", x = 0.4, y = 0.1, label = "Concentration of\n unknown", hjust = 0) +
  geom_curve(aes(x = 0.4, y = 0.1, xend = 0.365, yend = 0.01), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # so residual lines touch y axis
  scale_x_continuous(expand = c(0,0)) +
  coord_cartesian(clip = "off") +
  labs(x = "concentration",
       y = "instrument response") +
  theme_classic() +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank())

```


Before we can calculate concentrations, we need a measurement model. In other words, an equation that relates instrument response to sample concentration (or other factors). For simple linear calibration, we use:

$$ y = a + bx$$

Where:

  - $y$ is the instrument response
  - $x$ is the independent variable (i.e. sample concentration)
  - $a$ and $b$ are the coefficients of the model; otherwise known as intercept and slope, respectively. 
  
  
We'll gloss over some of the more technical aspects of modelling, and discuss other in more detail below. For now, know that:

  - We're assuming our linear model is correct (i.e. the instruments actually responds linearly to concentration).
  - All uncertainties reside in the dependant variable $y$ (i.e. no errors in preparation of the standards). 
  - The values of $a$ and $b$ are determined by minimizing the *sum of the residuals squared*. 
    - The residuals are the difference between the actual measured response and where it would be if it were on the calibration line.  

Once we have our line of best fit, we can calculate the concentration of our unknown sample $i$, from it's measured response $y_i$ by:

$$ x_i = \frac{y_i ~-~b}{a}$$

  
There is more going on under the hood then what we're describing here, but this should be enough to get you up and running. If you would like a greater breakdown of linear calibration modelling, we suggest you read Chapter 5 of *Data Analysis for Chemistry* by Hibbert and Gooding. An online version is accessible via the University of Toronto's Library. Also there is no reason the instrument response must be linear. In fact, we spend a great deal of time arranging our experiment to that we land in the 'linear range'. For details on non-linear modelling in R see [Non-Linear Regression]

## Modelling in R

Now that we have a rough understanding of what we're trying to do, let's go over *how* to calculate linear regression models in R. Note model is a general term, in this situation we'll be calculating a **calibration curve**. All calibration curves are models, but not all models are calibration curves.

For our example dataset we'll import a dataset consisting of four analytical standards of sodium plus a calibration blank all run in triplicate. The standards were measured via flame atomic emission spectroscopy (FAES). Let's import the FAES calibration results we saw in [Transform: dplyr and data manipulation]. As we've already seen, our data is composed of four standards and a blank analyzed in triplicate. Since we're focusing on modelling, *we'll treat the blank as a standard in our model fitting*.
So let's import our dataset:

```{r, message = FALSE}

# Importing using tips from Import chapter

FAES <- read_csv(file = "data/FAES_original_wide.csv") %>%
  pivot_longer(cols = -std_Na_conc,
               names_to = "replicate", 
               names_prefix = "reading_",
               values_to = "signal") %>%
  separate(col = std_Na_conc,
           into = c("type", "conc_Na", "units"),
           sep = " ",
           convert = TRUE) %>%
  mutate(type = "standard")

DT::datatable(FAES)
```

And let's quickly plot our data. You should always visualize your data before modelling, especially for linear calibration modelling. Unlike *statistical* modelling. Visualizing your data is the easiest way to spot trends and gross errors in your data. 

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point()
```

### Base R Linear Model

R's base `lm()` function for linear regression is excellent, but it's outputs have some messy quirks. It's easier to show that, so let's calculate the linear relationship between the `signal` as a function of `conc_Na`:

```{r}
lm_fit <- lm(signal ~ conc_Na, data = FAES)
lm_fit
```

Reading the code above (recall that we're reading it from *right to left* because it's base R):

1.  We're taking the FAES data we created earlier; `data = FAES`
2.  We're comparing `signal` (the dependent, y, variable) to `conc_Na` (the independent, x, variable) via the tilde `~`. The way to read this is: *"Signal depends on concentration"*.
3.  We're comparing these two variables using the `lm()` function for generalized linear models.
4.  All of the model outputs are stored in the `lm_fit` variable.

As we can see, the model outputs are pretty brief and not much more than *Excel*'s outputs. We can use `summary()` to extract more information to better understand our model:

```{r}
summary(lm_fit)
```

Now we have a lot more information from our model (don't worry about what everything means, it's discussed further in Section 3. For now, understand that it's a hot mess.

### Cleaning up model ouputs

`summary()` provides a decent overview of our model's performance, but the outputs are difficult to work with. Let's turn to the `broom()` package to clean up our model outputs.

```{r FAES-calCurves}
library(broom)

calCurve <- FAES %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         )
calCurve
```

Things look a bit more complicated than our earlier example, so let's break down our code line by line:

1.  We're taking the `FAES` dataset that we created earlier.
2.  `group_by(type)` groups all rows by `type`, in this situation we have only one type: `standard`.
3.  `nest()` collapses everything other than the `type` column into smaller data-frames. In this situation, all other information is stored as a `tibble` under the `data` column; this is the data used to calculate the linear model.
4.  Withing the `mutate` function, we've created four columns: `fit`, `tidied`,  `glanced`, and `augmented`.

And it's these columns that contain our cleaned up model outputs. `fit` contains the raw output from the linear regression model for `signal` as a function of `conc_Na` using the `lm()` function. The output is in the form of a list, similar to what `summary()` gave us above. Again, this is exceptionally messy, hence why we used the `tidy()`, `glance()`, and `augment()` functions from the `broom` package . `map()` just means we're applying the function `tidy()` to the individual output list created by `lm()` and stored in the `fit` column. Note that the `tidy()`, `glanced()`, and `augmented` outputs are tibbles. So we now have a tibble containing specific model output values (i.e. `(Intercept)`), lists (i.e. `fit`), and tibbles (`tidied`). This is known as \*\*nested data\*. We're no longer in Kansas anymore...

We'll break down what each function did below. Keep in mind however that `lm()` is used for a variety of statistical tests, and consequently has many associated outputs. Some are essential, others are useful, and some are useless for linear calibration. There are also many ways to use these additional model outputs to calculate outliers, etc. but you shouldn't have any outliers in your calibration model. Don't rely on statistics to bail you out of poor chemical technique. 

### Glanced outputs

Anyways, let's take a look at our model results. The `glanced` tibble contains "...a concise one-row summary of the model. This typically contains values such as R\^2, adjusted R\^2, and residual standard error that are computed once for the entire mode"[^linear-regression-1] Because the data is nested, we'll need to use `unnest()` to flatten it back out into regular columns:

```{r FAES-lm-Glanced}
glanced <- calCurve %>%
  unnest(glanced)

# DT is to make interactive tables for the book.
DT::datatable(glanced, 
              options = list(scrollX = TRUE))
```

What you see here is a bit more than what you'd get from *Excel*'s 'line-of-best fit' output. In brief, :

  - `type`, `data`, `fit`, `tidied`, and `augmented` are columns we've created earlier.
  - `r.squared` is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable. The closer `r.squared` is to 1, the more variance is captured by the model. 
  - `adj.r.squared` is the same as `r.squared` in this situation. This is because `r.squared` will always increase if we add more exploratory variables to our model; the `adj.r.squared` accounts for the number of exploratory variables used in the model. 
    - In our case we only have one exploratory variable, hence they're approximately the same.
  - The other columns are different measurements of goodness-of-fit and hypothesis testing of the model. See [Further reading]. 

### Tidied outputs
    
But what about the slope and the intercept? After all, that's what we need to calculate the concentration in our unknowns. Let's take a look at `tidied` from the `tidy()` function which constructs a tibble that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression:[linear-regression-2]

[^linear-regression-2]: From the *broom* package vignette.

```{r FAES-lm-tidied}
# storing because we'll use it later on. 

tidied <- calCurve %>% 
  unnest(tidied)

# DT is to make interactive tables for the book.
DT::datatable(tidied, 
              options = list(scrollX = TRUE))

```

Again, a lot more to unpack compared to *Excel*. That's because the `lm()` function in R calculates a generalized linear model. `lm()` performs a linear regression model, which we normally think of as an equation of the form $y= a + bx$ as discussed earlier. But, regression models can be expanded to account for multiple variables (hence *multiple linear regression*) of the form:

$$y = \beta _{0} + \beta _{1} x_{1} + \beta _{2} x_{2} ... \beta _{p} x_{p}$$]

where,

-   $y$ = dependent variable
-   $x$ = exploratory variable; there's no limit how many you can input
-   $\beta _{0}$ = y-intercept (constant term)
-   $\beta _{p}$ = slope coefficient for each explanatory variable

With our linear calibration model, we only have one input variable for our model (`conc`), so the above formula collapses down to $y = \beta _{0} + \beta _{1} x_{1}$. So looking at our tidied model outputs: 


  - each row corresponds to a model coefficient (under the `term` column). 
  - For each modelling parameter, we're provided an estimate of it's numerical value: `estimate`.  These are the values we'll use to calculate concentration. 
  - `std.error` measures how precisely the model estimates the coefficient's unknown value; smaller is better. 
  - `p.value` is a indication of the significance of a model coefficient; the closer to zero the better. 
      - If we were to use multiple parameters in our model (eg. concentration and temperature) we could use the `p.value` to determine if a given coefficient was useful for our model. 
      
We can extract the value of the model coefficients for subsequent calculations as follows:

```{r}
# intercept
a <- as.numeric(tidied[1,5])

# slope 
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```


### Augmented outputs 

Finally let's take a look at the outputs of the `augment` function: 

```{r FAES-lm-augmented}
# storing because we'll use it later on. 

augmented <- calCurve %>% 
  unnest(augmented)

# DT is to make interactive tables for the book.
DT::datatable(augmented, 
              options = list(scrollX = TRUE))

```
As you can see, `augment()` adds columns to the original data that was modelled. For our purposes we're interested:

  - `signal` and `conc_Na`, the original data used in our model.
  - `.fitted`, the predicted value of the point according to our calculated model. 
  - `.resid`, the residuals of that point (different between measured and fitted values.)
  - The other parameters are different measurements of the influence of each point on the model fitting. They can be used to detect outliers; see [Further reading] 

### Why we approach modelling this way

You may be wondering why we've seemingly overcomplicated a simple enough procedure. Fair enough, we've showcased an analysis with a simple data set. However, as you progress in your studies you'll be quantifying *many* compounds, often at the same time in the same instrument runs. If you organize your data in a tidy format, you can plot calibration curves for *all* of your compounds with the same block of code. Essentially you use `group_by()` to group your data by compound/element. Subsequently, the same code is expandable from 1 compound to as many as you can ever hope to quantify in one shot. So for upper year labs where you're analyzing tens of compounds (*cough* CHM410 Dust Lab) you can generate calibration curves for all your compounds at once.

## Visualizing models

At the top of the chapter we plotted out standards to visualize a linear trend. Visualizations is an essential component when calculating calibration curves, and indeed our standards appeared to follow a linear trend, which was corroborated by the model we calculated above. However, for publications/reports you'll need to create a plot with both your standards *and* model with the displayed equation, so below is a bit of stock code you can use as a starting point to create these plots. Note that it requires the `ggpmisc` package to display the equation:

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 



```

### Vizualizing residuals

As stated earlier, residuals are the difference between measured and fitted values. They're often overlooked in linear calibration and folks are hot to plot a straight line through their data. This has the unintended effect of fooling your eyes into thinking your data is linear. Consequently, it is always a good idea to plot the residuals of your model as this will magnify any trends or discrepancies of your calibration model. A good linear model will have the residuals randomly distributed about zero. Other examples of patterns in residuals are shown below: 

```{r, echo = FALSE, warnin = FALSE, message = FALSE, fig.cap = "Example residual patterns; figure adapted from Hibbert and Gooding (2006). "}

residData <- data.frame("x" = c(1,2,3,4,5,6,7,8,9,10),
                        "good" = c(0.4, 0.01, -0.35, 0.2, 0.05,
                                   0.3, 0.07, 0.21, -0.5, 0.5),
                        "invertedU" = c(-4, -1, 1, 2, 3, 5, 3.5, 1.9, 0.8, -0.8),
                        "hetero" = c(-0.5, 0.7, -1, 1.5, 2.1, -3, 3.5, -4, 4.5, -5),
                        "outlier" = c(1, 1.5, 0.9, 1.6, -10, 0.7, 2, 1.1, 0.95, 1.2)
                        )

p <- ggplot(data = residData) +
  labs(x = "concentration", 
       y = "residuals") + 
  geom_hline(yintercept =  0) +
  theme_classic() 
  
  
normal <- p + 
  geom_point(aes(x = x, y = good)) +
  labs(subtitle = "Normally distributed residuals")

invertedU <- p + 
  geom_point(aes(x = x, y = invertedU)) +
  labs(subtitle = "Curvature throughout range")

hetero <- p + 
  geom_point(aes(x = x, y = hetero)) +
  labs(subtitle = "Heteroscedasticity with \nincreasing concentration")

outlier <- p + 
  geom_point(aes(x = x, y = outlier)) +
  labs(subtitle = "Outlier")


gridExtra::grid.arrange(normal, invertedU, hetero, outlier, ncol = 2, nrow = 2)
```

- Normally distributed residuals is satisfactory for linear modelling. Note the relatively small magnitude of the residuals. 
- Curvature throughout range results from an instrument become saturated. Consequently, the linear model will 'cut' through the curve. This is a good indication that you'll need to either breakdown your calibration curve into two or more parts or utilize a non-linear model. 
- Heteroscedasticity means the variance of the response is proportional to the concentration. This is often the case in instrumental analysis. See [Weighing] below. 
- Outliers shouldn't exist in your calibration plot, nevertheless, a plot of residuals can readily highlight an outlier point. 

### Plotting residuals

To plot residuals, we use our `augmented` dataset from above, and simply create a plot of the independent variable vs. the residuals. Here we plot the FAES calibration model and it's residuals. Note that the residuals indicate curvature throughout the range. We may have over extended our calibration range *outside* of the linear range of our instrument. 

```{r, fig.cap = "(A) linear calibration model and (B) plot of model residuals."}
a <- ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 

b <- ggplot(data = augmented, 
       aes(x = conc_Na, y = .resid)) +
  geom_point()

ggpubr::ggarrange(a, b, ncol = 2, labels = c("A", "B"))
```



## Calculating Concentrations

In [Tidied outputs] we extracted our model coefficients (slope and intercept): 

```{r}

# intercept
a <- as.numeric(tidied[1,5])

# slope
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```

Now that we have our coefficients, we can calculate the sample concentration as described in [Modelling Theory]. Let's import the FAES unknown dataset first:

```{r}
FAESsamples <- read_csv(file = "data/FAESUnknowns.csv") %>%
  pivot_longer(cols = -c(sample, `dilution factor`), 
               names_to = "replicate",
               names_prefix = "reading_",
               values_to = "signal")
FAESsamples

```

Now it's simply a matter of calculating the concentration of the sample analyzed by the instrument, and correcting for the dilution factor to find the concentration in the parent sample:

```{r}
FAESsamples <- FAESsamples %>%
  mutate("instConc" = (signal - a)/b,
         "sampleConc" = instConc * `dilution factor`)

DT::datatable(FAESsamples, options = list(scrollX = TRUE))
```

And let's summarize our results using code from the [Summarizing data] chapter:

```{r}
FAESsamples %>%
  group_by(sample) %>%
  summarize(mean = mean(sampleConc),
            sd = sd(sampleConc),
            n = n())
```
And there we go. See the [Summarizing data] and [Visualizations] chapters for assiting an making prettier tables and visualizations, respectively. 



## Summary

In this chapter we've covered: 

  - The essential theory undergirding the linear calibration model.
  - How to scallably model in R with a consistent workflow for one or thousands of linear calibration models. 
  - How to extract model parameters using the `tidy()`, `glance()`, and `augment()` functions from the `broom` package. 
  - A brief overview of model parameters as they pertain to linear calibration. 
  - Viasualizing models and the importance of visualizing residuals. 
  - Calculating sample concentration from model outputs. 
  - Running multiple linear regressions.
  - Storing regression

## Further reading

As previously stated, we highly recommend reading Chapter 5: Calibration from *Data Analysis for Chemistry* by Hibbert and Gooding for a more in-depth discussion of linear calibration modelling. The book can be accessed online via the University of Toronto's Library. 

For a greater discussion on modelling in R, see [Modelling](https://r4ds.had.co.nz/model-intro.html) in [*R for Data Science*](https://r4ds.had.co.nz/index.html).


## Going deeper with modelling

### Weighing 


As shown above, you'll often find that your calibration data is *heteroscedastic*, meaning the variance increases with the concentration. This leads to *leverage* of your line-of-best fit, as it is 'pulled' by one way or another by the higher concentration standards then the lower. You can assign 'weights' (how much a point impacts the model) in R, although you'll need to justify the validity of your approach. A common approach however, is to weight each standard by $\frac{1}{x^2}$. This ensures that samples with higher concentration impact the line less, and vice-versa with low-concentration standards. 

To utilize weight in R, we need to calculate the weigh prior to modelling, and subsequently specify the weights column 

```{r}

# note that our blank has a concentration of 0, hence infinite weight. 
# we need to remove it to weight our data. 

FAESweighed <- FAES %>% 
  filter(conc_Na > 0) %>%
  mutate(wght = 1/(conc_Na^2))

weightedCalCurve <- FAESweighed %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x, weights = wght)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         ) %>%
  unnest(augmented)

ggplot(data = weightedCalCurve, 
       aes(x = conc_Na, y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F, colour = "red", label = "unweighed") +
  geom_smooth(method = 'lm', se=F, colour = "blue", 
              aes(weight=`(weights)`, label = "weighed")) 
  
```

```{r child='src/common/end-of-chapter-exercise.Rmd'}
```


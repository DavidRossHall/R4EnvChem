# (PART\*) Part 5: Modelling in R {.unnumbered}

# Modelling: Linear Regression

Modelling is basically math used to describe some type of system, and they are a forte of R, a language tailor made for statistical computing. Every model has assumptions, limitations, and all around tricky bits to working. There is no shortage of modelling in a myriad of contexts, but in this chapter we'll discuss and break down the most common model you'll encounter, the *linear regression model*, in the most common context, the *linear calibration model*, using the most common function, `lm()`.

You have probably encountered the linear regression model under the pseudonym "trend-lines", most likely generated by *Excel*'s "add trend-line option" (as in CHM135). While the models we'll be constructing with `lm()` work much the same *mathematically*, unlike *Excel*, R returns *all* of the model outputs. Correspondingly, it's easy to get lost between juggling R code, the seemingly endless model outputs, and keeping yourself grounded in the real systems you're attempting to model.

## Modelling Theory

The *linear calibration model* relates the response of an instrument to the value of the *measurand*. The measurand is simply what we're measuring, often the concentration of an analyte. So we use the measurand, which we can control via preparation of standards from reference material as the *independent* variable, with the instrument output being the *dependent* variable (as instrument response varies with concentration). Altogether we're:

1.  Measuring the instrument response of standards of known concentration and samples of unknowns concentration.
2.  Calculating the linear calibration model (i.e. line of best fit) through our standards.
3.  Using the measurement model to calculate the concentration in our unknown from their respective instrument response.

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
library(tidyverse)
df <- data.frame(conc = c(0, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8),
                 abs = c(0, 0.057, 0.119, 0.261, 0.353, 0.599, 0.730))

df <- df %>%
  nest() %>%
  mutate(fit = map(data, ~lm(abs ~ conc, data = .x)),
         tidied = map(fit, broom::tidy),
         glanced = map(fit, broom::glance),
         augmented = map(fit, broom::augment)
         ) %>%
  unnest(augmented)
```

```{r reg-plot, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Linear calibration model; figure modified from Hibbert and Gooding (2006)."}
ggplot(df, 
       aes( x = conc, y = abs)) +
  geom_point() +
  
  # for residuals
  geom_smooth(method = lm, se = FALSE) +
  
  # actual measurement
  geom_segment(aes(xend = conc, yend = .fitted)) +
  
  # y4 measurement
  geom_segment(aes(x = 0, y = 0.261, xend = 0.20, yend = 0.261),
               alpha = 0.6, linetype = "dashed") +
  annotate("text", x = 0.025, y = 0.281, label = expression(y[4] )) +

  # y4 fitted
  geom_segment(aes(x = 0, y = 0.20391462, xend = 0.20, yend = 0.20391462),
               alpha = 0.6, linetype = "dashed") +
    annotate("text", x = 0.025, y = 0.23, label = expression(widehat(y)[4])) +

  # residual annotation
  annotate("text", x = 0.25, y = 0.1, label = "This is a\n residual", hjust = 1) +
  geom_curve(aes(x = 0.24, y = 0.15, xend = 0.205, yend = 0.245),
              curvature = 0.5, arrow = arrow(length = unit(0.03, "npc"))) +

  # measured point annotation
  annotate("text", x = 0.5, y = 0.3, label = "This is the \n measurement \n of a standard", hjust = 0) +
  geom_curve(aes(x = 0.5, y = 0.3, xend = 0.4, yend = 0.33),
              curvature = -0.7, arrow = arrow(length = unit(0.03, "npc"))) +

  # fitted line annotation
  annotate("text", x = 0.65, y = 0.45, label = "This is the \n calibration line") +
  geom_curve(aes(x = 0.65, y = 0.5, xend = 0.65, yend = 0.6),
              curvature = 0, arrow = arrow(length = unit(0.03, "npc"))) +

    # unknown inst. response annotation
  geom_point(aes(x = 0, y = 0.35), colour = "red", shape = "square") +
  geom_curve(aes(x = 0, y = 0.35, xend = 0.359, yend = 0.35),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash", colour = "red") +
  annotate("text", x = 0.05, y = 0.5, label = "Response of\n unknown", hjust = 0.25) +
  geom_curve(aes(x = 0.05, y = 0.45, xend = 0.005, yend = 0.36), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # unknown calculated conc
  geom_curve(aes(x = 0.359, y = 0.35, xend = 0.359, yend = 0),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash",
            colour = "red") + 
  annotate("text", x = 0.4, y = 0.1, label = "Concentration of\n unknown", hjust = 0) +
  geom_curve(aes(x = 0.4, y = 0.1, xend = 0.365, yend = 0.01), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # so residual lines touch y-axis
  scale_x_continuous(expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  labs(x = "concentration",
       y = "instrument response") +
  theme_classic() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank())
```

Before we can calculate concentrations, we need a measurement model. In other words, an equation that relates instrument response to sample concentration (or other factors). For simple linear calibration, we use:

$$y = a + bx$$
Where:

-   $y$ is the instrument response
-   $x$ is the independent variable (i.e. sample concentration)
-   $a$ and $b$ are the coefficients of the model; otherwise known as intercept and slope, respectively.

We'll gloss over some of the more technical aspects of modelling, and discuss other in more detail below. For now, know that:

-   We're assuming our linear model is correct (i.e. the instruments actually respond linearly to concentration).
-   All uncertainties reside in the dependent variable $y$ (i.e., no errors in preparation of the standards).
-   The values of $a$ and $b$ are determined by minimizing the *sum of the residuals squared*.
    -   The residuals are the difference between the actual measured response and where it would be if it were on the calibration line.

Once we have our line of best fit, we can calculate the concentration of our unknown sample $i$, from its measured response $y_i$ by:

$$x_i = \frac{y_i - b}{a}$$

There is more going on under the hood than what we're describing here, but this should be enough to get you up and running. If you would like a greater breakdown of linear calibration modelling, we suggest you read Chapter 5 of *Data Analysis for Chemistry* by Hibbert and Gooding. An online version is accessible via the University of Toronto's Library. Also there is no reason the instrument response must be linear. In fact, we spend a great deal of time arranging our experiment to that we land in the 'linear range'. For details on non-linear modelling in R see [Modelling: Non-Linear Regression].

## Linear Regression in R

Now that we have a rough understanding of what we're trying to do, let's go over *how* to calculate linear regression models in R. Note model is a general term, in this situation we'll be calculating a **calibration curve**. All calibration curves are models, but not all models are calibration curves.

For our example dataset we'll import a dataset consisting of four analytical standards of sodium plus a calibration blank all run in triplicate. The standards were measured via flame atomic emission spectroscopy (FAES). Let's import the FAES calibration results we saw in [Transform: Data manipulation]. As we've already seen, our data is composed of four standards and a blank analyzed in triplicate. Since we're focusing on modelling, *we'll treat the blank as a standard in our model fitting*. So let's import our dataset:

```{r, message = FALSE}
FAES <- read_csv(file = "data/FAES_original_wide.csv") %>%
  pivot_longer(cols = -std_Na_conc,
               names_to = "replicate", 
               names_prefix = "reading_",
               values_to = "signal") %>%
  separate(col = std_Na_conc,
           into = c("type", "conc_Na", "units"),
           sep = " ",
           convert = TRUE) %>%
  mutate(type = "standard")

DT::datatable(FAES)
```

And let's quickly plot our data. You should always visualize your data before modelling, especially for linear calibration modelling. Visualizing your data is an easy way to spot trends and gross errors in your data.

```{r}
ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point()
```

Let's use the `lm()` function to calculate the linear relationship between the `signal` as a function of `conc_Na`:

```{r}
lm_fit <- lm(signal ~ conc_Na, data = FAES)
lm_fit
```

Reading the code above:

1.  We're using the FAES data we created earlier: `data = FAES`.
2.  We're comparing `signal` (the dependent variable) to `conc_Na` (the independent variable) via the tilde `~`. The way to read this is: *"signal depends on concentration"*.
3.  We're fitting a *linear model* for comparing these two variables.
4.  The model's outputs are stored in the `lm_fit` variable.

As we can see, the model outputs are pretty brief and not much more than *Excel*'s outputs. We can use `summary()` to extract more information to better understand our model:

```{r}
summary(lm_fit)
```

Now we have a lot more information from our model, including the slope and intercept with their corresponding standard error as well as the \( R^2 \) value.

## Building Multiple Linear Regressions

In this section, we will delve into the world of multiple linear regressions, where we extend our analysis beyond single analytes to situation that arises often where you have mixed calibration standards that contain multiple analytes. 

### Dataset

The dataset `metal_conc` contains information on metal concentrations measured in parts per million, or ppm, and corresponding signal values (counts per seconds, or cps) from the inductively coupled plasma optical emission spectrometer (ICP-OES) in ANALEST for 4 metals: Aluminum, Magnesium, Calcium, and Potassium. Take a look at the data before proceeding with our analysis:

```{r}
metal_conc <- read_csv("data/metal_concentration.csv")
DT::datatable(metal_conc)
```

### Building a Linear Regression for Each Analyte

Now, we will build separate linear regression models for each metal to predict signal values based on metal concentrations.

```{r}
# Create a list to store slope and intercept for each metal
metal_results <- list()

# Iterate over each metal
for (metal in colnames(metal_conc)[-1]) {
  # Build linear regression model
  model <- lm(metal_conc[[metal]] ~ Concentration, data = metal_conc)
  
  # Extract slope and intercept
  slope <- coef(model)[2]
  intercept <- coef(model)[1]
  r_squared <- summary(model)$r.squared

  # Print slope and intercept
  cat("For:", metal, "\n")
  cat("Slope:", slope, "\n")
  cat("Intercept:", intercept, "\n\n")
  
  # Store slope and intercept in metal_results list
  metal_results[[metal]] <- list(slope = slope, intercept = intercept, r2=r_squared)
}
```

The code above uses a `for` loop, that was discussed in the [Loops] section of this resource. The loop will move through each metal in the dataset and then store the slope, intercept and \( R^2 \) values in a table called `metal_results`. After constructing linear regression models for each metal, visualizing these results can provide valuable insights into the relationships between metal concentrations and signal values. Below, we will create a single scatterplot for four selected metals along with their regression lines. This visualization will help in assessing the fit of the models and understanding the variance in signal values as a function of concentration.

```{r}
# Use pivot_longer to reshape the data frame for ggplot
metal_conc_long <- pivot_longer(metal_conc,
                                cols = c("Al", "Mg", "Ca", "K"),
                                names_to = "Metal",
                                values_to = "Signal")

# Create a scatterplot for each metal with its regression line
ggplot(metal_conc_long, aes(x = Concentration, y = Signal, color = Metal)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, aes(color = Metal)) +
  labs(title = "Regression Lines for Selected Metals", x = "Concentration", y = "Signal") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue", "green", "purple"))
```

To make this plot we used many of the skills we have learned throughout this resource such as the `pivot_longer` function in [Making Data Longer] and the functionality of `ggplot()` in [Ggplot Basic Visualizations].

### Table with Slopes, Intercepts, and \( R^2 \)  

We have successfully built linear regression models for each metal, and the slope, intercept, and \( R^2 \) values have been stored in the `metal_results` list. This variable is a list containing key values for each metal, where the metal name serves as the key, and the corresponding regression statistics—slope, intercept, and \( R^2 \)—are stored as sublists. To visualize and use the data stored in `metal_results`, we can create a dataframe called `metal_results_df` that includes each metal alongside its slope, intercept, and \( R^2 \), offering a clear overview of the regression outcomes.

```{r, echo=FALSE}
# Display metal_results variable
metal_results_df <- data.frame(Metal = names(metal_results),
                               Slope = sapply(metal_results, function(x) x$slope),
                               Intercept = sapply(metal_results, function(x) x$intercept),
                               R2 = sapply(metal_results, function(x) x$r2))
metal_results_df
```

To access the slope and intercept variables for a specific metal from the `metal_results` list, we can use the dollar sign notation (`$`). For instance, to retrieve the slope and intercept for calcium (Ca), we can use the syntax `metal_results$Ca$slope` and `metal_results$Ca$intercept`, respectively. This allows us to directly access the slope and intercept values associated with the metal of interest. The following code snippet illustrates how to access these variables:

```{r}
# Access slope and intercept for calcium (Ca)
slope_ca <- metal_results$Ca$slope
intercept_ca <- metal_results$Ca$intercept

# Print the slope and intercept values
print("Slope for Ca:", slope_ca, "\n")
cat("Intercept for Ca:", intercept_ca, "\n")
```


## Calculating Concentration Using the Regression Variables

To calculate the concentration of the metal ions from the provided signal values (in counts per second, cps), we can use the slope and intercept values obtained from the linear regression models. 

Take a look at the following signal values that we don't know the concentrations of:
```{r}
# Read the metal signal data
metal_signals <- read_csv("data/metal_signals.csv", show_col_types = FALSE)
DT::datatable(metal_signals)
```

Let's assume that the linear regression models were built using the concentration of each metal as the independent variable and the signal values as the dependent variable. Here's how we can calculate the concentration for each metal:

```{r}
# Function to calculate concentration from signal values
calculate_concentration <- function(signal, metal_name) {
  # Extract slope and intercept for the specified metal
  slope <- metal_results[[metal_name]]$slope
  intercept <- metal_results[[metal_name]]$intercept
  
  # Calculate concentration using the linear regression equation: concentration = (signal - intercept) / slope
  concentration <- (signal - intercept) / slope
  
  return(concentration)
}

# Apply the calculate_concentration function to each metal column in the dataset
metal_signals$Al_concentration <- calculate_concentration(metal_signals$Al, "Al")
metal_signals$Mg_concentration <- calculate_concentration(metal_signals$Mg, "Mg")
metal_signals$Ca_concentration <- calculate_concentration(metal_signals$Ca, "Ca")
metal_signals$K_concentration <- calculate_concentration(metal_signals$K, "K")

# Print the updated dataset with calculated concentrations (Aluminum as the example)
DT::datatable(metal_signals)
```

*Note*: In the provided code snippet, the use of `[[]]` (double brackets) allows for dynamic access to list elements using a variable name. This is particularly useful when the exact name of the element is stored as a string in another variable, such as `metal_name`. The double brackets evaluate the variable to get its value, thereby accessing the corresponding list element. This method is distinct from the use of `$`, which requires a direct, literal name of the element and cannot interpret a variable's value as an element name. 

Let's take a closer look at one of the analytes' calculated concentrations:

```{r, echo=FALSE}
DT::datatable(metal_signals[, c("Ca", "Ca_concentration")])
```

Do you see anything odd about the calculated concentrations of Calcium in the above table? 

Note that negative concentration values can arise when signal values fall below the intercept of the linear regression model, indicating a deviation from the model's assumptions. This situation typically occurs at lower signal levels, where instrument noise or background signal may influence measurements. Negative concentrations lack meaningful interpretation in chemical analysis and are often considered artifacts of the model's limitations. To address this issue, we could establish a minimum detectable concentration threshold, treating signal values below this threshold as non-detects or zero values. Alternatively, quadratic or logarithmic regression models may better capture non-linear instrument responses, enabling accurate concentration determination across a wider dynamic range. And the great news is, we'll see how to do these non-linear regressions in the next chapter!


## Conclusion

In conclusion, while multiple linear regression serves as a powerful tool for analyzing the relationship between chemical concentrations and instrumental signals, its applicability may be limited by the assumption of linearity. Negative concentration values observed in linear regression analyses highlight the need for robust statistical techniques and critical evaluation of instrument responses. As we move forward into the realm of non-linear regression in the subsequent chapter, we explore alternative modeling approaches that can better accommodate non-linear relationships between variables. By embracing the versatility of non-linear regression techniques, we expand our analytical toolkit, allowing for more accurate and comprehensive analysis of complex chemical systems. Through a combination of linear and non-linear regression methods, chemists can unlock deeper insights into the quantitative relationships governing chemical phenomena, paving the way for advancements in analytical chemistry and beyond.


## Further reading {#further_reading_chapter19}

As previously stated, we highly recommend reading Chapter 5: Calibration from *Data Analysis for Chemistry* by Hibbert and Gooding for a more in-depth discussion of linear calibration modelling. The book can be accessed online via the University of Toronto's Library.

For a greater discussion on modelling in R, see [Modelling](https://r4ds.had.co.nz/model-intro.html) in [*R for Data Science*](https://r4ds.had.co.nz/index.html).

```{r child='src/common/end-of-chapter-exercise.Rmd'}
```

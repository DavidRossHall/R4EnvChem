# Augmented Outputs and Evaluating Your Model

In this section we'll discuss how to get more out of the model outputs and how you can evaluate the goodness of your fit beyond the use of $R^2$.

## Cleaning up Model Ouputs

The `summary()` function provides a decent overview of our model's performance, but the outputs are difficult to work with. Let's turn to the `broom()` package to clean up our model outputs.

```{r FAES-calCurves}
library(broom)

calCurve <- FAES %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         )
calCurve
```

Things look a bit more complicated than our earlier example, so let's break down our code line by line:

1.  We're taking the `FAES` dataset that we created earlier.
2.  `group_by(type)` groups all rows by `type`, in this situation we have only one type: `standard`.
3.  `nest()` collapses everything other than the `type` column into smaller data-frames. In this situation, all other information is stored as a `tibble` under the `data` column; this is the data used to calculate the linear model.
4.  With the `mutate` function, we've created four columns: `fit`, `tidied`, `glanced`, and `augmented`.

It's these columns that contain our cleaned up model outputs. `fit` contains the raw output from the linear regression model for `signal` as a function of `conc_Na` using the `lm()` function. The output is in the form of a list, similar to what `summary()` gave us above. Again, this is exceptionally messy, hence why we used the `tidy()`, `glance()`, and `augment()` functions from the `broom` package . `map()` just means we're applying the function `tidy()` to the individual output list created by `lm()` and stored in the `fit` column. Note that the `tidy()`, `glanced()`, and `augmented` outputs are tibbles. So we now have a tibble containing specific model output values (i.e. `(Intercept)`), lists (i.e. `fit`), and tibbles (`tidied`). This is known as \*\*nested data\*. We're no longer in Kansas anymore...

We'll break down what each function did below. Keep in mind however that `lm()` is used for a variety of statistical tests, and consequently has many associated outputs. Some are essential, others are useful, and some are useless for linear calibration. There are also many ways to use these additional model outputs to calculate outliers, etc. but you shouldn't have any outliers in your calibration model. Don't rely on statistics to bail you out of poor chemical technique.

### Glanced Outputs

Let's take a look at our model results. The `glanced` tibble contains "...a concise one-row summary of the model. This typically contains values such as R\^2, adjusted R\^2, and residual standard error that are computed once for the entire model"[^02-multiple-linear-models-1] Because the data is nested, we'll need to use `unnest()` to flatten it back out into regular columns:

[^02-multiple-linear-models-1]: From the *broom* package vignette.

```{r FAES-lm-Glanced}
glanced <- calCurve %>%
  unnest(glanced)

# DT is to make interactive tables for the book.
DT::datatable(glanced, 
              options = list(scrollX = TRUE))
```

What you see here is a bit more than what you'd get from *Excel*'s 'line-of-best fit' output. In brief, :

-   `type`, `data`, `fit`, `tidied`, and `augmented` are columns we've created earlier.
-   `r.squared` is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable. The closer `r.squared` is to 1, the more variance is captured by the model.
-   `adj.r.squared` is the same as `r.squared` in this situation. This is because `r.squared` will always increase if we add more exploratory variables to our model; the `adj.r.squared` accounts for the number of exploratory variables used in the model.
    -   In our case we only have one exploratory variable, hence they're approximately the same.
-   The other columns are different measurements of goodness-of-fit and hypothesis testing of the model. See [Further reading](#further_reading_chapter19).

### Tidied Outputs

But what about the slope and the intercept? After all, that's what we need to calculate the concentration in our unknowns. Let's take a look at `tidied` from the `tidy()` function which constructs a tibble that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression:[^02-multiple-linear-models-2]

[^02-multiple-linear-models-2]: From the *broom* package vignette.

```{r FAES-lm-tidied}
# storing because we'll use it later on. 

tidied <- calCurve %>% 
  unnest(tidied)

# DT is to make interactive tables for the book.
DT::datatable(tidied, 
              options = list(scrollX = TRUE))

```

Again, a lot more to unpack compared to *Excel*. That's because the `lm()` function in R calculates a generalized linear model. `lm()` performs a linear regression model, which we normally think of as an equation of the form $y= a + bx$ as discussed earlier. But, regression models can be expanded to account for multiple variables (hence *multiple linear regression*) of the form:

$$y = \beta _{0} + \beta _{1} x_{1} + \beta _{2} x_{2} ... \beta _{p} x_{p}$$

where,

-   $y$ = dependent variable
-   $x$ = exploratory variable; there's no limit how many you can input
-   $\beta _{0}$ = y-intercept (constant term)
-   $\beta _{p}$ = slope coefficient for each explanatory variable

With our linear calibration model, we only have one input variable for our model (`conc`), so the above formula collapses down to $y = \beta _{0} + \beta _{1} x_{1}$. So looking at our tidied model outputs:

-   each row corresponds to a model coefficient (under the `term` column).
-   For each modelling parameter, we're provided an estimate of its numerical value: `estimate`. These are the values we'll use to calculate concentration.
-   `std.error` measures how precisely the model estimates the coefficient's unknown value; smaller is better.
-   `p.value` is an indication of the significance of a model coefficient; the closer to zero the better.
    -   If we were to use multiple parameters in our model (e.g. concentration and temperature) we could use the `p.value` to determine if a given coefficient was useful for our model.

We can extract the value of the model coefficients for subsequent calculations as follows:

```{r}
# intercept
a <- as.numeric(tidied[1,5])

# slope 
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```

### Augmented Outputs

Finally, let's take a look at the outputs of the `augment` function:

```{r FAES-lm-augmented}
# storing because we'll use it later on. 

augmented <- calCurve %>% 
  unnest(augmented)

# DT is to make interactive tables for the book.
DT::datatable(augmented, 
              options = list(scrollX = TRUE))

```

As you can see, `augment()` adds columns to the original data that was modelled. For our purposes we're interested:

-   `signal` and `conc_Na`, the original data used in our model.
-   `.fitted`, the predicted value of the point according to our calculated model.
-   `.resid`, the residuals of that point (different between measured and fitted values.)
-   The other parameters are different measurements of the influence of each point on the model fitting. They can be used to detect outliers; see [Further reading](#further_reading_chapter19)

## Incorporating Weights into your Model

You'll often find that your calibration data is *heteroscedastic*, meaning the variance increases with the concentration. This leads to *leverage* of your line-of-best fit, as it is 'pulled' by one way or another by the higher concentration standards than the lower. You can assign 'weights' (how much a point impacts the model) in R, although you'll need to justify the validity of your approach. A common approach, however, is to weight each standard by $\frac{1}{x^2}$. This ensures that samples with higher concentration impact the line less, and vice-versa with low-concentration standards.

To utilize weight in R, we need to calculate the weight prior to modelling, and subsequently specify the weights column

```{r}

# note that our blank has a concentration of 0, hence infinite weight. 
# we need to remove it to weight our data. 

FAESweighed <- FAES %>% 
  filter(conc_Na > 0) %>%
  mutate(wght = 1/(conc_Na^2))

weightedCalCurve <- FAESweighed %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x, weights = wght)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         ) %>%
  unnest(augmented)

ggplot(data = weightedCalCurve, 
       aes(x = conc_Na, y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F, colour = "red", label = "unweighed") +
  geom_smooth(method = 'lm', se=F, colour = "blue", 
              aes(weight=`(weights)`, label = "weighed")) 
  
```

### Why we approach modelling this way

You may be wondering why we've seemingly overcomplicated a simple enough procedure. Fair enough, we've showcased an analysis with a simple data set. However, as you progress in your studies you'll be quantifying *many* compounds, often at the same time in the same instrument runs. If you organize your data in a tidy format, you can plot calibration curves for *all* of your compounds with the same block of code. Essentially you use `group_by()` to group your data by compound/element. Subsequently, the same code is expandable from 1 compound to as many as you can ever hope to quantify in one shot. So for upper year labs where you're analyzing tens of compounds (*cough* the CHM410 labs) you can generate calibration curves for all your compounds at once.

## Visualizing models

At the top of the chapter we plotted out standards to visualize a linear trend. Visualization is an essential component when calculating calibration curves, and indeed our standards appeared to follow a linear trend, which was corroborated by the model we calculated above. However, for publications/reports you'll need to create a plot with both your standards *and* model with the displayed equation, so below is a bit of stock code you can use as a starting point to create these plots. Note that it requires the `ggpmisc` package to display the equation:

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 



```

### Visualizing residuals

As discussed in the [Modelling Theory] section, residuals are the difference between measured and fitted values. They're often overlooked in linear calibration and folks are hot to plot a straight line through their data. This has the unintended effect of fooling your eyes into thinking your data is linear. Consequently, it is always a good idea to plot the residuals of your model as this will magnify any trends or discrepancies of your calibration model. A good linear model will have the residuals randomly distributed about zero. Other examples of patterns in residuals are shown below:

```{r, echo = FALSE, warnin = FALSE, message = FALSE, fig.cap = "Example residual patterns; figure adapted from Hibbert and Gooding (2006). "}

residData <- data.frame("x" = c(1,2,3,4,5,6,7,8,9,10),
                        "good" = c(0.4, 0.01, -0.35, 0.2, 0.05,
                                   0.3, 0.07, 0.21, -0.5, 0.5),
                        "invertedU" = c(-4, -1, 1, 2, 3, 5, 3.5, 1.9, 0.8, -0.8),
                        "hetero" = c(-0.5, 0.7, -1, 1.5, 2.1, -3, 3.5, -4, 4.5, -5),
                        "outlier" = c(1, 1.5, 0.9, 1.6, -10, 0.7, 2, 1.1, 0.95, 1.2)
                        )

p <- ggplot(data = residData) +
  labs(x = "concentration", 
       y = "residuals") + 
  geom_hline(yintercept =  0) +
  theme_classic() 
  
  
normal <- p + 
  geom_point(aes(x = x, y = good)) +
  labs(subtitle = "Normally distributed residuals")

invertedU <- p + 
  geom_point(aes(x = x, y = invertedU)) +
  labs(subtitle = "Curvature throughout range")

hetero <- p + 
  geom_point(aes(x = x, y = hetero)) +
  labs(subtitle = "Heteroscedasticity with \nincreasing concentration")

outlier <- p + 
  geom_point(aes(x = x, y = outlier)) +
  labs(subtitle = "Outlier")


gridExtra::grid.arrange(normal, invertedU, hetero, outlier, ncol = 2, nrow = 2)
```

-   Normally distributed residuals are satisfactory for linear modelling. Note the relatively small magnitude of the residuals.
-   Curvature throughout range results from an instrument becoming saturated. Consequently, the linear model will 'cut' through the curve. This is a good indication that you'll need to either breakdown your calibration curve into two or more parts or utilize a non-linear model.
-   Heteroscedasticity means the variance of the response is proportional to the concentration. This is often the case in instrumental analysis. See [Weighing] above.
-   Outliers shouldn't exist in your calibration plot, nevertheless, a plot of residuals can readily highlight an outlier point.

### Plotting residuals

To plot residuals, we use our `augmented` dataset from above, and simply create a plot of the independent variable vs. the residuals. Here we plot the FAES calibration model and its residuals. Note that the residuals indicate curvature throughout the range. We may have overextended our calibration range *outside* of the linear range of our instrument.

```{r, fig.cap = "(A) linear calibration model and (B) plot of model residuals."}
a <- ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 

b <- ggplot(data = augmented, 
       aes(x = conc_Na, y = .resid)) +
  geom_point()

ggpubr::ggarrange(a, b, ncol = 2, labels = c("A", "B"))
```

## Calculating Concentrations from the Augmented Outputs

In [Tidied outputs] we extracted our model coefficients (slope and intercept):

```{r}

# intercept
a <- as.numeric(tidied[1,5])

# slope
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```

Now that we have our coefficients, we can calculate the sample concentration as described in [Modelling Theory]. Let's import the FAES unknown dataset first:

```{r}
FAESsamples <- read_csv(file = "data/FAESUnknowns.csv") %>%
  pivot_longer(cols = -c(sample, `dilution factor`), 
               names_to = "replicate",
               names_prefix = "reading_",
               values_to = "signal")
FAESsamples

```

Now it's simply a matter of calculating the concentration of the sample analyzed by the instrument, and correcting for the dilution factor to find the concentration in the parent sample:

```{r}
FAESsamples <- FAESsamples %>%
  mutate("instConc" = (signal - a)/b,
         "sampleConc" = instConc * `dilution factor`)

DT::datatable(FAESsamples, options = list(scrollX = TRUE))
```

And let's summarize our results using code from the [Summarizing data] chapter:

```{r}
FAESsamples %>%
  group_by(sample) %>%
  summarize(mean = mean(sampleConc),
            sd = sd(sampleConc),
            n = n())
```

And there we go. See the [Summarizing data] and [Visualizations for Env Chem] chapters for assisting in making prettier tables and visualizations, respectively.


```{r child='src/common/end-of-chapter-exercise.Rmd'}
```

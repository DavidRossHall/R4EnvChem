# Augmented Outputs and Evaluating Your Model

In this section we'll discuss how to get more out of the model outputs and how you can evaluate the goodness of your fit beyond the use of $R^2$.

## Cleaning up Model Ouputs

The `summary()` function provides a decent overview of our model's performance, but the outputs are difficult to work with. Let's turn to the `broom()` package to clean up our model outputs.

```{r FAES-calCurves}
library(broom)

calCurve <- FAES %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         )
calCurve
```

Things look a bit more complicated than our earlier example, so let's break down our code line by line:

1.  We're taking the `FAES` dataset that we created earlier.
2.  `group_by(type)` groups all rows by `type`, in this situation we have only one type: `standard`.
3.  `nest()` collapses everything other than the `type` column into smaller data-frames. In this situation, all other information is stored as a `tibble` under the `data` column; this is the data used to calculate the linear model.
4.  With the `mutate` function, we've created four columns: `fit`, `tidied`, `glanced`, and `augmented`.

It's these columns that contain our cleaned up model outputs. `fit` contains the raw output from the linear regression model for `signal` as a function of `conc_Na` using the `lm()` function. The output is in the form of a list, similar to what `summary()` gave us above. Again, this is exceptionally messy, hence why we used the `tidy()`, `glance()`, and `augment()` functions from the `broom` package . `map()` just means we're applying the function `tidy()` to the individual output list created by `lm()` and stored in the `fit` column. Note that the `tidy()`, `glanced()`, and `augmented` outputs are tibbles. So we now have a tibble containing specific model output values (i.e. `(Intercept)`), lists (i.e. `fit`), and tibbles (`tidied`). This is known as \*\*nested data\*. We're no longer in Kansas anymore...

We'll break down what each function did below. Keep in mind however that `lm()` is used for a variety of statistical tests, and consequently has many associated outputs. Some are essential, others are useful, and some are useless for linear calibration. There are also many ways to use these additional model outputs to calculate outliers, etc. but you shouldn't have any outliers in your calibration model. Don't rely on statistics to bail you out of poor chemical technique.

### Glanced Outputs

Let's take a look at our model results. The `glanced` tibble contains "...a concise one-row summary of the model. This typically contains values such as R\^2, adjusted R\^2, and residual standard error that are computed once for the entire model"[^02-multiple-linear-models-1] Because the data is nested, we'll need to use `unnest()` to flatten it back out into regular columns:

[^02-multiple-linear-models-1]: From the *broom* package vignette.

```{r FAES-lm-Glanced}
glanced <- calCurve %>%
  unnest(glanced)

# DT is to make interactive tables for the book.
DT::datatable(glanced, 
              options = list(scrollX = TRUE))
```

What you see here is a bit more than what you'd get from *Excel*'s 'line-of-best fit' output. In brief, :

-   `type`, `data`, `fit`, `tidied`, and `augmented` are columns we've created earlier.
-   `r.squared` is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable. The closer `r.squared` is to 1, the more variance is captured by the model.
-   `adj.r.squared` is the same as `r.squared` in this situation. This is because `r.squared` will always increase if we add more exploratory variables to our model; the `adj.r.squared` accounts for the number of exploratory variables used in the model.
    -   In our case we only have one exploratory variable, hence they're approximately the same.
-   The other columns are different measurements of goodness-of-fit and hypothesis testing of the model. See [Further reading](#further_reading_chapter19).

### Tidied Outputs

But what about the slope and the intercept? After all, that's what we need to calculate the concentration in our unknowns. Let's take a look at `tidied` from the `tidy()` function which constructs a tibble that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression:[^02-multiple-linear-models-2]

[^02-multiple-linear-models-2]: From the *broom* package vignette.

```{r FAES-lm-tidied}
# storing because we'll use it later on. 

tidied <- calCurve %>% 
  unnest(tidied)

# DT is to make interactive tables for the book.
DT::datatable(tidied, 
              options = list(scrollX = TRUE))

```

Again, a lot more to unpack compared to *Excel*. That's because the `lm()` function in R calculates a generalized linear model. `lm()` performs a linear regression model, which we normally think of as an equation of the form $y= a + bx$ as discussed earlier. But, regression models can be expanded to account for multiple variables (hence *multiple linear regression*) of the form:

$$y = \beta _{0} + \beta _{1} x_{1} + \beta _{2} x_{2} ... \beta _{p} x_{p}$$

where,

-   $y$ = dependent variable
-   $x$ = exploratory variable; there's no limit how many you can input
-   $\beta _{0}$ = y-intercept (constant term)
-   $\beta _{p}$ = slope coefficient for each explanatory variable

With our linear calibration model, we only have one input variable for our model (`conc`), so the above formula collapses down to $y = \beta _{0} + \beta _{1} x_{1}$. So looking at our tidied model outputs:

-   each row corresponds to a model coefficient (under the `term` column).
-   For each modelling parameter, we're provided an estimate of its numerical value: `estimate`. These are the values we'll use to calculate concentration.
-   `std.error` measures how precisely the model estimates the coefficient's unknown value; smaller is better.
-   `p.value` is an indication of the significance of a model coefficient; the closer to zero the better.
    -   If we were to use multiple parameters in our model (e.g. concentration and temperature) we could use the `p.value` to determine if a given coefficient was useful for our model.

We can extract the value of the model coefficients for subsequent calculations as follows:

```{r}
# intercept
a <- as.numeric(tidied[1,5])

# slope 
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```

### Augmented Outputs

Finally, let's take a look at the outputs of the `augment` function:

```{r FAES-lm-augmented}
# storing because we'll use it later on. 

augmented <- calCurve %>% 
  unnest(augmented)

# DT is to make interactive tables for the book.
DT::datatable(augmented, 
              options = list(scrollX = TRUE))

```

As you can see, `augment()` adds columns to the original data that was modelled. For our purposes we're interested:

-   `signal` and `conc_Na`, the original data used in our model.
-   `.fitted`, the predicted value of the point according to our calculated model.
-   `.resid`, the residuals of that point (different between measured and fitted values.)
-   The other parameters are different measurements of the influence of each point on the model fitting. They can be used to detect outliers; see [Further reading](#further_reading_chapter19)

## Incorporating Weights into your Model

You'll often find that your calibration data is *heteroscedastic*, meaning the variance increases with the concentration. This leads to *leverage* of your line-of-best fit, as it is 'pulled' by one way or another by the higher concentration standards than the lower. You can assign 'weights' (how much a point impacts the model) in R, although you'll need to justify the validity of your approach. A common approach, however, is to weight each standard by $\frac{1}{x^2}$. This ensures that samples with higher concentration impact the line less, and vice-versa with low-concentration standards.

To utilize weight in R, we need to calculate the weight prior to modelling, and subsequently specify the weights column

```{r}

# note that our blank has a concentration of 0, hence infinite weight. 
# we need to remove it to weight our data. 

FAESweighed <- FAES %>% 
  filter(conc_Na > 0) %>%
  mutate(wght = 1/(conc_Na^2))

weightedCalCurve <- FAESweighed %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x, weights = wght)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance),
         augmented = map(fit, augment)
         ) %>%
  unnest(augmented)

ggplot(data = weightedCalCurve, 
       aes(x = conc_Na, y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F, colour = "red", label = "unweighed") +
  geom_smooth(method = 'lm', se=F, colour = "blue", 
              aes(weight=`(weights)`, label = "weighed")) 
  
```

### Why we approach modelling this way

You may be wondering why we've seemingly overcomplicated a simple enough procedure. Fair enough, we've showcased an analysis with a simple data set. However, as you progress in your studies you'll be quantifying *many* compounds, often at the same time in the same instrument runs. If you organize your data in a tidy format, you can plot calibration curves for *all* of your compounds with the same block of code. Essentially you use `group_by()` to group your data by compound/element. Subsequently, the same code is expandable from 1 compound to as many as you can ever hope to quantify in one shot. So for upper year labs where you're analyzing tens of compounds (*cough* the CHM410 labs) you can generate calibration curves for all your compounds at once.

## Visualizing models

At the top of the chapter we plotted out standards to visualize a linear trend. Visualization is an essential component when calculating calibration curves, and indeed our standards appeared to follow a linear trend, which was corroborated by the model we calculated above. However, for publications/reports you'll need to create a plot with both your standards *and* model with the displayed equation, so below is a bit of stock code you can use as a starting point to create these plots. Note that it requires the `ggpmisc` package to display the equation:

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 



```

### Visualizing residuals

As discussed in the [Modelling Theory] section, residuals are the difference between measured and fitted values. They're often overlooked in linear calibration and folks are hot to plot a straight line through their data. This has the unintended effect of fooling your eyes into thinking your data is linear. Consequently, it is always a good idea to plot the residuals of your model as this will magnify any trends or discrepancies of your calibration model. A good linear model will have the residuals randomly distributed about zero. Other examples of patterns in residuals are shown below:

```{r, echo = FALSE, warnin = FALSE, message = FALSE, fig.cap = "Example residual patterns; figure adapted from Hibbert and Gooding (2006). "}

residData <- data.frame("x" = c(1,2,3,4,5,6,7,8,9,10),
                        "good" = c(0.4, 0.01, -0.35, 0.2, 0.05,
                                   0.3, 0.07, 0.21, -0.5, 0.5),
                        "invertedU" = c(-4, -1, 1, 2, 3, 5, 3.5, 1.9, 0.8, -0.8),
                        "hetero" = c(-0.5, 0.7, -1, 1.5, 2.1, -3, 3.5, -4, 4.5, -5),
                        "outlier" = c(1, 1.5, 0.9, 1.6, -10, 0.7, 2, 1.1, 0.95, 1.2)
                        )

p <- ggplot(data = residData) +
  labs(x = "concentration", 
       y = "residuals") + 
  geom_hline(yintercept =  0) +
  theme_classic() 
  
  
normal <- p + 
  geom_point(aes(x = x, y = good)) +
  labs(subtitle = "Normally distributed residuals")

invertedU <- p + 
  geom_point(aes(x = x, y = invertedU)) +
  labs(subtitle = "Curvature throughout range")

hetero <- p + 
  geom_point(aes(x = x, y = hetero)) +
  labs(subtitle = "Heteroscedasticity with \nincreasing concentration")

outlier <- p + 
  geom_point(aes(x = x, y = outlier)) +
  labs(subtitle = "Outlier")


gridExtra::grid.arrange(normal, invertedU, hetero, outlier, ncol = 2, nrow = 2)
```

-   Normally distributed residuals are satisfactory for linear modelling. Note the relatively small magnitude of the residuals.
-   Curvature throughout range results from an instrument becoming saturated. Consequently, the linear model will 'cut' through the curve. This is a good indication that you'll need to either breakdown your calibration curve into two or more parts or utilize a non-linear model.
-   Heteroscedasticity means the variance of the response is proportional to the concentration. This is often the case in instrumental analysis. See [Weighing] above.
-   Outliers shouldn't exist in your calibration plot, nevertheless, a plot of residuals can readily highlight an outlier point.

### Plotting residuals

To plot residuals, we use our `augmented` dataset from above, and simply create a plot of the independent variable vs. the residuals. Here we plot the FAES calibration model and its residuals. Note that the residuals indicate curvature throughout the range. We may have overextended our calibration range *outside* of the linear range of our instrument.

```{r, fig.cap = "(A) linear calibration model and (B) plot of model residuals."}
a <- ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 

b <- ggplot(data = augmented, 
       aes(x = conc_Na, y = .resid)) +
  geom_point()

ggpubr::ggarrange(a, b, ncol = 2, labels = c("A", "B"))
```

## Calculating Concentrations from the Augmented Outputs

In [Tidied outputs] we extracted our model coefficients (slope and intercept):

```{r}

# intercept
a <- as.numeric(tidied[1,5])

# slope
b <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", a, " + ", b,"x", sep="")

```

Now that we have our coefficients, we can calculate the sample concentration as described in [Modelling Theory]. Let's import the FAES unknown dataset first:

```{r}
FAESsamples <- read_csv(file = "data/FAESUnknowns.csv") %>%
  pivot_longer(cols = -c(sample, `dilution factor`), 
               names_to = "replicate",
               names_prefix = "reading_",
               values_to = "signal")
FAESsamples

```

Now it's simply a matter of calculating the concentration of the sample analyzed by the instrument, and correcting for the dilution factor to find the concentration in the parent sample:

```{r}
FAESsamples <- FAESsamples %>%
  mutate("instConc" = (signal - a)/b,
         "sampleConc" = instConc * `dilution factor`)

DT::datatable(FAESsamples, options = list(scrollX = TRUE))
```

And let's summarize our results using code from the [Summarizing data] chapter:

```{r}
FAESsamples %>%
  group_by(sample) %>%
  summarize(mean = mean(sampleConc),
            sd = sd(sampleConc),
            n = n())
```

And there we go. See the [Summarizing data] and [Visualizations for Env Chem] chapters for assisting in making prettier tables and visualizations, respectively.

## Summary

In this chapter we've covered:

-   The essential theory undergirding the linear calibration model.
-   How to scalably model in R with a consistent workflow for one or thousands of linear calibration models.
-   How to extract model parameters using the `tidy()`, `glance()`, and `augment()` functions from the `broom` package.
-   A brief overview of model parameters as they pertain to linear calibration.
-   Visualizing models and the importance of visualizing residuals.
-   Calculating sample concentration from model outputs.
-   Running multiple linear regressions.
-   Storing regression




In the realm of environmental chemistry, understanding the quantitative relationship between chemical concentrations and corresponding instrumental signals is paramount. While simple linear regression models offer insights into this relationship for individual analytes, the complexity of real-world chemical systems often necessitates more nuanced approaches.

In this chapter, we delve into the world of multiple linear regressions, where we extend our analysis beyond single analytes to simultaneously model the influence of multiple factors on observed signals. By leveraging the power of multiple linear regression, chemists gain a deeper understanding of the intricate interplay between chemical concentrations and instrumental responses, empowering them to make more informed decisions and extract richer insights from their analytical data. Join us as we embark on this journey to unlock the potential of multiple linear regressions in the realm of analytical chemistry!

## Building Multiple Linear Regressions

### Dataset

The dataset `metal_conc` contains information on metal concentrations measured (parts per million, or ppm) and corresponding signal values (counts per seconds, or cps) for 4 types of metals: Aluminum, Magnesium, Calcium, and Potassium. Take a look at the data before proceeding with our analysis:

```{r}
# Load the necessary libraries
library(readr)
library(dplyr)
library(DT)

# Read the CSV file
metal_conc <- read_csv("data/metal_concentration.csv", show_col_types = FALSE)
DT::datatable(metal_conc)
```

### Regressions

Now, we will build separate linear regression models for each metal to predict signal values based on metal concentrations.

```{r}
# Create a list to store slope and intercept for each metal
metal_results <- list()

# Iterate over each metal
for (metal in colnames(metal_conc)[-1]) {
  # Build linear regression model
  model <- lm(metal_conc[[metal]] ~ metal_conc[["Concentration"]], data = metal_conc)
  
  # Extract slope and intercept
  slope <- coef(model)[2]
  intercept <- coef(model)[1]
  r_squared <- summary(model)$r.squared

  
  # Print slope and intercept
  cat("For", metal, ":\n")
  cat("Slope:", slope, "\n")
  cat("Intercept:", intercept, "\n\n")
  
  
  # Store slope and intercept in metal_results list
  metal_results[[metal]] <- list(slope = slope, intercept = intercept, r2=r_squared)
}
```

After constructing linear regression models for each metal, visualizing these results can provide valuable insights into the relationships between metal concentrations and signal values. Below, we will create a single scatterplot for four selected metals along with their regression lines. This visualization will help in assessing the fit of the models and understanding the variance in signal values as a function of concentration.

```{r}
# Use pivot_longer to reshape the data frame for ggplot
metal_conc_long <- pivot_longer(metal_conc, cols = c("Al", "Mg", "Ca", "K"), names_to = "Metal", values_to = "Signal")

# Create a scatterplot for each metal with its regression line
ggplot(metal_conc_long, aes(x = Concentration, y = Signal, color = Metal)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, aes(color = Metal)) +
  labs(title = "Regression Lines for Selected Metals", x = "Concentration", y = "Signal") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue", "green", "purple"))

```

### Slopes and Intercepts

We have successfully built linear regression models for each metal, and the slope, intercept, and $R^2$ values have been stored in the `metal_results` list. This variable is a list containing key-value pairs for each metal, where the metal name serves as the key, and the corresponding regression statistics---slope, intercept, and $R^2$---are stored as sublists. To effectively visualize the comprehensive results stored in `metal_results`, we can display this information in a tabular format that includes each metal alongside its slope, intercept, and $R^2$, offering a clear overview of the regression outcomes.

```{r, echo=FALSE}
# Display metal_results variable
metal_results_df <- data.frame(Metal = names(metal_results),
                                Slope = sapply(metal_results, function(x) x$slope),
                                Intercept = sapply(metal_results, function(x) x$intercept),
                                R2 = sapply(metal_results, function(x) x$r2))
metal_results_df
```

To access the slope and intercept variables for a specific metal from the `metal_results` list, we can use the dollar sign notation (`$`). For instance, to retrieve the slope and intercept for calcium (Ca), we can use the syntax `metal_results$Ca$slope` and `metal_results$Ca$intercept`, respectively. This allows us to directly access the slope and intercept values associated with the metal of interest. The following code snippet illustrates how to access these variables:

```{r}
# Access slope and intercept for calcium (Ca)
slope_ca <- metal_results$Ca$slope
intercept_ca <- metal_results$Ca$intercept

# Print the slope and intercept values
cat("Slope for Ca:", slope_ca, "\n")
cat("Intercept for Ca:", intercept_ca, "\n")
```

## Using Regression Variables for Inference

To calculate the concentration of the metal ions from the provided signal values (in counts per second, cps), we can use the slope and intercept values obtained from the linear regression models.

Take a look at the following signal values that we don't know the concentrations of:

```{r}
# Read the metal signal data
metal_signals <- read_csv("data/metal_signals.csv", show_col_types = FALSE)
DT::datatable(metal_signals)
```

Let's assume that the linear regression models were built using the concentration of each metal as the independent variable and the signal values as the dependent variable. Here's how we can calculate the concentration for each metal:

```{r}
# Function to calculate concentration from signal values
calculate_concentration <- function(signal, metal_name) {
  # Extract slope and intercept for the specified metal
  slope <- metal_results[[metal_name]]$slope
  intercept <- metal_results[[metal_name]]$intercept
  
  # Calculate concentration using the linear regression equation: concentration = (signal - intercept) / slope
  concentration <- (signal - intercept) / slope
  
  return(concentration)
}

# Apply the calculate_concentration function to each metal column in the dataset
metal_signals$Al_concentration <- calculate_concentration(metal_signals$Al, "Al")
metal_signals$Mg_concentration <- calculate_concentration(metal_signals$Mg, "Mg")
metal_signals$Ca_concentration <- calculate_concentration(metal_signals$Ca, "Ca")
metal_signals$K_concentration <- calculate_concentration(metal_signals$K, "K")

# Print the updated dataset with calculated concentrations (Aluminum as the example)
DT::datatable(metal_signals)

```

*Note*: In the provided code snippet, the use of `[[]]` (double brackets) allows for dynamic access to list elements using a variable name. This is particularly useful when the exact name of the element is stored as a string in another variable, such as `metal_name`. The double brackets evaluate the variable to get its value, thereby accessing the corresponding list element. This method is distinct from the use of `$`, which requires a direct, literal name of the element and cannot interpret a variable's value as an element name.

Let's take a closer look at one of the analytes' calculated concentrations:

```{r, echo=FALSE}
DT::datatable(metal_signals[, c("Ca", "Ca_concentration")])
```

Do you see anything odd about the calculated concentrations of Calcium in the above table?

Note that negative concentration values can arise when signal values fall below the intercept of the linear regression model, indicating a deviation from the model's assumptions. This situation typically occurs at lower signal levels, where instrument noise or background signal may influence measurements. Negative concentrations lack meaningful interpretation in chemical analysis and are often considered artifacts of the model's limitations. To address this issue, we could establish a minimum detectable concentration threshold, treating signal values below this threshold as non-detects or zero values. Alternatively, quadratic or logarithmic regression models may better capture non-linear instrument responses, enabling accurate concentration determination across a wider dynamic range. And the great news is, we'll see how to do these non-linear regressions in the next chapter!

## Conclusion

In conclusion, while multiple linear regression serves as a powerful tool for analyzing the relationship between chemical concentrations and instrumental signals, its applicability may be limited by the assumption of linearity. Negative concentration values observed in linear regression analyses highlight the need for robust statistical techniques and critical evaluation of instrument responses. As we move forward into the realm of non-linear regression in the subsequent chapter, we explore alternative modeling approaches that can better accommodate non-linear relationships between variables. By embracing the versatility of non-linear regression techniques, we expand our analytical toolkit, allowing for more accurate and comprehensive analysis of complex chemical systems. Through a combination of linear and non-linear regression methods, chemists can unlock deeper insights into the quantitative relationships governing chemical phenomena, paving the way for advancements in analytical chemistry and beyond.

```{r child='src/common/end-of-chapter-exercise.Rmd'}
```

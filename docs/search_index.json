[["index.html", "R for Environmental Chemistry Preface Providing Feedback Acknowlegements", " R for Environmental Chemistry David Hall, Steven Kutarna, Kristen Yeh, Hui Peng, Chaerin Song, and Jessica D’eon Last built on: 2024-09-27 Preface Howdy, This website is more-or-less the living result of a collaborative project between us. We’re not trying to be an exhaustive resource for all environmental chemists. Rather, we’re focusing on developing broadly applicable data science course content (tutorials and recipes) based in R chemistry courses and research. This book is broken up into five parts: Part 1: Getting Started in R is a general guide for the complete novice that will help you install, setup, and run R code. Part 2: How to Code in R introduces the basics of R programming as well as a usual R workflow, and how to use R markdown to communicate your code with others. Part 3: Data Analysis in R introduces data analysis workflows and showcases how you can use R and the tidyverse to import and clean up your data into a consistent format to tackle the vast majority of the data science/analysis problems you’ll encounter in undergraduate environmental chemistry courses. Part 4: Visualization in R goes deeper in techniques used for visualizing different forms of data. Part 5: Modelling in R introduces different types of linear and non-linear models that you can use to understand and analyse data. We recommend that you read through Parts 1, 2, and 3 in sequential order. These provide the foundation for the more advanced workflows used in Parts 4 and 5. Providing Feedback If you notice an error, mistake or if you have suggestions for adding features or improving the book, please reach out to us or flag an issue on GitHub. Jessica D’eon at jessica.deon@utoronto.ca Acknowlegements Additionally, we would like to thank Jeremy Gauthier, Andrew Folkerson, Mark Panas, and Stephanie Schneider for all of their comments, suggestions, and hard work integrating the concepts of this book into the CHM410 Laboratory curriculum. "],["intro-to-r-and-rstudio.html", "Chapter 1 Intro to R and RStudio 1.1 R Language 1.2 RStudio 1.3 Setting Up Your Environment 1.4 Using RStudio 1.5 Running R Code 1.6 Customizing RStudio 1.7 Where to get help 1.8 Summary 1.9 Exercise", " Chapter 1 Intro to R and RStudio You may have heard about coding or the R programming language, but figuring out how to get started can be a hurdle; at least it was for us. In this guide, we will walk you through the process of setting up R and RStudio, both locally on your computer and remotely using the University of Toronto’s JupyterHub R Studio server. 1.1 R Language R is the programming language we’ll code in. R is hosted on the Comprehensive R Archive Network (CRAN) and is one of the most popular programming languages for statisticians and scientists alike due to its vast array of tools and packages. A quick aside, but don’t be intimidated by the term “coding”. Coding is simply writing instructions for the computer to execute. The only catch is has to be in a language that both we, humans, and the computer can understand. For our needs we’re using R, and like any language, R has it’s own syntax, rules, and quirks which we’ll cover in later chapters. 1.2 RStudio RStudio is a popular integrated development environment (IDE) specifically designed for working with R, providing a user-friendly interface and various productivity features. It’s where you’ll actually be typing your code and interacting with R. Again, R is a language, and you need somewhere to write it down to make use of it. Writing in English can be done with a pencil and notepad or a word processor filed with useful tools to help you write. R and RStudio work in tandem to provide an efficient and seamless experience for data analysis, visualization, and model building. RStudio enhances the R workflow with features like code editing, interactive visualization, version control, and package management. 1.3 Setting Up Your Environment Students learning R have two options: working locally or remotely. Working locally involves installing R and RStudio on their computer, providing direct control over data and code without an internet connection. On the other hand, working remotely enables access to RStudio through a web browser, avoiding local installations and allowing collaboration. For R programming in chemistry courses at the University of Toronto we recommend working remotely using the University of Toronto’s JupyterHub so we can ensure a stable R Studio environment where all relevant packages are installed and ready to go. Working locally might make sense for independent work as it doesn’t require an internet connection. More details on each option are provided below. 1.3.1 Working Remotely (Recommended for Coursework) Working remotely means accessing R and RStudio from a remote server or cloud-based platform. UofT JupyterHub RStudio server To facilitate remote access to RStudio, the University of Toronto provides a JupyterHub R Studio server. This allows you to access RStudio from any web browser, eliminating the need for local installations. With this, you can perform data analysis, collaborate with others, and work on your R projects remotely with ease. To get started, visit UofT JupyterHub. You will need to log in with your UofT credentials to access the RStudio environment. While working remotely, you may need to upload data to the RStudio server or download analysis results. The RStudio interface allows you to upload files directly from your computer to the server and vice versa. When working remotely, ensure that you save your R scripts and analysis files on the server. This will allow you to continue your work from any device with internet access. Most R packages are pre-installed on the University of Toronto’s RStudio server. However, if you require additional packages, we will soon learn how to install packages. Remember that while working remotely, a stable Internet connection is essential to ensure a smooth and uninterrupted experience. Additionally, always remember to save your work and log out properly after each session to maintain the security of your data. Happy coding! 1.3.2 Working Locally When you work locally, you need to install both R and RStudio on your personal computer or a machine that you physically have access to. 1.3.2.1 Downloading R and RStudio You can download the latest build of R for your operating system here. Choose the appropriate version for your operating system (Windows, macOS, or Linux) and follow the installation instructions. You can download the latest version of RStudio here. Once you have both R and RStudio downloaded, go ahead and open up RStudio. 1.4 Using RStudio When you open your RStudio (either locally or remotely), you’ll be greeted with an interface divided into numerous panes. We’ve highlighted the major ones in the image below: The RStudio interface with annottated regions Each pane serves a specific role: The console allows you to directly type and run your code. It also provides messages, warnings, and errors from any code you run. The environment window lists all variables, data, and functions you’ve created since the start of your coding session. The viewer shows your outputs, help documents, etc. which each has their own tab. 1.5 Running R Code You can run bits of R code directly from the console. Throughout the book, code you can copy and run will look like this: 2 + 2 ## [1] 4 Notice that both the code (the first part) and what the code outputs (the second part) are shown. Throughout this book code outputs will be preceded by ##. You can run code directly from the console. It’s handy for short and sweet snippets of code, something that can be typed in a single line. Examples of this is the install.packages() function, or to use R as a calculator: 2 * 3 ## [1] 6 pi * (10/2) ## [1] 15.70796 However, working like this isn’t very useful. Imagine printing a book one sentence at a time, you couldn’t really go back and edit earlier work because it’s already printed. That’s why we write out code in scripts. Scripts are similar to recipes, in that they’re a series of instructions that R evaluates from the top of the script to the bottom. More importantly, writing your code out in a script makes it more readable to humans (presumably this includes you). Don’t undervalue the usefulness of legible code. Your code will execute in seconds or minutes but it might take you hours to understand what it does. Let’s open up a new script in RStudio by going to File-&gt;New File-&gt;R Script, or by clicking on the highlighted button in the image below. This should open up a new window in the RStudio interface, as shown in the following image. You can copy and paste the code above into the script, save it, edit it, etc. and ultimately run specific lines of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on Mac), or by clicking the “Run” button in the top right corner of the Scripts window. You can copy an entire block of code using the copy button in the top right corner of the code block. In this way, this research is a tool to learn R coding and also a repository for code that you can copy and use. We’ll dive into the basics of coding in R in the next chapter. 1.6 Customizing RStudio As many of us spend an absurd amount of time staring at bright screens, some of you may be interested in setting your RStudio to Dark Mode. You can customize the appearance of your RStudio interface by clicking Tools-&gt;Global Options, or RStudio-&gt;Preferences on Mac, then clicking “Appearance” on the left. Select your preferred Editor Theme from the list. 1.7 Where to get help Given the popularity of R if you’ve run into an issue it’s very likely that someone else has too and they’ve complained about it and someone else has almost certainly solved it! An often unappreciated aspect of coding/data science is knowing how to get help, how to search for it, and how to translate someone’s solutions to your unique situation. Places to get help include: Google, Stack Overflow, etc. When in doubt Google it. Using built-in documentation: In the bottom-right pane in RStudio, click on the “Help” tab Or, in the Console type a ? followed by a name or function you want to learn more about, like ?pi or ?install.packages. Reference books such as the invaluable R for Data Science, which inspired this entire project. Ask for help, from TAs or instructors if using this book as a resource for a class, or from colleagues or peers. 1.8 Summary In this chapter we’ve covered: How to use RStudio to do R programming, both remotely and locally. The benefits of working remotely using platforms like the University of Toronto’s JupyterHub RStudio server. Resources for help when you get stuck, including Google, Stack Overflow, and reference books. In the next chapter we’ll break down how to setup your work in R for legibility, simplicity, and reproducibility. After all, the person cursing any of your sloppy work will invariably be you, so be kind to yourself, and do it right the first time. 1.9 Exercise Now that you’ve learned the basics of setting up and customizing R and RStudio, let’s put some of that knowledge into practice. 1.9.1 Setup Access UofT JupyterHub RStudio server here. (Optional) Change your RStudio appearance as you like. 1.9.2 Basic R Commands In the Console tab, write an expression to calculate 10 plus 5 and press enter. Create a new R script and type in the following commands: x &lt;- 10 y &lt;- 5 z &lt;- x + y print(z) Run the script. What is the output? 1.9.3 Using the Help Command (?) Let’s say you’ve come across a function in R that you don’t know how to use, for example, sqrt(). Use the ? command to access the documentation for this function from your Console tab by typing ?sqrt. What does the sqrt() function do? 1.9.4 Reflection What are your first impressions of RStudio as an IDE? Do you have any prior experience with other programming languages or IDEs? If so, how does RStudio compare? "],["rstudio-projects.html", "Chapter 2 RStudio Projects 2.1 Uploading Files to RStudio server on JupyterHub 2.2 Paths and Directories 2.3 Importing a Project 2.4 Creating an RStudio Project 2.5 A Sneak Peek at .Rmd Files 2.6 Summary 2.7 Exercise", " Chapter 2 RStudio Projects You’re probably eager to start coding, but an equally important aspect is understanding the structure of your work. Knowing how to organize the files needed for your analysis and how to access them quickly is critical. Learning this early on will save you plenty of time and heartache down the line. So let’s hold off on coding and consider where we’re working on your computer. Because we believe in it so much, we’ll say it up top: Always work inside an RStudio Project, and use a unique project for each lab/experiment. An RStudio Project defines a folder (called a working directory) where all relevant files and information are held. This simplifies coding significantly as you can simply indicate a file name in your code to access it without the full file path information. 2.1 Uploading Files to RStudio server on JupyterHub When using RStudio provided through the UofT JupyterHub, you may want to upload local data files, R scripts, or other relevant resources to work with them directly in RStudio. Here’s a straightforward guide on how to accomplish this. Once inside the RStudio server, you’ll notice several panes. One of these is the Files pane, typically found in the bottom right corner. This pane displays the current directory’s contents and allows you to manage files and folders. In the Files pane, click on the Upload button. Then, click Choose File button to navigate to the location of the files on your local computer that you wish to upload to the RStudio Server. This will prompt a file dialog box to appear. Select the desired file and click on Open or Choose (depending on your browser). Once the file names appear in the RStudio interface, there might be a confirmation step to complete the upload. Click on OK or Upload to finalize the process. After uploading, the uploaded files will appear in the Files pane. 2.2 Paths and Directories Before you get started with running your code, it is good to know where your analysis is actually occurring, or where your working directory is. The working directory is the folder where R looks for files that you have asked it to import, and the folder where R stores files that you have asked it to save. RStudio displays the current working directory at the top of the console, as shown below, but can also be printed to the console using the command getwd(). By default, R usually sets the working directory to the home directory on your computer. The ~ symbol denotes the home directory, and can be used as a shortcut when writing a file path that references the home directory. You can change the working directory using setwd() and an absolute file path. Absolute paths are references to files which point to the same file, regardless of what your working directory is set to. In Windows, absolute paths begin with \"C:\", while they begin with with a slash in Mac and Linux (i.e., \"/Users/Vinny/Documents\"). It is important to note that absolute paths and setwd() should never be used in your scripts because they hinder sharing of code – no one else will have the same file configuration as you do. If you share your script with your TA or Prof, they will not be able to access the files you are referencing in an absolute path. Thus, they will not be able to run the code as-is in your script. In order to overcome the use of absolute paths and setwd(), we strongly recommend that you conduct all work in RStudio within an R project. When you create an R project, R sets the working directory to a file folder of your choice. Any files that your code needs to run (i.e., data sets, images, etc.) are placed within this folder. You can then use relative paths to refer to data files in the project folder, which is much more conducive to sharing code with colleagues, TAs, and Profs. 2.3 Importing a Project While you can create a project from scratch (discussed below), we’ve created a draft project template. Download it, and you’ll have a working RStudio project that you can use as you follow along with the code in the rest of this chapter and the tutorial exercise. Downloading the template project (zip file) from the GitHub repository here; there are instructions on how to download the project at the bottom of the webpage. Upload the project zip file to JupyterHub, and unzip the folder. From RStudio click File -&gt; Open Project... and open the R4EnvChem-ProjectTemplate.Rproj file from the unzipped folder. If you’ve followed the steps above you should have successfully downloaded and opened an RStudio project, and it should contain these files in this structure: Note how the project name is displayed on the top right. You can quickly switch between projects here which is useful if you’ll be using R for many different labs/courses. As well, take note that the working directory has changed to the one where the RStudio project is located. Since you’ve downloaded the entire project, the working directory for the project includes the example scripts and data files you’ll need to continue along with the remainder of this book. If you open the project folder (or access it from the Files tab) it should look like this: R4EnvChem-ProjectTemplate │ R4EnvChem-ProjectTemplate.Rproj │ Rscript-example.R | README.md | Rmarkdown-example.rmd │ └───data │ 2018-01-01_60430_Toronto_ON.csv │ 2018hourlyNO2_Atl_wide.csv | ... │ └───images │ DHall_TorontoPano.jpg The R4EnvChem-ProjectTemplate.Rproj file defines your project and is located in the main folder, which RStudio will now treat as the working directory. Essentially it means we’ll be able to quickly access files in project subfolders such as data and images without having to find out what the full file path is for your own computer. You’ll appreciate this as you progress through this book. In the future you can create your own projects from scratch, but we recommend you follow the file structure demonstrated here as having consistently named folders you’ll use in every project will help simplify your life down the road. 2.4 Creating an RStudio Project We’ve provided instructions on creating your own RStudio project from scratch, but you can always copy the template project folder above (or any for that matter) to re-purpose it as you see fit. To create a new project: go to File-&gt;New Project, or click the button highlighted in the image below. Click New Directory, then New Project. You may want your project directory to be a sub-folder of an existing directory on your computer which already contains your data sets. If this is the case, click Existing Directory instead of New Directory at the previous step, and then select the folder of your choice. Next, you’ll be asked to choose a sub-directory name and location. Enter your selected name and choose an appropriate location for the folder on your computer. Click Create Project, and you should now see your chosen file path displayed in the Files tab of the Viewer pane: When working on assignments for coursework, it is good practice to create a new R project for each assignment you work on. You should store the data, images, and any other files required for that assignment within the folder for the designated R project. You can create sub-folders for data and images, however, you may want to avoid making too many nested sub-folders, as this will make your paths long and tiresome to type. 2.5 A Sneak Peek at .Rmd Files In this textbook, you will exclusively work with .Rmd (R Markdown) files, which offer a dynamic and interactive platform for blending code, text, and output. Within an .Rmd file, you will encounter two distinct components: code and text. Text fields, easily accessible by inserting regular text, allow you to compose explanations, context, and interpretations using plain language. These text fields can be created directly within the .Rmd document. Code chunks, on the other hand, house R code that can be executed to generate results and graphics. We will learn more about working with R markdown in the later chapters. 2.6 Summary In this chapter we’ve covered: Importing the R4EnvChem Project Template so we have access to data for the tutorial (amongst other things) The concept of paths and directories and how relative referencing withing a project greatly simplify this 2.7 Exercise For this chapter, you will create your own R project in UofT JupyterHub RStudio. 2.7.1 Get Started Launch RStudio on the UofT JupyterHub server. 2.7.2 Confirm Your Working Directory Use the getwd() function in RStudio to display the current working directory in the console. Ensure that the working directory in RStudio is the location where you’d like to set up your project. 2.7.3 Creating Your Own Project Launch a new RStudio project. To do this, go to File -&gt; New Project. Choose “New Directory”. Select “New Project”. Name the project “MyFirstRProject” and choose a convenient location to save it. Click on “Create Project”. Use the getwd() function again to check your current working directory and confirm you’re in the “MyFirstRProject” directory. Inside the “MyFirstRProject” directory, create two new folders: “data” and “notebook”. You can do this using RStudio’s ‘Files’ tab or using the dir.create() function in the R console. 2.7.4 Create Your Rmd File Within your “MyFirstRProject” directory, create a new .Rmd (R Markdown) file. You can do this by going to File -&gt; New File -&gt; R Markdown. Name the file “MyFirstRMarkdown” and set HTML as the default output format. In the text section of the .Rmd file, write one thing you remember about R and RStudio. Insert a code chunk below what you wrote. In this code chunk, type sum(1:10), which calculates the sum of numbers from 1 to 10. Knit the document to see the results. This will produce an HTML or PDF document that shows both your text and the results of your R code. 2.7.5 Upload a file from your computer Start by visiting the Exploring Air Quality Data website and navigate to the My Data tab. Once there, choose your preferred options on the left side. Then, on the right side, input your student number to retrieve your data. After your data displays, click on the Download Your Data! button to download the data as a CSV file (as shown in Figure 1). Next, upload this downloaded data to the “data” folder within your RStudio project. If you’ve forgotten how, just refer back to the beginning of this chapter for a quick reminder. After the upload, you’ll be able to spot your data in the Files pane of RStudio. Simply click on the data file’s name and then select View File to peek at your raw data. Figure 1 2.7.6 Reflection Explain the difference between relative and absolute paths. Why are relative paths preferred when working in RStudio projects? "],["using-r-markdown.html", "Chapter 3 Using R Markdown 3.1 Getting Started with R Markdown 3.2 Compiling your final report 3.3 Authoring with R Markdown 3.4 R Markdown resources", " Chapter 3 Using R Markdown Before going into more details of R Markdown, let’s talk about two common options in the world of R coding: the R script (.R) and the dynamic R Markdown document (.Rmd). R Scripts: Imagine coding as crafting a detailed recipe of R commands—a script—that guides R through specific tasks. Conventional R scripts (.R files) are dedicated to these commands, handling calculations and operations. However, as scripts grow, they become complex and sharing insights alongside code becomes challenging. R Markdown: R Markdown (.Rmd) elevates the coding experience by harmonizing code with explanatory text. Within an R Markdown document, code blocks act like individual scripts—smaller, more focused units. These blocks merge code with explanations seamlessly, creating a coherent narrative. Unlike isolated scripts, R Markdown emphasizes both code functionality and its significance within the context. For these reasons, we’ll be sticking to working in .Rmd files. In a nutshell, R Markdown allows you to analyse your data with R and write your report in the same place (this entire book was written with R Markdown). This has loads of benefits including a reproducible workflow, and streamlined thinking. No more flipping back and forth between coding and writing to figure out what’s going on. Let’s run some simple code as an example: x &lt;- 2+2 x ## [1] 4 What we’ve done here is write a snippet of R code, ran it, and printed the results (as they would appear in the console). While the above code isn’t anything special, we can extend this concept so that our R Markdown document contains any data, figures or plots we generate throughout our analysis in R. For example here is a time series of 2018 ambient atmospheric O3, NO2, and SO2 concentrations (ppb) in downtown Toronto: library(tidyverse) library(knitr) airPol &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ggplot(data = airPol, aes(x = date.time, y = concentration, colour = pollutant)) + geom_line() + theme_classic() sumAirPol &lt;- airPol %&gt;% drop_na() %&gt;% group_by(city, naps, pollutant) %&gt;% summarize(mean = mean(concentration), sd = sd(concentration), min = min(concentration), max = max(concentration)) knitr::kable(sumAirPol, digits = 1) city naps pollutant mean sd min max Toronto 60430 NO2 20.5 11.5 7 55 Toronto 60430 O3 19.7 8.7 1 33 Toronto 60430 SO2 1.1 0.3 1 3 Pretty neat, eh? You might not think so, but let’s imagine a scenario you’ll encounter soon enough. You’re about to submit your assignment, you’ve spent hours analyzing your data and beautifying your plots. Everything is good to go until you notice at the last minute you were supposed to subtract value x and not value y in your analysis. If you did all your work in Excel (tsk tsk), you’ll need to find the correct worksheet, apply the changes, reformat your plots, and import them into word (assuming everything is going well, which it never does with looming deadlines). Now if you did all your work in R Markdown, you go to your one .rmd document, briefly apply the changes and re-compile your document. A lot of scientists work with R Markdown for writing their reports for numerous reasons: Integrated Workflow: Combines narrative, data analyses, and visualizations in one document, promoting reproducibility and transparency. Versatility: Easily exports to diverse formats like HTML, PDF, and Word, catering to different dissemination needs. Plot Management: Offers precise control over visual presentations, allowing for tailored figure sizes, resolutions, and formats. In sum, R Markdown provides a streamlined platform for scientific communication, merging data analysis with polished publication seamlessly. 3.1 Getting Started with R Markdown As you’ve already guessed, R Markdown documents use R and are most easily written and assembled in RStudio. If you have not done so, revisit Chapter 1: Intro to R and RStudio. Once setup with R and RStudio, you’ll need to install the R Markdown and tinytex packages by running the following code in the console: # These are large packages so it&#39;ll take a couple of minutes to install install.packages(&quot;R Markdown&quot;) install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() # install TinyTeX The R Markdown package is what we’ll use to generate our documents, and the tinytex package enables compiling documents as PDFs. There’s a lot more going on behind the scenes, but you shouldn’t need to worry about it. Now that everything is set up, you can create your first R Markdown document by opening up RStudio, selecting File -&gt; New File -&gt; R Markdown.... A dialog box will appear asking for some basic information for your R Markdown document. Add your title and select PDF as your default output format (you can always change these later if you want). A new file should appear using a basic template that illustrates the key components of an R Markdown document. 3.1.1 Understanding R Markdown Your first reaction when you opened your newly created R Markdown document is probably that it doesn’t look anything at all like something you’d show your prof. You’re right, what you’re seeing is the plain text code which needs to be knit to create the final document. When you create a R Markdown document like this in R Studio a bunch of example code is already written. You can knit this document (see below) to see what it looks like, but let’s break down the primary components. At the top of the document you’ll see something that looks like this: --- title: &quot;Temporal Analysis of Foot Impacts While Birling Down the White Water&quot; author: &quot;Jean Guy Rubberboots&quot; date: &quot;24/06/2021&quot; output: pdf_document --- This section is known as the preamble and it’s where you specify most of the document parameters. In the example we can see that the document title is “Temporal Analysis of Foot Impacts While Birling Down the White Water”, it’s written by Jean Guy Rubberboots, on the 24th of June, and the default output is a PDF document. You can modify the preamble to suit your needs. For example, if you wanted to change the title you would write title: \"Your Title Here\" in the preamble. 3.1.1.1 Output Options in R Markdown You can compile your entire document using the Knit document button. This is a great way to tinker with your code before you compile your document. Knitting will sequentially run all of your code chunks, generate all the text, knit the two together and output a PDF. You’ll basically save this for the end. R Markdown offers flexibility in terms of output formats, allowing users to knit their documents into various outputs tailored to their needs. Three Common Output Options: HTML (html_document): Produces an HTML file, suitable for hosting on websites or for sharing via email. This format allows for interactive content, making it ideal for interactive graphs or web applications. PDF (pdf_document): Creates a PDF file. This format is best for documents intended for print or formal submissions, as it maintains consistent formatting across different devices and platforms. Word (word_document): Generates a Microsoft Word document, which can be useful when sharing drafts or collaborating with colleagues who use Word for edits. Controlling the Output: Modifying the metadata header: You can change the output format directly in the header of your R Markdown file. In the last example, replacing output: pdf_document with output: html_document or output: word_document would knit the document into HTML or Word, respectively. Using RStudio’s Knit Button: In RStudio, at the top of the script editor pane, there’s a Knit button. Clicking the small dropdown arrow next to this button allows you to choose the output format you desire. Selecting one of the options will knit the document into that format and update the header accordingly. 3.1.2 Running Code in R Markdown 3.1.2.1 How to Create Code Chunks To create a code chunk within RStudio, you have several options: Use the green “c” button located at the top right corner of your file view and select “R”. Make sure your cursor is positioned at the desired location within your .rmd file when you do this. Type ```{r} – three back-ticks followed by {r} – to initiate a new code chunk, and type ``` – three backticks (```) – to end the code chunk. You can specify code chunk options in the curly brackets. i.e. ```{r, fig.height = 2} sets figure height to 2 inches. See the Code Chunk Options section below for more details. Inline code expression, which starts with `r and ends with ` in the body text. Earlier we calculated x &lt;- 2 + 2, we can use inline expressions to recall that value. When we knit the markdown file shown in the figure, the knitted document will say “We found that x is 4.” 3.1.2.2 How to Run Code Chunks To run code within an R Markdown document, you again have various options to choose from. You can run a specific code chunk by clicking the green triangle button located within each chunk. This action will execute the entire chunk, including all the code it contains. For more control, you can run selected lines or chunks. To do this, use the “Run” button at the top of the file view. This button provides a range of execution options that allow you to run code in a manner that suits your needs. Note all the code chunks in a single document work together like a normal R script. That is, if you assign a value to a variable in the first chunk, you can use this variable in the second chunk. Also note that every time you knit an R Markdown document, it’s done in a “fresh” R session. If you’re using a variable that exist in your working environment, but isn’t explicitly created in the document, you’ll get an error. 3.1.3 Headings and Subheadings Structure your document with clear headings and subheadings by using the pound (#) sign. This not only helps in organizing content but also aids in creating a table of contents if required. The level of heading is denoted by the number of # signs, as you saw with R script headings in the previous section. Main Headings: Use a single pound sign (i.e. # Main Heading) Subheadings: Increase the number of pound signs based on the level of the subheading. ## Subheading Level 1 ### Subheading Level 2 #### Subheading Level 3 R Markdown will automatically format these appropriately when the document is knit. For example, a main heading will typically appear larger and bolder than its subheadings, like this: By effectively utilizing headings and subheadings, you can provide clear structure and flow to your document, making it more readable and navigable for your audience. 3.1.4 LaTeX Basics LaTeX (pronounced “lay-tech”) is a typesetting system that’s popular in academia due to its high-quality output format and the ability to handle complex formatting tasks. It’s especially favored for documents that contain mathematical symbols, equations, and other specialized notation. In R Markdown, LaTeX code can be integrated directly into text chunks to allow for advanced formatting, especially for mathematical expressions and equations. When you knit your R Markdown document, the LaTeX code is rendered into beautifully formatted text. Note: LaTeX code should always be written in text chunks, not code chunks! There are two common ways to turn your expressions in a math mode. Display mathematical expressions: centers the mathematical expression on its own line. Inline mathematical expressions: appears within the text of a paragraph. For chemistry students, one common use of LaTeX is to typeset chemical equations. We will provide examples on the combustion of methane: 3.1.4.1 Display math mode You can have an entire line in a math mode using either \\[...\\] or $$...$$. For example, writing the following in R Markdown \\[ \\text{CH}_4 + 2\\text{O}_2 \\rightarrow \\text{CO}_2 + 2\\text{H}_2\\text{O} \\] produces the following output in the generated PDF: \\[ \\text{CH}_4 + 2\\text{O}_2 \\rightarrow \\text{CO}_2 + 2\\text{H}_2\\text{O} \\] 3.1.4.2 Inline math mode On the other hand, if you want to insert your expression within your sentence, you can use $...$ syntax. With our methane combustion example, we can write something like this: Methane ($\\text{CH}_4$) reacts with oxygen ($\\text{O}_2$) to produce carbon dioxide ($\\text{CO}_2$) and water ($\\text{H}_2\\text{O}$). When you knit it, this will be displayed as: Methane (\\(\\text{CH}_4\\)) reacts with oxygen (\\(\\text{O}_2\\)) to produce carbon dioxide (\\(\\text{CO}_2\\)) and water (\\(\\text{H}_2\\text{O}\\)). 3.1.4.3 Useful LaTeX Syntax Now that you’ve seen how you can write your scientific expression in two different ways, let’s look at some useful LaTeX Syntax for our purpose. Symbols Greek letters: Use a backslash followed by the name of the letter, e.g., \\alpha for \\(\\alpha\\). Special symbols \\times for \\(\\times\\) \\approx for \\(\\approx\\) \\geq for \\(\\geq\\) \\rightarrow for \\(\\rightarrow\\) Superscripts and Subscripts Superscripts: x^2 renders as \\(x^2\\). Subscripts: H_2O renders as \\(H_2O\\) Formatting Boldface: \\textbf{Text} for \\(\\textbf{Text}\\) Italics: \\textit{Text} for \\(\\textit{Text}\\) Tip: In RStudio, you can place your cursor over LaTeX code to preview its generated output. 3.1.4.4 More LaTeX Resources There are numerous online resources dedicated to LaTeX symbols and their usage. A popular starting point is the Comprehensive LaTeX Symbol List. This extensive compilation offers a wide range of symbols used in various disciplines. Platforms like Detexify allow users to sketch a symbol, and the tool then identifies the corresponding LaTeX command. Engaging with online communities, such as the TeX Stack Exchange, can also be invaluable for finding specific symbols or seeking advice on LaTeX-related challenges. 3.2 Compiling your final report To hand in your work, you’ll need to knit your document to generate a PDF. To knit your R Markdown file, click the knit button in RStudio (yellow box, Figure 2). 3.3 Authoring with R Markdown Below is a brief summary of the major elements required to author an R Markdown document. They should address the majority of your needs, but please see the R Markdown resources for more information. 3.3.1 R Markdown Syntax Unlike Microsoft Word, R Markdown utilizes a specific syntax for text formatting. Once you get used to it, it makes typing documents much easier than Word’s approach. The table below is how some of the most common text formatting is typed in your R Markdown document (syntax &amp; example column) and how it’ll appear in the final output. Text formatting syntax Example Example output italics *text* this is *italics* this is italics bold **text** this is **bold** this is bold subscript ~text~ this is ~subscript~ this is subscript superscript ^text^ this is ^superscript^ this is superscript monospace `text` this is `monospaced` this is monospace For a collection of other R Markdown syntax, please see the useful (and brief) list compiled online here. 3.3.2 R Code Chunk Options Your R code is run in chunks and the results will be embedded in the final output file. To each chunk you can specify options that’ll affect how your code chunk is run and displayed in the final output document. You include options in the chunk delimiters ```{r} and ```. For example the following options indicate that the code chunk contains R code, that when you knit your document the code in this chunk should be evaluated, and that the resulting figure should have the caption “Some Caption”. ```{r, eval = TRUE, fig.cap = &quot;Some caption&quot;} # some code to generate a plot worth captioning. ``` The most common and useful chunk options are shown below. Note that they all have a default value. For example, eval tells R Markdown whether the code within the block should be run. It’s default option is TRUE, so by default any code in a chunk will be run when you knit your document. If you don’t want that code to be run, but still displayed, you would set eval = FALSE. Another example would be setting echo = FALSE which allows the code to run, but the code won’t be displayed on the output document (the outputs will still be displayed though); useful for creating clean documents with plots only (i.e. lab reports…). option default effect eval TRUE whether to evaluate the code and include the results echo TRUE whether to display the code along with its results warning TRUE whether to display warnings error FALSE whether to display errors message TRUE whether to display messages tidy FALSE whether to reformat code in a tidy way when displaying it fig.width 7 width in inches for plots created in chunk fig.height 7 height in inches for plots created in chunk fig.cap NA include figure caption, must be in quotation makrs (““) 3.3.3 Inserting images Images not produced by R code can easily be inserted into your document. ![Caption for the picture.](path/to/image.png){width=50%, height=50%} Note that in the above the use of image attributes, the {width=50%, height=50%} at the end. This is how you’ll adjust the size of your image. Size dimensions you can use include px, cm, mm, in, inch, and %. A final note on images: when compiling to PDF, your image will be placed in the “optimal” location (as determined by LaTeX), so you might find your image isn’t exactly where you thought it would be. A more in-depth guide to image placement can be found here. 3.3.4 Generating tables There are multiple ways of creating tables in R Markdown. Assuming you want to display results calculated through R code, you can use the kable() function. Or you can consult the Summarizing Data chapter for making publication ready tables. Alternatively, if you want to create simple tables manually use the following code in the main body, outside of an R code chunk. You can increase the number of rows/columns and the location of the horizontal lines. To generate more complex tables, see the kable() function and the kableExtra package. | Header 1 | Header 2 | Header 3 | |:---------|:---------|:----------------| | Row 1 | Data | Some other Data | | Row 2 | Data | Some other Data | Header 1 Header 2 Header 3 Row 1 Data Some other Data Row 2 Data Some other Data We know this can be a tedious process. Luckily, there is a website that generates the markdown syntax when you input the values, and this can save your time trying to correctly format tables. Check it out here. 3.3.5 Spellcheck in R Markdown While writing an R Markdown document in R studio, go to the Edit tab at the top of the window and select Check Spelling. You can also use the F7 key as a shortcut. The spell checker will literally go through every word it thinks you’ve misspelled in your document. You can add words to it so your spell checker’s utility grows as you use it. Note that the spell check may also check your R code; be wary of changing words in your code chunks because you may get an error down the line. 3.3.6 Exporting R Markdown documents You’ll most likely be exporting your R Markdown documents as PDFs, but the beauty of R Markdown is it doesn’t stop there. Your R Markdown documents can be knitted as a HTML document, a book (or both like this book!). You can even make slideshow presentations and yes, if need be, export as a word document that you can open in Microsoft Word. You specify the output format in the document header. To specify you want your document to be outputted as a PDF your header would look like this: --- title: &quot;Your title here&quot; output: pdf_document --- Here are some links to different output formation available in R Markdown and how to use them: pdf_document creates a PDF document via Latex; probably your defacto output. word_document creates a Word document. Note that the formatting options are pretty basic, so while everything will be where you want it to be, you’ll need to pretty it up in Word to comply with your instructor’s specifications. tufte_handout for a PDF handout in the style of Edward Tufte. Check it out. ioslides_presentation, revealjs::revealjs_presentation, and powerpoint_presentation are all options to create slideshow presentations. revealjs has the steepest learning curve of the bunch, but once set up, you can make incredibly slick slides with ease. Note: like word_document, powerpoint_presentation’s outputs are stylistically simple. You’ll definitely need to pretty them up manually in Powerpoint. 3.3.7 RStudio tips and tricks To further the usefulness of R Markdown, the latest release of RStudio has a Visual R Markdown editor which introduces many useful features for authoring documents in R Markdown. Some of the most pertinent are: Visual editor so you can see how your document looks (top left of script pane) Combining Zotero and RStudio for easy citations, of your document (read more here) 3.4 R Markdown resources There’s a plethora of helpful online resources to help hone your R Markdown skills. We’ll list a couple below (the titles are links to the corresponding document): Chapter 2 of the R Markdown: The Definitive Guide by Xie, Allaire &amp; Grolemund (2020). This is the simplest, most comprehensive, guide to learning R Markdown and it’s available freely online. The R Markdown cheat sheet, a great resource with the most common R Markdown operations; keep on hand for quick referencing. Bookdown: Authoring Books and Technical Documents with R Markdown (2020) by Yihui Xie. Explains the bookdown package which greatly expands the capabilities of R Markdown. For example, the table of contents of this document is created with bookdown. "],["how-to-use-this-textbook.html", "Chapter 4 How to Use This Textbook 4.1 Useful Features 4.2 End-of-Chapter Exercises 4.3 Running Tests for Your Exercises", " Chapter 4 How to Use This Textbook Before we move onto the actual coding part, let’s talk about how to navigate and utilize this textbook. 1. Reading and Active Engagement This textbook encourages active learning. Don’t merely read through the content—interact with it. Type out the code in your R environment and see the results firsthand. This hands-on approach will solidify your comprehension and enhance your practical skills. Observe how the code behaves, experiment with modifications, and observe how changes impact the outcomes. 2. Curiosity and Inquisitiveness When you encounter code you don’t fully understand or want to know the underlying process, lean into your curiosity. Don’t hesitate to ask “Why?” and explore concepts beyond the immediate scope. Seek to understand the “why” and “how” alongside the “what.” 3. Resources and Further Explanation This textbook is a stepping stone to your R journey. Beyond the content provided, explore the references, suggested readings, and online resources mentioned throughout the chapters. Embrace a curious attitude and continue to expand your knowledge by delving into more advanced topics or specific applications that align with your interests. 4. Discussion and Collaboration If you’re using this textbook as part of a class or a group, engage in discussions with your peers. Sharing insights, clarifying doubts, and collaborating on exercises can enhance your learning experience. Don’t hesitate to ask questions, seek help, and contribute to a supportive learning environment. 4.1 Useful Features 4.1.1 Searching the Textbook By clicking on the magnifying glass icon in the top left corner, you have the ability to search for keywords across the entire textbook without worrying about case sensitivity. For instance, entering “tidyverse” will display all chapters where tidyverse is mentioned. This gives you a glimpse into future chapters, offering a preview of the various ways you’ll be engaging with tidyverse later on! 4.1.2 Original R Markdown of the Textbook Chapters This textbook is assembled from individual Rmd files, each representing a chapter. As you progress through the chapters, you may wish to examine the associated Rmd files to delve deeper into the code and its execution. Simply click on the edit icon in the top left corner to be directed to the corresponding Rmd file on GitHub, opened in a new tab. You’re encouraged to download these files, experiment with the code, and observe our Rmd formatting techniques! 4.2 End-of-Chapter Exercises As you progress through each chapter, you’ll find .Rmd (R Markdown) files available for practice and reinforcement. These Rmd files are designed to provide you with hands-on exercises that align with the concepts covered in the textbook. Within each Rmd file, you’ll encounter straightforward exercises that give you the opportunity to apply what you’ve learned in each chapter. After completing an exercise, you can run the provided unit test cell to check your answers and receive instant feedback. Here is an instruction to how to start on the exercises: Access the Repository: Recommended for those who have access to UofT JupyterHub: Click on the following link to automatically clone the “R4EnvChem-Exercises” repository to your UofT JupyterHub: R4EnvChem-Exercises Repository. This is a preferred method for anyone completing exercises as part of a UofT course. Alternatively, you can directly download the files from this repository. Work on the Exercise: Once inside your JupyterHub’s RStudio environment, in the “Files” pane you’ll see “R4EnvChem-Exercises” folder (figure below). If you click into this folder, you will see a list of chapter folders. Each chapter folder contains the respective exercise Rmd files. Navigate to the desired chapter’s folder and click on the exercise Rmd file you wish to work on. This will open the file in the RStudio editor. Once the Rmd file is open, you can edit, run code chunks, and add your solutions directly in the file. Remember to save your progress regularly. If you want to generate an output document (like a PDF or HTML) to view your results, click on the “Knit” button usually located at the top of the script editor. To enhance readability, you optionally click on the Visual tab at the top of the file view and work on the exercises. 4.2.1 Optional Extra Questions For those seeking an additional challenge and a chance to delve into topics beyond the textbook, we offer optional extra questions. Resources and explanations will be provided to support you in tackling these optional questions. By engaging with these interactive Rmd files, you can actively reinforce your learning, gain practical experience, and explore R concepts in depth. We encourage you to make the most of these resources to enhance your R proficiency. Happy learning! 4.3 Running Tests for Your Exercises Each chapter’s exercise folder also contains an R test file (e.g., chapterX_tests.R). This file includes unit tests to verify the correctness of your solutions. 4.3.1 How to Run the Tests Complete the Exercises: Work through the exercises in the Rmd file that you encountered in Using R Markdown, adding your solutions to the code chunks, and save your progress. You can save the progress by choosing “File” from the top menu and then clicking on “Save” or by using the keyboard shortcut Ctrl + S (Windows/Linux) or Cmd + S (Mac) for the current Rmd file. Set the Working Directory: Whether you are working on the exercises in the UofT JupyterHub or on your local machine, set the working directory to the folder containing your exercise files. To do this in RStudio, choose “Session” from the top menu, click on “Set Working Directory,” and then select “Choose Directory.” Navigate to the folder containing the exercise files and click “Choose.” Open the Test File: After setting the working directory, open the corresponding R test file located in the same chapter folder. Run the Tests: To run the tests inside the R test file, click on the “Run Tests” button in the top-right corner of the script editor. This will execute the unit tests and provide feedback on your solutions. Review the Test Results: The test results will be displayed in the R console, showing which tests passed or failed. Expected Output: If all tests pass, you’ll see a summary indicating success. Expected Errors: If any tests fail, the console will show which tests didn’t pass, along with error messages that provide details on the issue. Troubleshooting: Syntax Errors: Ensure your R code is syntactically correct (e.g., missing commas, unmatched parentheses). Logical Errors: Double-check that your calculations or logic align with the problem requirements. In these cases, you will see an error message indicating the expected output and the actual output, or missing output. Missing Variables or Functions: Make sure all necessary variables and functions are correctly defined and used. In these cases, the error message will indicate that the object or function is missing. 4.3.2 Optional: Autotests on MarkUs For those who have access to MarkUs, you have the option to use it for autotests on your assignments. These autotests provide automated feedback to help ensure your solutions are correct. Submit Your Work: From the list of assessments, select the chapter exercise you wish to submit. Go to the Submissions section. Click on Submit File and then on Choose Files to upload your Rmd file. Ensure that the files you submit match exactly the names listed in the Required Files section. If you prefer not to rename the files locally, you can use the Rename file to option, which will show you a list of files you need to submit. Choose the correct file name from this list for your upload. Run Autotests: Go to the Automated Testing section and click on Run Tests. This will trigger the autotests to check your submission against the provided test cases. Review Feedback: MarkUs will provide immediate feedback on your submission, indicating which tests passed or failed. Examine this feedback to identify any issues. Resubmit if Necessary: If any autotests fail, revise your Rmd file based on the feedback and resubmit it for further testing. "],["r-coding-basics.html", "Chapter 5 R Coding Basics 5.1 Variables 5.2 Data Types 5.3 Data Structures 5.4 Conditional Statements 5.5 R built-in Functions 5.6 Summary 5.7 Exercise", " Chapter 5 R Coding Basics Now that you know how to navigate RStudio and have a working project, we’ll take a look at the basics of R. As we’re chemists first, and not computer programmers, we’ll try and avoid as much of the nitty-gritty underneath the hood aspects of R. However, a risk of this approach is being unable to understand errors and warnings preventing your code from running. As such, we’ll introduce the most important and pertinent aspects of the R language to meet your environmental chemistry needs. 5.1 Variables We’ve already talked about how R can be used like a calculator: (1000 * pi) / 2 ## [1] 1570.796 (2 * 3) + (5 * 4) ## [1] 26 But managing these inputs and outputs is simplified with variables. Variables in R, like those you’ve encountered in math class, can only have one value, and you can reference or pass that value along by referring the variable name. And, unlike the variables in math classes, you can change that value whenever you want. Another way to think about it is that a variable is a box in which you store your value. When you want to move (reference) your value, you move the box (and whatever is inside of it). Then you can simply open the box somewhere else without having to worry about the hassle of what’s inside. You can assign a value to a variable using &lt;-, as shown below. x &lt;- 12 x ## [1] 12 Codes using &lt;- should be read right to left: x &lt;- 12 would be read as “take the value 12 and store it into the variable x”. The second line of code, x, simply evaluates the value that x is assigned to. Note that when a variable is typed on it own, R will print out its contents. You can now use this variable in snippets of code: x &lt;- x * 6.022e23 x ## [1] 7.2264e+24 Remember, R evaluates &lt;- from right to left, so the code above is taking the number 6.022e23 and multiplying it by the value of x, which is 12 and storing that value back into x. That’s how we’re able to modifying the contents of a variable using its current value. You can also overwrite the contents of a variable at anytime (i.e. x &lt;- 25). Variable names are case sensitive, so if your variable is named x and you type X into the console, R will not be able to print the contents of x. Variable names can consist of letters, numbers, dots (.) and/or underlines (_). Here are some rules and guidelines for naming variables in R: Variable Name Requirements as dictated by R names must begin with a letter or with the dot character. var and .var are acceptable. Variable names cannot start with a number or the . character cannot be followed by a number. var1 is acceptable, 1var and .1var are not. Variable names cannot contain a space. var 1 is interpreted as two separate values, var and 1. Certain words are reserved for R, and cannot be used as variable names. These include, but are not limited to, if, else, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NA, and NaN Good names for variables are short, sweet, and easy to type while also being somewhat descriptive. For example, let’s say you have an air pollution data set. A good name to assign the data set to would be airPol or air_pol, as these names tell us what is contained in the data set and are easy to type. A bad name for the data set would be airPollution_NOx_O3_June20_1968. While this name is much more descriptive than the previous names, it will take you a long time to type, and will become a bit of a nuisance when you have to type it 10+ times to refer to the data set in a single script. Please refer to the Style Guide found in Advanced R by H. Wickham for more information. Lastly, R evaluates multiple lines of code one at a time, from top-to-bottom. So if you reference a variable it must have already been created at an earlier point in your script. For example: y + 1 ## [1] 6 y &lt;- 12 The code above returns the object 'y' not found error because we’re adding + 1 to y which hasn’t been created yet, it’s created on the next line. These errors also pop up when you edit your code without clearing your workplace. All variables created in a session are stored in the working environment so you can call them, even if you change your code. This means you can accidentally reference a variable that isn’t reproduced in the latest iteration of your code. Consequently, a good practice is to frequently clear your work-space using the ‘broom’ button in the Environment pane. This will help you to ensure the code you’re writing is organized in the correct order; see Saving R Markdown for why this is important. 5.2 Data Types Data types refer to how data is stored and handled by and in R. This can get complicated quickly, but we’ll focus on the most common types here so you can get started on your work. Firstly, here are the data types you’ll likely be working with: character: \"a\", \"howdy\", \"1\", is used to represent text values in R. Character values may be wrapped in either single-quotes or double-quotes; for consistency in this textbook, we’ll always use double-quotes. For example, \"1\", despite being read as number by us, is stored as a character and treated as such by R. Also known as string values. numeric: any real or decimal number such as 2, 3.14, 6.022e23. integer such as 2L, note the ‘L’ tells R this is an integer. logical: either TRUE or FALSE; also known as a boolean values. Sometimes R will misinterpret a value as the wrong data type. This can hamper your work as you can’t do arithmetic on a string! x &lt;- &quot;6&quot; x / 2 ## Error in x/2: non-numeric argument to binary operator “non-numeric argument to binary operator” is a commonly encountered error, and it’s simply telling you that you’re trying to do math on something you can’t do math on. You might think if x is 6, why can’t I divide it by 2? Let’s look at some helpful functions to test the data type of a value in R, and how to fix errors like this one. First, let’s see what type of data x is: is.numeric(x) # test if numeric ## [1] FALSE is.logical(x) # test if logical ## [1] FALSE is.integer(x) # test if integer ## [1] FALSE is.character(x) # test if character ## [1] TRUE So the value of x is a character, in other words R treats it as a word, and we can’t do math on that (note the quotation marks “” around the 6 in the code above that defined it as text). So let’s convert the data type of x to numeric to proceed. x ## [1] &quot;6&quot; x &lt;- as.numeric(x) is.numeric(x) ## [1] TRUE x ## [1] 6 x / 2 ## [1] 3 So we’ve converted our character string \"6\" to the numerical value 6. Keep in mind there are other conversion functions which are described elsewhere, but you can’t always convert types. In the above example we could convert a character to numeric because it was ultimately a number, but we couldn’t do the same if the value of x was \"six\". x &lt;-&quot;six&quot; x &lt;- as.numeric(x) ## Warning: NAs introduced by coercion x ## [1] NA “NAs introduced by coercion” means that as.numeric didn’t know how to convert “six” to a numeric value, so it instead turned it into an NA, representing a missing value. 5.3 Data Structures Data structures refers to how R stores data. It’s easy to get lost in the weeds here, so we’ll start with the focus on the most common and useful data structure for your work: data frames. 5.3.1 Data Frames Data frames consist of data stored in rows and columns. If you’ve ever worked with a spreadsheet, it’s essentially that with the caveat that all data stored in a column must be of the same type. Different columns can have different data types, but within a column all the data needs to be the same type. When importing data into a data frame, R will automatically convert data to ensure consistency in each column. This can lead to surprising behaviour: a common error is a single character in a column of numerical values leading to the entire column to be interpreted as character values. We’ll need to be careful about this when importing our own data, which we’ll start doing shortly! 5.3.1.1 Creating a Data Frame from Scratch Let’s see how we can create a data frame by explicitly listing out the values. # First, create data for each column. We use the &quot;c&quot; function to create # one *vector* for each column. We&#39;ll discuss vectors in more detail below. names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;, &quot;Eve&quot;) ages &lt;- c(20, 21, 22, 23, 19) food &lt;- c(&quot;Bubble Tea&quot;, &quot;Pineapple Pizza&quot;, &quot;Diet Pepsi&quot;, &quot;Korean BBQ&quot;, &quot;Sushi AYCE&quot;) # Creating the data frame students &lt;- data.frame(Name = names, Age = ages, Food = food) # Displaying the data frame print(students) ## Name Age Food ## 1 Alice 20 Bubble Tea ## 2 Bob 21 Pineapple Pizza ## 3 Charlie 22 Diet Pepsi ## 4 David 23 Korean BBQ ## 5 Eve 19 Sushi AYCE 5.3.1.2 Reading Data from a File Obviously when we have many more data, it would be unrealistic to manually list them out in our code. So instead, we can create a data frame by reading a file. From the R4EnvChem-ProjectTemplate, downloaded in Importing a project, let’s import some real data that was included in the downloaded project by typing the following code into the console: airPol &lt;- read.csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) read.csv() is a useful R built-in function which, as you might guess from its name, can read a .csv file and convert it into a data frame. The data we just imported contains air quality data measured in downtown Toronto around January 2018. The “Column specification” summary printed to the console is a useful feature of read.csv(). It tells you what data type was determined for each column when it was imported. Note that double is simply another term for the numeric data type. Some of the variables are: naps, city, p, latitude, longitude to tell you where the data was measured. data.time for when the measurements were taking. Note this is a datetime, which is a subset of numeric data. The values contained herein correspond to time elements such as year, month, data, and time. pollutant for the chemical measured concentration for the measured concentration in parts-per-million (ppm). We’ve assigned it to the variable airPol. This is so we can reference it and make use of it later on (see below). If we didn’t do this our data would simply be printed to the console which isn’t helpful. Let’s take a look at the first few rows of the data using the head function: head(airPol) ## naps city p latitude longitude date.time pollutant ## 1 60430 Toronto ON 43.70944 -79.5435 2018-01-01 00:00:00 O3 ## 2 60430 Toronto ON 43.70944 -79.5435 2018-01-01 00:00:00 NO2 ## 3 60430 Toronto ON 43.70944 -79.5435 2018-01-01 00:00:00 SO2 ## 4 60430 Toronto ON 43.70944 -79.5435 2018-01-01 01:00:00 O3 ## 5 60430 Toronto ON 43.70944 -79.5435 2018-01-01 01:00:00 NO2 ## 6 60430 Toronto ON 43.70944 -79.5435 2018-01-01 01:00:00 SO2 ## concentration ## 1 3 ## 2 39 ## 3 1 ## 4 1 ## 5 47 ## 6 3 In this data frame, each column is a variable and each row is an observation. So reading the first row, we know that the Toronto 60430 station on 2018-07-01 at midnight measured ambient O3 concentrations of 46 ppm (Note the concentration column isn’t printed due to width). Using head will only output a small chunk of our data for us to see. If you’d like to see it in full, go to the Environment pane and double-click on the airPol variable. 5.3.2 Accessing Data in Subfolders Note that read.csv() requires us to specify the file name, but in the above example we prefixed our file name with \"data/2018...\". This is because the .csv file we want to open is stored in the data subfolder. By specifying this in the prefix, we tell read.csv() to first go to the data sub folder in the working directory and then search for and open the specified data file. What we’ve done above is called relative referencing and it’s a huge benefit of projects. The actual data file is stored somewhere on your computer in a folder like \"C:/User/Your_name/Documents/School/Undergrad/Second_Year/R4EnvChemTemplate/data/2018-01-01_60430_Toronto_ON.csv\". If we weren’t in a project, this is what you’d need to type to open your file, but since we’re working in the project, R assumes the long part, and begins searching for files inside the project folder. Hence, why we only need \"data/2018...\". Not only is this much simpler to type, and but it makes sharing your work with colleagues, TAs, and instructors (and yourself!) much easier. In other words, if you wanted to share your code, you would send the entire project folder (code &amp; data) and the receiver could open it and run it as is. 5.3.3 Other Data Structures R has several other data structures. They aren’t as frequently used, but it’s worth being aware of their existence. Other structures include: Vectors, which contain multiple elements of the same type; either numeric, character, logical, or integer. Vectors are created using c(), which is short for combine. A data frame is just multiple vectors arranged into columns. Some examples of vectors are shown below. num &lt;- c(1, 2, 3, 4, 5) num ## [1] 1 2 3 4 5 char &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;) char ## [1] &quot;blue&quot; &quot;green&quot; &quot;red&quot; log &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE) log ## [1] TRUE TRUE TRUE FALSE FALSE FALSE Lists are similar to vectors in that they are one dimensional data structures which contain multiple elements. However, there are two important differences: (1) lists can contain multiple elements of different types, while vectors only contain a single type of data; and (2) each piece of data in the list is given a name with which we can refer to that data. You can create lists using list(), as illustrated below. # Creating a list with three components: &quot;Greetings&quot;, &quot;someNumbers&quot;, and &quot;someBooleans&quot; hi &lt;- list(&quot;Greetings&quot; = &quot;Hello&quot;, &quot;someNumbers&quot; = c(5,10,15,20), &quot;someBooleans&quot; = c(TRUE, TRUE, FALSE)) hi ## $Greetings ## [1] &quot;Hello&quot; ## ## $someNumbers ## [1] 5 10 15 20 ## ## $someBooleans ## [1] TRUE TRUE FALSE We can access individual components of the list by using the $ operator: # Access the &quot;Greetings&quot; component of list &quot;hi&quot; hi$Greetings ## [1] &quot;Hello&quot; # Access the &quot;someNumbers&quot; component of list &quot;hi&quot; hi$someNumbers ## [1] 5 10 15 20 There are many freely available resources online which dive more in depth into different data structures in R. If you are interested in learning more about different structures, you can check out the Data structure chapter of Advanced R by Hadley Wickham. 5.4 Conditional Statements In programming, it’s often necessary to make decisions and execute certain portions of code based on specific conditions. That’s where conditional statements come into play. In R, the primary mechanism to make decisions is the if-else construct. With it, you can evaluate a condition and, based on whether it’s true or false, choose which code block to execute. 5.4.1 Understanding R Syntax Before diving into conditional statements, let’s take a moment to understand the syntax used in R. R, like many programming languages, uses a combination of parentheses (), curly braces {}, and other symbols to organize and structure the code. Parentheses (): These are primarily used to enclose arguments of functions and conditions in control statements, like ‘if’. For example, in if (x &gt; 5), the condition x &gt; 5 is enclosed in parentheses. Curly Brackets {}: These are used to group multiple lines of code into a block. This is particularly useful in control statements where more than one line of code should be executed based on a condition. The reason the curly bracket might span multiple lines is for readability. It makes it clear where a block of code begins and ends. While it’s possible to write if-else statements without curly brackets if only one statement is being conditioned, it’s good practice to always use them for clarity. Now, with this understanding, let’s move on to how R uses these in conditional statements. 5.4.2 The Basic if Statement x &lt;- 10 # In this example, R checks if x is greater than 5. if (x &gt; 5) { print(&quot;x is greater than 5!&quot;) } ## [1] &quot;x is greater than 5!&quot; 5.4.3 Expanding with else and else if For situations where you want to specify actions for both true and false conditions, you can add an else section. x &lt;- 3 if (x &gt; 5) { print(&quot;x is greater than 5!&quot;) } else { # if x &lt;= 5 print(&quot;x is 5 or less!&quot;) } ## [1] &quot;x is 5 or less!&quot; Here, because x is 3 (which is not greater than 5), R prints “x is 5 or less!”. For situations where multiple conditions need to be evaluated in sequence, you can use the else if construct. This allows you to add more conditions after the initial if. x &lt;- 6 if (x &gt; 10) { print(&quot;x is greater than 10!&quot;) } else if (x &gt; 5) { print(&quot;x is greater than 5 but less than or equal to 10!&quot;) } else { print(&quot;x is 5 or less!&quot;) } ## [1] &quot;x is greater than 5 but less than or equal to 10!&quot; In terms of syntax, it’s important to remember: Always enclose the condition you’re testing within parentheses (). Use curly brackets {} to group the lines of code that should be executed for a particular condition. Make sure each else if or else follows an if or another else if. They cannot stand alone. 5.5 R built-in Functions Built-in functions are the essential tools that allow you to perform a wide range of tasks without having to write the underlying code from scratch. These functions are part of the R language itself and are readily available for your use. In this chapter, you’ve already come across a few built-in functions that are incredibly useful. For instance, you’ve used the read.csv() function to import data from CSV files into your R environment. Additionally, the as.numeric() function has been employed to convert data to numeric format, and the list() function has aided in creating lists to organize and store data elements. 5.5.1 Exploring More Built-In Functions Let’s delve into a few more built-in functions that are integral to your R experience: print(): The print() function displays output on the console. When you want to see the result of an expression or the contents of a variable, print() makes it effortless. print(&quot;Hello, R!&quot;) ## [1] &quot;Hello, R!&quot; mean(): The mean() function calculates the average of a numeric vector. mean(c(5, 10, 15, 20)) ## [1] 12.5 max()/min(): With the max() function, you can effortlessly determine the maximum value within a numeric vector. Similarly, min() function returns the minimum value within a vector. max(c(5, 10, 15, 20)) ## [1] 20 min(c(5, 10, 15, 20)) ## [1] 5 5.5.2 Function Documentation An often unappreciated aspect of packages is that they not only contain functions we can use, but documentation. Documentation provides a description of the function (what it does), what arguments it takes, details, and working examples. Often the easiest way to learn how to use a function is to take a working example and change it bit by bit to see how it works etc. To see documentation check the “help” tab in the “outputs” window or type a question mark in front of a functions name: # Takes you to the help document for the read.csv function ?read.csv You can also write your own functions. Please see Writing custom functions in R for additional details. 5.6 Summary In this chapter we’ve covered: The basics of coding in R including variables, data types, and data structures (notably data.frames). Importing data from your project folder into R Using if-else structure to build a conditional logic Using R built-in functions and opening function documentations Now that you’re familiar with navigating RStudio and some basic R coding, you may have realized that working the console can get real messy, real quick. Read on to Workflows for R Coding where we’ll discuss R workflows to make everyone’s lives easier. 5.7 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["workflows-for-r-coding.html", "Chapter 6 Workflows for R Coding 6.1 Creating or opening an R Markdown document 6.2 Workspace and What’s Real 6.3 Saving R Markdown 6.4 Script formatting 6.5 Viewing Data and Code Simultaneously 6.6 Troubleshooting Error Messages 6.7 Summary 6.8 Exercise", " Chapter 6 Workflows for R Coding In the previous chapter, we conducted our coding in the console, which quickly became unwieldy. To address this, we will transition to using R Markdown .Rmd files, which we briefly talked about in A Sneak Peek at .Rmd Files. Instead of running code interactively in the console, we write code blocks within the .Rmd files, creating a comprehensive document that others can follow. 6.1 Creating or opening an R Markdown document To start an R Markdown document: Go to File -&gt; New File -&gt; R Markdown. Then save your document by going to File -&gt; Save As…. - Make sure to save your file with the .Rmd suffix. - Save your ‘.Rmd’ file in your project folder, so you can easily refer to data files. We’ve also provided an example script in the R4EnvChem project template. Assuming you’re currently in the template project you can open the script as follows: Go to File -&gt; Open File -&gt; open the Rscript-example.Rmd file or double click the Rscript-example.Rmd in the files pane. This action will open a new pane above the console, dedicated to writing your R Markdown content. 6.2 Workspace and What’s Real We’ve already mentioned the environment pane that displays objects present in your R session. While they are useful to work with, they’re not considered real. That is to say, if you close your R session, those objects will be lost. And while RStudio allows you to save a working environment (and its associated objects), it’s crucial to understand that only your saved scripts/markdown documents are real. You can’t readily share your working environment, and even so it’s bad practice as you may reference a previous iteration of an object giving you erroneous results. Think back to the chemistry labs: you may jot notes down on loose leaf, but only what’s written in your lab book is considered real… well that’s how it’s supposed to work anyways. The idea is everything you need can be generated from the original data and the instructions in your R script/markdown document. Anyone should be able to take your data and your code and get the same results you got. This is paramount for the reproducibility of your work and your results. 6.3 Saving R Markdown To save an R Markdown document: Navigate to File -&gt; Save or use the ‘Save’ icon in the top left corner of your document. Content saved to an .Rmd file is considered real and self-contained. Variables, plots, or datasets that appear in your workspace or the Environment window aren’t self-contained. Whenever you close RStudio, any objects in R that are not considered real will be lost in that R session. Furthermore when you need to share your code (for school or publication) you’ll need to share your data and your script, but never your work-space. This is to increase predictability and helps people (and you) to make sure your work is reproducible, an underappreciated hallmark of science. 6.3.1 What Should I Save? At this point in the chapter, two things should be clear: R Markdown documents saved to .Rmd files are the real record of your work. Data in your workspace/environment are not real, and will not be available to you after you close and re-open RStudio unless you re-run the code used to generate the workspace. So what is important to save in R, and how often should you save these files? Save the R Markdown scripts you write, and do so regularly. Even minor changes are worth saving before closing RStudio, as it’s easy to forget those small differences upon return. Ensuring that even if you lose an object in your workspace, your R Markdown script contains the code needed to recreate that object. Generate the object before referencing it in subsequent commands. This ensures that variables are generated in the workspace before being referenced by later commands when running scripts from top to bottom. By adhering to these practices, you ensure your R Markdown documents remain accurate, your code is complete, and your work remains reproducible. 6.3.2 Saving Data Frames In some cases, your code may be used to generate large data frames which require quite a bit of time to create. It can be quite tedious to re-run the code used to generate these large data frames every time you open RStudio, and you might find yourself wanting to save the data to a real file that you can simply import the next time you open the application. Also, you may be finished with your analysis and want to save the final data. You can save the data contained in your data frame as a .csv file using write.csv(). # dummy data frame to save df &lt;- data.frame(x = c(1,2,3), y = c(&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;)) # save the data frame to a file write.csv(df, file = &quot;data/testData.csv&quot;) Breaking down the code above: we created a dummy data frame df; in reality you’ll use a data frame from your analysis. we called write.csv() on two values df specifies we want to save the data frame df file = \"data/testData.csv\" specifies where we want the file to save (in the data sub-directory, more on this later), and what our file will be called (testData.csv). It’s important to specify the file extension so R knows how to save it. 6.4 Script formatting You should now be familiar with how to open the Scripts window, as well as some of the advantages of typing your code into this window rather than into the console directly. Before you write your first script, let’s review some basic script formatting. Before you enter any code into your script, it is good practice to fill the first few lines with text comments which indicate the script’s title, author, and creation or last edit date. You can create a comment in a script by typing # before any text. An example is given below. # Title: Ozone time series script # Author: Georgia Green # Date: January 8, 2072 Below your script header, you should include any packages that need to be loaded for the script to run. Including the necessary packages at the top of the script allows you, and anyone you share your code with, to easily see what packages they need to install. This also means that if you decide to run an entire script at once, the necessary packages will always be loaded before any subsequent code that requires those packages to work. The first few lines of your scripts should look something like the following. # Title: Example R Script for Visualizing Air Quality Data # Author: John Guy Rubberboots # Date: 24 June 2021 # 1. Packages ---- # Install tidyverse if you haven&#39;t already # install.packages(&quot;tidyverse&quot;) library(tidyverse) The rest of your script should contain the R code you want to run, and text comments in between different chunks of code to remind yourself what the different sections of code are for (e.g., # 1. Packages ---- in the above example). This also makes it easy for anyone you share your code with to understand what you’re trying to do with different sections within the script. You can also use headers and sub-headers in your scripts using #, ##, and ### before your text and --- after as shown below: # Section ---- ## Subsection ---- ### Sub-subsection ---- Headings and subheadings are picked up by RStudio and displayed in the Document Outline box. You can open the Document Outline box by clicking the button highlighted in the image below. Use of these headings allows easy navigation of long scripts, as you can navigate between sections using the Document Outline box. Example script headings, document outlines, and comments. Note the “—” which tells RStudio this comment is to be treated as a script heading. 6.5 Viewing Data and Code Simultaneously Before we get into more about coding and workflows, you may find yourself wanting to be able to view your scripts and data side-by-side. You can open a script, plot, or data set in a new window by clicking and dragging the tab in RStudio or by clicking the button highlighted in the image below. How to open an R script/plot/data set in a new window. 6.6 Troubleshooting Error Messages In a previous section, you were introduced to your first error message in R, and we briefly discussed how to resolve the issue. As you begin to code, many of your errors will be routine syntax error such as unmatched parenthesis (the dreaded “Incomplete expression:”). Fortunately, RStudio will highlight any syntax errors in your code with a red squiggly line and an ‘x’ in the side bar, as shown below. You can hover over the ‘x’ to see what is causing the error. In the above message, R is telling you that it is not sure what to do with b. As mentioned previously, variable assignment is done in the format name &lt;- assignment. However, in the above example, the variable assignment statement is written as name name &lt;- assignment. Since variable names cannot contain spaces, R reads a b as two separate input variable names, not as a single string. If you wanted to assign a value of 0 to both a and b, you would need to write the statement once per variable, as shown below. a &lt;- 0 b &lt;- 0 Let’s look at another example. Some functions require you to write code with nested parentheses. A good example would be the aes() argument, which is aesthetic mapping, that is called inside of ggplot(), as shown below. # plot ozone concentration vs. time ggplot(data = airPol, aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() If you were to forget one of the parentheses in the previous line of code, RStudio would highlight it similar to below: Here R is telling you that you have an unmatched opening bracket. To resolve the error, simply add a closing bracket to match. The expected ',' after expression is a common error that you will see accompanying unmatched opening brackets. Sometimes you might get this error in the console after running code that is missing a bracket somewhere. It is good practice to check your parentheses a few times before running your code to make sure that all the commands are closed, and that R doesn’t keep waiting for you to continue inputting code after you’ve click Run. If you notice that the &gt; in your R console has turned into a +, this is likely because you’ve just run a command that is missing a closing bracket, and thus, R is not aware that your code is finished. Simply input a closing bracket into the console, and the &gt; should return. While the script window is very useful for pointing out syntax errors in your code, there are many other errors that can arise in RStudio which the script window is not able to capture. These are generally errors that arise from trying to execute your code, rather than from mistakes in your syntax. The following is a prime example of such an error. q &lt;- 8 + &quot;hi&quot; ## Error in 8 + &quot;hi&quot;: non-numeric argument to binary operator Here we are trying to add a numeric value (8) to a character string (“hi”), then set the sum of the two to variable q. R has given us an error in return, because there is no logical way for R to add a numeric value to non-numeric text. The error indicates that we have passed a non-numeric argument to binary operator, meaning we have used a non-numeric data type for an expression which is exclusively reserved for numeric data. It is important to be aware of error codes as many functions require specific data types as their inputs. Remember, you can always consult the function documentation using ?. 6.7 Summary In this chapter we’ve covered: R workflows in the context of projects and markdown documents What’s considered real when working in RStudio How to format your markdown for legibility (Remember you’re the one who’s going to be stuck rereading it!) Troubleshooting some common error messages Now that you’re familiar with the above, we’ll introduce Using R Markdown, a way to combine your R code, its outputs, and your writing all in one dynamic document (like your lab reports!). 6.8 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["r-packages.html", "Chapter 7 R Packages 7.1 What are R packages? 7.2 How to use R packages 7.3 Tidyverse: The Golden Toolbox of R 7.4 Summary 7.5 Exercise", " Chapter 7 R Packages Before we do any real data work with R, now is the good time to introduce you to R packages. R, as a powerful programming language for statistics and data analysis, boasts a rich ecosystem of packages. In this chapter, we’ll demystify what these packages are, their importance, and how to utilize them efficiently. 7.1 What are R packages? In a programming context, a package is akin to a toolbox. It contains sets of functions and data sets crafted to perform specific tasks, similar to how a toolbox contains various tools for different jobs. Instead of building every tool from scratch each time you need it, you can simply open your toolbox and grab the necessary instrument. In R, these tools come in the form of functions and data sets bundled inside packages. Packages are previously written snippets of code that extend the capabilities of base R. Typically packages are created to address specific issues or workflows in different types of analysis. 7.1.0.1 Benefits of using packages: Efficiency: Why reinvent the wheel? Packages save time by offering tried and tested functions for specific tasks. Community Support: R packages often have a strong community of developers and users. This means frequent updates, thorough documentation, and a network of users to answer questions and offer support. Versatility: The vast library of R packages means that you have tools at your disposal for almost every conceivable task or analysis. 7.2 How to use R packages 7.2.1 How to install packages Before using a package, you must first install it. This is a one-time process unless you need to update the package to a newer version. To install a package, use the install.packages() function. This book will make frequent use of a family of packages called the tidyverse (a popular collection of packages for data manipulation). These packages all share a common thought process and integrate naturally with one another. If you want to install the package named “tidyverse”, you would use the following code: install.packages(&quot;tidyverse&quot;) 7.2.2 How to load packages After installing a package, you need to tell R to load the package so that you can actually use it! You do so with the library() function. For example, to load the tidyverse package: library(tidyverse) The output shows us which packages are included in the tidyverse() and their current version numbers, as well as conflicts (where functions from different packages share the same name). Don’t worry about these for now. After this, all the functions and data sets contained in the “tidyverse” package are available for you to use in your session. If you’re ever uncertain about how to use a particular function, the R community and the package’s documentation are excellent resources. For example, you can take a look at the official tidyverse documentation here. 7.2.3 Calling specific functions We’ve called functions like ggplot() and read_csv() from the ggplot2 and readr packages, respectively. When we did so, they were implicitly imported when we called library(tidyverse). What library does is import all of the functions within a package into the R workspace, so we can simply refer to them by name later on. Sometimes you’ll want to be explicit to which function you call, as you can run into conflicts where different functions from different packages have the same name. Either way, to explicitly call a function from a specific package you type the package name, followed by ::, and the function name. For example, we can use read_csv() by simply typing readr::read_csv(), without needing to load the readr package. 7.3 Tidyverse: The Golden Toolbox of R We’ve emphasized before that tidyverse is an indispensable collection of R packages tailored specifically for data science and in-depth data analysis. As you proceed, you’ll find that our chapters heavily, if not exclusively, rely on its functionalities. Let’s dive into some of its pivotal functions. 7.3.1 Loading the Tidyverse Before using the functions from the tidyverse, make sure to load the entire collection. library(tidyverse) 7.3.2 Data Visualization One of the strengths of the tidyverse is data visualization. The example below shows how you can iteratively build plots by adding layers of details. # ggplot2 built-in dataset on fuel economy data(mpg) # draw a scatterplot ggplot(mpg, aes(x=displ, y=hwy)) + geom_point() 7.3.3 Data Manipulation The tidyverse provides a set of functions to help solve common data manipulation challenges. The syntax is intuitive and readable, which simplifies both writing and understanding the code. filtered_mpg &lt;- filter(mpg, class == &quot;suv&quot;) summarize(filtered_mpg, mean_hwy = mean(hwy)) ## # A tibble: 1 × 1 ## mean_hwy ## &lt;dbl&gt; ## 1 18.1 Code explanation (optional for now): The filter() function is used to extract a subset of rows from a data frame based on logical conditions. It returns all rows where the condition is TRUE. The summarize() function is used to create summary statistics for different variables. You provide named arguments where the name will be the name of a new column, and the value will be the summary statistic to compute. 7.3.4 Efficient Data Reading The tidyverse allows for efficient reading of tabular data, such as csv files. For instance, instead of using the R built-in function read.csv(), you can employ a similar function, read_csv(), from tidyverse to quickly import a csv file as a data frame. This function provides more flexibility and is optimized for faster performance. 7.4 Summary In this chapter, we delved into the world of R packages, understanding their significance and advantages over built-in R functions. We highlighted the functionalities of tidyverse, showcasing practical examples. Armed with this foundational knowledge of R packages, we’re now poised to harness their capabilities in our R programming journey. 7.5 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["loops-and-functions-in-r.html", "Chapter 8 Loops and Functions in R 8.1 Loops 8.2 Writing custom functions in R 8.3 Further reading", " Chapter 8 Loops and Functions in R In this chapter, we will delve into two fundamental concepts in R programming: loops and writing functions. Loops are a powerful tool for iterating over a sequence, making them essential for tasks that require repetitive operations. Writing functions, on the other hand, allows you to create reusable blocks of code, enhancing the efficiency and organization of your programming projects. By the end of this chapter, you will gain a solid understanding of how to effectively use for loops to automate repetitive tasks and how to write your own functions in R, thereby elevating your coding skills to a new level. 8.1 Loops Repetition is a key component of programming. Loops enable you to execute the same piece of code multiple times, making data processing more efficient. If you have a previous programming experience in any language, these syntax should look familiar. 8.1.0.1 The for loop The for loop in R is used to iterate over elements in a vector or list. data &lt;- c(1, 2, 3, 4, 5) for (i in data) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 This will print numbers from 1 to 5. The loop iterates over each element in the vector c(1, 2, 3, 4, 5), setting the value to i and executing the code inside the loop. 8.1.0.2 The while loop The while loop continues executing as long as a specified condition remains true. count &lt;- 1 while (count &lt;= 5) { print(count) count &lt;- count + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 In this example, the loop will keep printing and incrementing the value of counter until counter is no longer less than or equal to 5. Be cautious with while loops: if the condition never becomes false, the loop will run indefinitely! 8.2 Writing custom functions in R Functions form the backbone of most programming languages, and R is no exception. While R provides a rich library of built-in functions, there are times when you’ll need to define your own. Custom functions in R allow you to encapsulate a series of commands into a single reusable unit. Why write custom functions? Reusability: Once you’ve written and tested a function, you can use it repeatedly without having to retype or copy-paste the same lines of code. Maintainability: Changes can be made in one place (inside the function) rather than at multiple locations where the code might be used. Clarity: Well-named functions can make your main code more readable, as they abstract away the complexity. 8.2.0.1 Simple example add_two &lt;- function(a, b) { # Add a and b and return the value added &lt;- a + b return(added) } add_two is the custom name of the function. a and b are the inputs to the function. You can name them x and y, or anything else, as long as they are kept consistent throughout the function definition. You can also change the number of inputs for your function. The return statement specifies the output of your function. If omitted, the function will return the result of the last expression evaluated. You can run this function with various choices of a and b: add_two(1, 3) ## [1] 4 add_two(20, 35.5) ## [1] 55.5 At this time, we shall keep it simple. You will eventually see more complex usages of custom functions. 8.3 Further reading This chapter has been intentionally succinct. We’ve omitted several other aspects of programming in R such as for loops, and other aspects of iterative programming. To get a better sense of programming in R and to learn more, please see the following links: Chapter 19: Functions, Chapter 20: Vectors, and Chapter 21: Iteration of R for Data Science by H. Wickham and G. Grolemund. Hands-on Programming in R by G. Grolemund for a more in-depth (but still approachable) take on programming in R. "],["intro-to-data-analysis.html", "Chapter 9 Intro to Data Analysis 9.1 Example Data 9.2 Sneak Peek at Data Analysis 9.3 Further Reading", " Chapter 9 Intro to Data Analysis Now, we will embark on a comprehensive journey through the data analysis process, focusing on the essential steps of data wrangling and advanced analytical techniques. The chapter is designed to equip you with the necessary skills to use R effectively for organizing and transforming your data, a crucial foundation for any data analysis project. We will delve into the core workflow that is applicable to every data analysis task, regardless of its complexity or duration. This workflow is not just a one-time learning curve but a set of skills you will repeatedly use across various projects. The explicit workflow we’ll be teaching was originally described by Wickham and Grolemund, and consists of six key steps: Import is the first step and consist of getting your data into R. Seems obvious, but doing it correctly will save you time and headaches down the line. Tidy refers to organizing your data in a tidy manner where each variable is a column, and each observation a row. - Transform is anything you do to your data including any mathematical operations or narrowing in on a set of observations. It’s often the first stage of the cycle as you’ll need to transform your data in some manner to obtain a desired plot. Visualize is any of the plots/graphics you’ll generate with R. Take advantage of R and plot often, it’s the easiest way to spot an error. Model is an extension of mathematical operations to help understand your data. The linear regressions needed for a calibration curve are an example of a model. Communicate is the final step and is where you share the knowledge you’ve squeezed out of the information in the original data. Import, Tidy, and Transformation go hand-in-hand in a process called wrangling, which encompasses all of the steps needed to get your data ready for analysis. It’s often the most tedious and frustrating, hence “wrangling” (it’s a fight…), but once done make the subsequent cycle of understanding your data via transformation, visualizations, and modelling much easier and more predictable. 9.1 Example Data Throughout this chapter and the next chapter we’ll be making use of a couple of example datasets. These datasets are all available in the data subfolder of the R4EnvChem Project Template. If you haven’t already, read Importing a project for instructions on downloading the repository and data. 9.2 Sneak Peek at Data Analysis As a preview, we’ll explore how data analysis can provide insights into real-world phenomena. We’ll use the storm dataset from the tidyverse suite in R to investigate the patterns and relationships of storms over the years. 9.2.1 Setting Up Let’s begin by loading our dataset and essential packages. library(tidyverse) # Import the storm dataset data(storms) Let’s see how the data looks: DT::datatable(storms) 9.2.2 Tidying Our Data Now that you saw how the data looks, before delving into deeper analysis, it’s important to have a general sense of our data. To narrow our focus, let’s consider the storm name, year, month, day, lat, long, and wind speed. # 1. Selecting relevant variables selected_storms &lt;- storms %&gt;% select(name, year, month, day, lat, long, wind) DT::datatable(selected_storms) Next, instead of having separate columns for year, month, and day, it might be more useful to have a single date column. # 2. Creating a unified date column storms_with_date &lt;- selected_storms %&gt;% unite(&quot;date&quot;, year, month, day, sep = &quot;-&quot;) %&gt;% mutate(date = as.Date(date, format = &quot;%Y-%m-%d&quot;)) DT::datatable(storms_with_date) Column names should be self-explanatory. Let’s rename wind to wind_speed_knots. # 3. Renaming columns for clarity tidied_storms &lt;- storms_with_date %&gt;% rename(wind_speed_knots = wind) Now that our data is tidied up, let’s see if we can do more in-depth analysis. 9.2.3 Investigating Storm Patterns 9.2.3.1 Analyzing powerful storms For safety and preparedness reasons, meteorologists are often interested in particularly powerful storms. Let’s identify storms with wind speeds exceeding 100 knots. powerful_storms &lt;- tidied_storms %&gt;% filter(wind_speed_knots &gt; 100) %&gt;% arrange(desc(wind_speed_knots)) 9.2.3.2 Yearly trends How has the frequency of these powerful storms changed over the years? We can group our data by year to answer this. yearly_storms &lt;- powerful_storms %&gt;% mutate(year = year(date)) %&gt;% group_by(year) %&gt;% summarise(storm_count = n()) 9.2.4 Visualization: Geographical Distribution One of the key aspects of understanding storms is analyzing where they occur. Using our powerful storms data, let’s plot a scatterplot of latitude versus wind_speed_knots to visualize their geographical distribution and intensity. ggplot(data = powerful_storms, aes(x = lat, y = wind_speed_knots)) + geom_point(aes(color = wind_speed_knots), alpha = 0.6) + ggtitle(&quot;Distribution and Intensity of Powerful Storms&quot;) + xlab(&quot;Latitude&quot;) + ylab(&quot;Wind Speed (knots)&quot;) + scale_color_gradient(low = &quot;blue&quot;, high = &quot;red&quot;) Through this brief exploration, we’ve seen how data analysis can provide insights into storm patterns. With more advanced techniques, which we’ll explore in the subsequent chapters, we can delve even deeper, helping inform decisions, ensuring preparedness, and advancing our understanding of meteorological phenomena. 9.3 Further Reading In case it hasn’t been apparent enough, this entire endeavour was inspired by the R for Data Science reference book by Hadley Wickham and Garrett Grolemund. Every step described above is explored in more detail in their book, which can be read freely online at https://r4ds.had.co.nz/. We strongly encourage you to read through the book to supplement your R data analysis skills. "],["importing-your-data-into-r.html", "Chapter 10 Importing Your Data Into R 10.1 csv files 10.2 read_csv 10.3 Importing other data types 10.4 Saving data 10.5 Further Reading 10.6 Exercise", " Chapter 10 Importing Your Data Into R Unlike Excel, you can’t copy and paste your data into R (or RStudio). Instead you need to import your data into R so you can work with it. This chapter will discuss how your data is stored, and how to import it into R (with some accompanying nuances). 10.1 csv files While there are a myriad of ways data is stored, instruments often record results in a proprietary vendor format, the data you’re likely to encounter in an undergraduate lab will be in the form of a csv or comma-separated values file. As the name implies, values are separated by commas (go ahead and open any csv file in any text editor to observe this). Essentially you can think of each line as a row and commas as separating values into columns, which is exactly how R and Excel handle csv files. 10.2 read_csv Importing a csv file into R simply requires the read_csv tidyverse function. The first input to this function is the most important as it’s the file path. Recall that R, unless specified, uses relative referencing. So in the example below we’re importing the ATR_plastics.csv from the data subfolder in our project by specifying \"data/ATR_plastics.csv\" and assigning it to the variable atr_plastics. library(tidyverse) atr_plastics &lt;- read_csv(&quot;data/ATR_plastics.csv&quot;) ## Rows: 28628 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sample ## dbl (2): wavenumber, absorbance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. A benefit of using read_csv is that it prints out the column specifications with each column’s name (how you’ll reference it in code) and the column value type. Columns can have different data types, but a data type must be consistent within any given column. Having the columns specifications is a good way to ensure R is correctly reading your data. The most common data types are: int for integer values (-1,1, 2, 10, etc.) dbl for doubles (decimals) or real numbers (-1.20, 0.0, 1.200, 1e7, etc.) chr for character vectors or strings (“A”, “chemical”, “Howdy ma’am”, etc.) lgl for logical values, either TRUE or FALSE We can inspect this dataset either through the Environment pane or with the head() function. head(atr_plastics) ## # A tibble: 6 × 3 ## wavenumber sample absorbance ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 550. EPDM 0.212 ## 2 550. Polystyrene 0.0746 ## 3 550. Polyethylene 0.000873 ## 4 550. Sample: Shopping bag 0.0236 ## 5 551. EPDM 0.212 ## 6 551. Polystyrene 0.0746 As you can see, the head() function, by default, shows the first six rows of the data frame. If you want to inspect more or fewer rows, you can provide an optional n argument like head(data, n=10). Note the column specifications under the column name. Also note how the first line of the ATR_plastics.csv has been interpreted as columns names (or headers) by R. This is common practice, and gives you a handle by which you can manipulate your data. If you did not intend for R to interpret the first row as headers you can suppress this with the additional argument col_names = FALSE. head(read_csv(&quot;data/ATR_plastics.csv&quot;, col_names = FALSE)) ## Rows: 28629 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): X1, X2, X3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 6 × 3 ## X1 X2 X3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 wavenumber sample absorbance ## 2 550.0952 EPDM 0.2119556 ## 3 550.0952 Polystyrene 0.07463058 ## 4 550.0952 Polyethylene 0.000873196 ## 5 550.0952 Sample: Shopping bag 0.02364882 ## 6 550.5773 EPDM 0.2124079 Note in the example above that since the headers are now considered data, and are composed of a string of characters, the entire column is then interpreted as character values. This will happen if a single non-numeric character is introduced in the column, so beware of typos when recording data! If we wanted to skip rows (i.e. to avoid blank rows at the top of our csv file), we can use the skip = &lt;n&gt; to skip &lt;n&gt; rows: head(read_csv(&quot;data/ATR_plastics.csv&quot;, col_names = FALSE, skip = 1)) ## Rows: 28628 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): X2 ## dbl (2): X1, X3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 6 × 3 ## X1 X2 X3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 550. EPDM 0.212 ## 2 550. Polystyrene 0.0746 ## 3 550. Polyethylene 0.000873 ## 4 550. Sample: Shopping bag 0.0236 ## 5 551. EPDM 0.212 ## 6 551. Polystyrene 0.0746 Note in the example above that we skipped our headers, so read_csv() created placeholder headers (X1, X2, etc.). Another useful function to inspect data is tail(), which displays the last six rows of a data frame. Similarly, it accepts an optional n argument to specify the number of rows you want to view. tail(atr_plastics) ## # A tibble: 6 × 3 ## wavenumber sample absorbance ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4000. Polyethylene 0.000125 ## 2 4000. Sample: Shopping bag 0.0664 ## 3 4000. EPDM 0.113 ## 4 4000. Polystyrene 0.0706 ## 5 4000. Polyethylene 0.000195 ## 6 4000. Sample: Shopping bag 0.0663 10.2.1 Tibbles vs. data frames Quick eyes will notice the first line outputted above is # A tibble: 6 x 5. Tibbles are a variation of data frames introduced in Data Frames, but built specifically for the tidyverse. While data frames and tibbles are often interchangeable, it’s important to be aware of the difference in case you do run into a rare conflict. In these situations you can readily transform a tibble into a data frame by coercion with the as.data.frame() function, and vice-versa with the as_tibble() function. class(as.data.frame(atr_plastics)) ## [1] &quot;data.frame&quot; 10.3 Importing other data types There are other functions to import different types of tabular data which all function like read_csv, such as read_tsv for tab-separate value files (.tsv) and read_excel and read_xlsx for Excel files. Warning: most Excel files have probably been formatted for legibility (i.e. merged columns), which can lead to errors when importing into R. If you plan on importing Excel files, it’s probably best to open them in Excel to remove any formatting, and then save as .csv for smoother importing into R. 10.4 Saving data As you progress with your analysis you may want to save intermediate or final datasets. This is readily accomplished using the write_csv() tidyverse function. Similar rules apply to how we used read_csv, but now the second argument specifies the save location and file name, while the first argument is which tibble/data.frame we’re saving. Note that R will not create a folder this way, so if you’re saving to a subfolder you’ll have to make sure it exists or create it yourself. write_csv(atr_plastics, &quot;data/ATRSaveExample.csv&quot;) 10.5 Further Reading See Chapters 10 and 11 of R for Data Science for some more details on tibbles and read_csv. 10.6 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["tidying-your-data.html", "Chapter 11 Tidying Your Data 11.1 What is tidy data? 11.2 Tools to tidy your data 11.3 Tips for recording data 11.4 Further reading 11.5 Chapter References 11.6 Exercise", " Chapter 11 Tidying Your Data You might not have explicitly thought about how you store your data, whether working in Excel or elsewhere. Data is data after all. But having your data organized in a systematic manner that is conducive to your goal is paramount for working not only with R, but all of your experimental data. This chapter will introduce the concept of tidy data, and how to use some of the tools in the dplyr package to get there. Lastly we’ll offer some tips for how you should record your data in the lab. A bit of foresight and consistency can eliminate hours of tedious work down the line. 11.1 What is tidy data? Tidy data has “…each variable in a column, and each observation in a row…” (Hadley Wickham 2014) This may seem obvious to you, but let’s consider how data is often recorded in lab, as exemplified in Figure 11.1A. Here the instrument response of two chemicals (A and B) for two samples (blank and unknown) are recorded. Note how the samples are on each row and the chemical are columns. However, someone else may record the same data differently as shown in Figure 11.1B, with the samples occupying distinct columns, and the chemicals in rows. Either layout may work well, but analyzing both would require re-tooling your approach. This is where the concept of tidy data comes into play. By reclassifying our data into observations and variables we can restructure our data into a common format: the tidy format (Figure 11.1C). Figure 11.1: (A and B) The same data in different formats. (C) The data organized with clear variables and observations. Organizing data into distinct variables (like Sample, Chemical, and Reading) enhances clarity. This reorganization doesn’t change the data but rearranges it for better compatibility with analytical tools. 11.2 Tools to tidy your data Now one of the more laborious parts of data science is tidying your data. If you can, follow the tips in the Tips for recording data section, but the truth is you often won’t have control. To this end, the tidyverse offers several tools, notable dplyr (pronounces “dee-plier”), to help you get there. Let’s revisit our spectroscopy data from the previous chapter, but in a slightly different format. We will learn a variety of tools to change the format of a dataset as we move through Part 3 of this resource, but for now just observe some of the changes we make to the ATR plastics data in comparison to what we did in the previous chapter. atr_plastics &lt;- read_csv(&quot;data/ATR_plastics_original_wide.csv&quot;) DT::datatable(atr_plastics) As we can see this our ATR spectroscopy results of several plastics, as recorded for a CHM 317 lab, is structured similarly to the example in Figure 11.1A. The ATR absorbance spectra of the four plastics are recorded in separate columns. Again, this format makes intuitive sense when recording in the lab, and for working in Excel, but isn’t the friendliest with R. When making plots with ggplot, we can only specify one y variable. In the example plot below it’s the absorbance spectrum of Polystyrene. However, if we wanted to plot the other spectra for comparison, we’d need to repeat our geom_point call. In Ggplot Basic Visualizations after we’ve tidied this data we will see how easily we can make more interesting and informative plots with the data in this new format. # Plotting Polystyrene absorbance spectra ggplot(data = atr_plastics, aes(x = wavenumber, y = Polystyrene)) + geom_point() # Plotting Polystyrene and Polyethylene absorbance spectra ggplot(data = atr_plastics, aes(x = wavenumber, y = Polystyrene)) + geom_point() + geom_point(data = atr_plastics, aes(x = wavenumber, y = Polyethylene)) 11.2.1 Selection helpers There are multiple ways to select columns and variables with the dplyr package. For a complete rundown of other useful helper functions please see Subset columns using their names and types. starts_with() for selecting columns from a prefix, and contains() for selecting columns that contain a string are two of the most useful. 11.2.2 Separating columns Sometimes your data has already been recorded in a tidy-ish fashion, but there may be multiple observations recorded under one apparent variable, something like 1 mM for concentration. As it stands we cannot easily access the numerical value in the concentration recording because R will encode this as a string due to the mM. We can separate data like this using the separate function. Consider the following example scenario. You have sample names you’ll pass along to your TA where you crammed as much information as possible into that name so you and your TAs know exactly what’s being analyzed. In this example, the sample name contains the location (Toronto), the chemical measured (O3 or NO2) and the replicate number (i.e. 1). # Example with multiple encoded observations sep_example &lt;- data.frame(&quot;sample&quot; = c(&quot;Toronto_O3_1&quot;,&quot;Toronto_O3_2&quot;, &quot;Toronto_NO2_1&quot;), &quot;reading&quot; = c(&quot;10&quot;, &quot;22&quot;, &quot;30&quot;)) sep_example ## sample reading ## 1 Toronto_O3_1 10 ## 2 Toronto_O3_2 22 ## 3 Toronto_NO2_1 30 Using the separate function we can split up these three observations so we can properly group our data later on in our analysis. # Separating observations separated_data &lt;- separate( sep_example, col = sample, into = c(&quot;location&quot;, &quot;chemical&quot;, &quot;replicateNum&quot;), sep = &quot;_&quot;, convert = TRUE) separated_data ## location chemical replicateNum reading ## 1 Toronto O3 1 10 ## 2 Toronto O3 2 22 ## 3 Toronto NO2 1 30 Let’s break down what we did with the separate function: The first input, sep_example, is the data frame that we’re operating on. col = sample specifies we’re selecting the sample column. into = c(...) specifies what columns we’re separating our name into. sep = \"_\" specifies that each element is separated by an underscore (_). convert = TRUE converts the new columns to the appropriate data format. In the original column, the replicate number is a character value because it’s part of a string, convert ensures that it’ll be converted to a numerical value. 11.2.3 Uniting/combining columns The opposite of the separate function is the unite function. You’ll use it far less often, but you should be aware of it as it may come in handy. You can use it for combining strings together, or prettying up tables for publication/presentations as shown in Summarizing Data. # Uniting observations united_data &lt;- unite(separated_data, col=sample_reunited, c(&quot;location&quot;, &quot;chemical&quot;, &quot;replicateNum&quot;), sep = &quot;_&quot;, remove = TRUE) united_data ## sample_reunited reading ## 1 Toronto_O3_1 10 ## 2 Toronto_O3_2 22 ## 3 Toronto_NO2_1 30 You can read more about the unite function here. 11.2.4 Renaming columns/headers Sometimes a name is lengthy, or cumbersome to work with in R. While something like This_is_a_valid_header is valid and compatible with R and tidyverse functions, you may want to change it to make it easier to work with (i.e. less typing). First, here’s an example with some awkward column names: bad_col_names &lt;- data.frame(&quot;UVVis_Wave_Length_nM&quot; = c(500, 501), &quot;Absorbance&quot; = c(1, 0.999)) colnames(bad_col_names) ## [1] &quot;UVVis_Wave_Length_nM&quot; &quot;Absorbance&quot; Use rename() to change the column name and save the result to a new data frame: renamed_data &lt;- rename(bad_col_names, wavelength_nM = UVVis_Wave_Length_nM) # Inspect the column names of the renamed data frame colnames(renamed_data) ## [1] &quot;wavelength_nM&quot; &quot;Absorbance&quot; 11.2.5 Chaining multiple operations So far we learned some standalone functions that can tidy up your data. But what if you want to do multiple of these operations to a dataset? Let’s start by talking about the seemingly intuitive but tedious approach. We can transform data by breaking down the process into individual steps: selected_data &lt;- select(atr_plastics, wavenumber, EPDM, `Sample: Shopping bag`) atr_plastics_transformed &lt;- rename(selected_data, Wave_Num = wavenumber) atr_plastics_transformed ## # A tibble: 7,157 × 3 ## Wave_Num EPDM `Sample: Shopping bag` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.0236 ## 2 551. 0.212 0.0238 ## 3 551. 0.213 0.0239 ## 4 552. 0.213 0.0239 ## 5 552. 0.214 0.0240 ## 6 553. 0.214 0.0240 ## 7 553. 0.215 0.0241 ## 8 553. 0.215 0.0241 ## 9 554. 0.216 0.0242 ## 10 554. 0.216 0.0242 ## # ℹ 7,147 more rows Now, let’s see how we can transform the same atr_plastics tibble using the %&gt;% operator by chaining operations. atr_plastics_transformed &lt;- atr_plastics %&gt;% select(wavenumber, EPDM, `Sample: Shopping bag`) %&gt;% rename(Wave_Num = wavenumber) atr_plastics_transformed ## # A tibble: 7,157 × 3 ## Wave_Num EPDM `Sample: Shopping bag` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.0236 ## 2 551. 0.212 0.0238 ## 3 551. 0.213 0.0239 ## 4 552. 0.213 0.0239 ## 5 552. 0.214 0.0240 ## 6 553. 0.214 0.0240 ## 7 553. 0.215 0.0241 ## 8 553. 0.215 0.0241 ## 9 554. 0.216 0.0242 ## 10 554. 0.216 0.0242 ## # ℹ 7,147 more rows We will formally introduce the unfamiliar operator %&gt;% (pipe) in the next chapter The Pipe: Chaining Functions Together. For now, just remember that there is a way to chain your functions like the above example! 11.3 Tips for recording data In case you haven’t picked up on it, tidying data in R is much easier if the data is recorded consistently. You can’t always control how your data will look, but in the event that you can (i.e. your inputting the instrument readings into Excel on the bench top) here are some tips to make your life easier: Be consistent. If you’re naming your samples make sure they all contain the same elements in the same order. The sample names Toronto_O3_1 and Toronto_O3_2 can easily be broken up as demonstrated in Separating columns; O3_Toronto_1, TorontoO32, and Toronto_1 can’t be. Use as simple as possible headers. Often you’ll be pasting instrument readings into one .csv using Excel on whatever computer records the instrument readings. In these situations it’s often much easier to paste things in columns. We will learn more about how to do this in the next chapter. Make sure data types are consistent within a column. This harks back to the Importing your data into R chapter, but a single non-numeric character can cause R to misinterpret an entire column leading to headaches down the line. Save your data in UTF-8 format. Excel and other programs often allow you to export your data in a variety of .csv encodings, but this can affect how R reads when importing your data. Make sure you select UTF-8 encoding when exporting your data. 11.4 Further reading As always, the R for Data Science book goes into more detail on all of the elements discussed above. Topics covered here are explored in more detail in Chapter 12: Tidy Data. 11.5 Chapter References 11.6 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["transform-data-manipulation.html", "Chapter 12 Transform: Data Manipulation 12.1 Selecting by row or value 12.2 Arranging rows 12.3 Selecting column name 12.4 Deleting Columns or Rows 12.5 Adding new variables 12.6 Group and summarize data 12.7 The Pipe: Chaining Functions Together 12.8 Further reading 12.9 Exercise", " Chapter 12 Transform: Data Manipulation Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. We’ve already touched upon some useful transformation functions in previous example code snippets, such as the mutate function for adding columns. This section will explore some of the most useful functionalities of the dplyr package, explicitly introduce the pipe operator %&gt;%, and showcase how you can leverage these tools to quickly manipulate your data. The essential dplyr functions are : mutate() to create new columns/variables from existing data arrange() to reorder rows filter() to refine observations by their values (in other words by row) select() to pick variables by name (in other words by column) summarize() to collapse many values down to a single summary. We’ll go through each of these functions, for more details you can read Chapter 3: Data Transformation from R for Data Science which provides a more comprehensive breakdown of these functions. Note that the information here is based on a tidyverse approach, but this is only one way of doing things. See the Further reading section for links to other suitable approaches to data transformation. Let’s explore the functionality of dplyr using some flame absorption/emission spectroscopy (FAES) data from a CHM317 lab. This data represents the emission signal of five sodium (Na) standards measured in triplicate: FAES &lt;- read_csv(file = &quot;data/FAES.csv&quot;) head(FAES) ## # A tibble: 6 × 4 ## ...1 std_Na_conc replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 2 blank 0 ppm 2 592. ## 3 3 blank 0 ppm 3 581. ## 4 4 standard 0.1 ppm 1 5656. ## 5 5 standard 0.1 ppm 2 5654. ## 6 6 standard 0.1 ppm 3 5667. In this dataset you can see that two important aspects of the data, sample type (sample, blank or standard) and concentration are grouped in one column. We can use the separate() function we learned about in Separating columns to separate these values into two columns to facilitate further analysis. FAES &lt;- separate( FAES, col = std_Na_conc, into = c(&quot;type&quot;, &quot;conc_Na&quot;, &quot;units&quot;), sep = &quot; &quot;, convert = TRUE ) DT::datatable(FAES) 12.1 Selecting by row or value filter() allows up to subset our data based on observation (row) values. filter(FAES, conc_Na == 0) ## # A tibble: 3 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 2 blank 0 ppm 2 592. ## 3 3 blank 0 ppm 3 581. Note how we need to pass logical operations to filter() to specify which rows we want to select. In the above code, we used filter() to get all rows where the concentration of sodium is equal to 0 (== 0). Note the presence of two equal signs (==). In R one equal sign (=) is used to pass an argument, two equal signs (==) is the logical operation “is equal” and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use = instead of == when testing for equality. 12.1.1 Logical operators filter() can use other relational and logical operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators… let’s just list what they are and how we can use them: Operator Type Description &gt; relational Less than &lt; relational Greater than &lt;= relational Less than or equal to &gt;= relational Greater than or equal to == relational Equal to != relational Not equal to &amp; logical AND ! logical NOT | logical OR is.na() function Checks for missing values, TRUE if NA Selecting all signals below a threshold value: filter(FAES, signal &lt; 4450) ## # A tibble: 3 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 2 blank 0 ppm 2 592. ## 3 3 blank 0 ppm 3 581. Selecting signals between values: filter(FAES, signal &gt;= 4450 &amp; signal &lt; 8150) ## # A tibble: 3 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 standard 0.1 ppm 1 5656. ## 2 5 standard 0.1 ppm 2 5654. ## 3 6 standard 0.1 ppm 3 5667. Selecting all other replicates other than replicate 2: filter(FAES, replicate != 2) ## # A tibble: 10 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 3 blank 0 ppm 3 581. ## 3 4 standard 0.1 ppm 1 5656. ## 4 6 standard 0.1 ppm 3 5667. ## 5 7 standard 0.2 ppm 1 9393. ## 6 9 standard 0.2 ppm 3 9332. ## 7 10 standard 0.5 ppm 1 20187. ## 8 12 standard 0.5 ppm 3 20153. ## 9 13 standard 1 ppm 1 30798. ## 10 15 standard 1 ppm 3 30790. Selecting the first standard replicate OR any of the blanks: filter(FAES, (type == &quot;standard&quot; &amp; replicate == 1) | (type == &quot;blank&quot;)) ## # A tibble: 7 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 2 blank 0 ppm 2 592. ## 3 3 blank 0 ppm 3 581. ## 4 4 standard 0.1 ppm 1 5656. ## 5 7 standard 0.2 ppm 1 9393. ## 6 10 standard 0.5 ppm 1 20187. ## 7 13 standard 1 ppm 1 30798. Removing any rows with missing signal values (NA) using is.na(). Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (!) we would have selected all rows with missing values. filter(FAES, !is.na(signal)) ## # A tibble: 15 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 2 blank 0 ppm 2 592. ## 3 3 blank 0 ppm 3 581. ## 4 4 standard 0.1 ppm 1 5656. ## 5 5 standard 0.1 ppm 2 5654. ## 6 6 standard 0.1 ppm 3 5667. ## 7 7 standard 0.2 ppm 1 9393. ## 8 8 standard 0.2 ppm 2 9363. ## 9 9 standard 0.2 ppm 3 9332. ## 10 10 standard 0.5 ppm 1 20187. ## 11 11 standard 0.5 ppm 2 20141. ## 12 12 standard 0.5 ppm 3 20153. ## 13 13 standard 1 ppm 1 30798. ## 14 14 standard 1 ppm 2 30837. ## 15 15 standard 1 ppm 3 30790. These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, it’s up to you do figure out which works best for you. 12.2 Arranging rows arrange() reorders the rows based on the value you passed to it. By default it arranges the specified values into ascending order. Let’s arrange our data our data by increasing order of signal value: arrange(FAES, signal) ## # A tibble: 15 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. ## 2 3 blank 0 ppm 3 581. ## 3 2 blank 0 ppm 2 592. ## 4 5 standard 0.1 ppm 2 5654. ## 5 4 standard 0.1 ppm 1 5656. ## 6 6 standard 0.1 ppm 3 5667. ## 7 9 standard 0.2 ppm 3 9332. ## 8 8 standard 0.2 ppm 2 9363. ## 9 7 standard 0.2 ppm 1 9393. ## 10 11 standard 0.5 ppm 2 20141. ## 11 12 standard 0.5 ppm 3 20153. ## 12 10 standard 0.5 ppm 1 20187. ## 13 15 standard 1 ppm 3 30790. ## 14 13 standard 1 ppm 1 30798. ## 15 14 standard 1 ppm 2 30837. Since our original FAES data is already arranged by increasing conc_Na and replicate, let’s inverse that order by arranging conc_Na into descending order using the desc() function WHILE arranging the signal values in ascending order: # Note the order of precedence (left-to-right) arrange(FAES, desc(conc_Na), signal) ## # A tibble: 15 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15 standard 1 ppm 3 30790. ## 2 13 standard 1 ppm 1 30798. ## 3 14 standard 1 ppm 2 30837. ## 4 11 standard 0.5 ppm 2 20141. ## 5 12 standard 0.5 ppm 3 20153. ## 6 10 standard 0.5 ppm 1 20187. ## 7 9 standard 0.2 ppm 3 9332. ## 8 8 standard 0.2 ppm 2 9363. ## 9 7 standard 0.2 ppm 1 9393. ## 10 5 standard 0.1 ppm 2 5654. ## 11 4 standard 0.1 ppm 1 5656. ## 12 6 standard 0.1 ppm 3 5667. ## 13 1 blank 0 ppm 1 502. ## 14 3 blank 0 ppm 3 581. ## 15 2 blank 0 ppm 2 592. Just note with arrange() that NA values will always be placed at the bottom, whether you use desc() or not. 12.3 Selecting column name select() allows you to readily select columns by name. Note however that it will always return a tibble, even if you only select one variable/column. select(FAES, signal) ## # A tibble: 15 × 1 ## signal ## &lt;dbl&gt; ## 1 502. ## 2 592. ## 3 581. ## 4 5656. ## 5 5654. ## 6 5667. ## 7 9393. ## 8 9363. ## 9 9332. ## 10 20187. ## 11 20141. ## 12 20153. ## 13 30798. ## 14 30837. ## 15 30790. You can also select multiple columns using the same operators and helper functions described in Tidying Your Data:. select(FAES, conc_Na:replicate) ## # A tibble: 15 × 3 ## conc_Na units replicate ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0 ppm 1 ## 2 0 ppm 2 ## 3 0 ppm 3 ## 4 0.1 ppm 1 ## 5 0.1 ppm 2 ## 6 0.1 ppm 3 ## 7 0.2 ppm 1 ## 8 0.2 ppm 2 ## 9 0.2 ppm 3 ## 10 0.5 ppm 1 ## 11 0.5 ppm 2 ## 12 0.5 ppm 3 ## 13 1 ppm 1 ## 14 1 ppm 2 ## 15 1 ppm 3 # Getting columns containing the character &quot;p&quot; select(FAES, contains(&quot;p&quot;)) ## # A tibble: 15 × 2 ## type replicate ## &lt;chr&gt; &lt;dbl&gt; ## 1 blank 1 ## 2 blank 2 ## 3 blank 3 ## 4 standard 1 ## 5 standard 2 ## 6 standard 3 ## 7 standard 1 ## 8 standard 2 ## 9 standard 3 ## 10 standard 1 ## 11 standard 2 ## 12 standard 3 ## 13 standard 1 ## 14 standard 2 ## 15 standard 3 12.4 Deleting Columns or Rows While the process of selecting and filtering data is pivotal in data analysis, there are instances when you may need to remove specific columns or rows entirely. This is useful especially when you’re dealing with redundant or irrelevant data that might clutter your analysis. 12.4.1 Deleting columns To delete a column, you can use the select() function with the - sign before the column name you want to remove: # This will remove the &#39;signal&#39; column from the FAES dataset head(select(FAES, -signal)) ## # A tibble: 6 × 5 ## ...1 type conc_Na units replicate ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 ## 2 2 blank 0 ppm 2 ## 3 3 blank 0 ppm 3 ## 4 4 standard 0.1 ppm 1 ## 5 5 standard 0.1 ppm 2 ## 6 6 standard 0.1 ppm 3 Multiple columns can be deleted by providing more column names after the - sign: # Deleting both &#39;signal&#39; and &#39;replicate&#39; columns from the FAES dataset head(select(FAES, -c(signal, replicate))) ## # A tibble: 6 × 4 ## ...1 type conc_Na units ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 blank 0 ppm ## 2 2 blank 0 ppm ## 3 3 blank 0 ppm ## 4 4 standard 0.1 ppm ## 5 5 standard 0.1 ppm ## 6 6 standard 0.1 ppm 12.4.2 Deleting rows To delete rows, the filter() function can be used in conjunction with relational or logical conditions that define the rows you wish to exclude: # This will remove rows where &#39;signal&#39; values are less than 20000 filter(FAES, !(signal &lt; 20000)) ## # A tibble: 6 × 6 ## ...1 type conc_Na units replicate signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 standard 0.5 ppm 1 20187. ## 2 11 standard 0.5 ppm 2 20141. ## 3 12 standard 0.5 ppm 3 20153. ## 4 13 standard 1 ppm 1 30798. ## 5 14 standard 1 ppm 2 30837. ## 6 15 standard 1 ppm 3 30790. The key here is the use of the ! (NOT) operator which excludes rows that meet the specified condition. 12.5 Adding new variables mutate() allows you to add new variables (read columns) to your existing data set. It’ll probably be the workhorse function you’ll use during your data transformation as you can readily pass other functions and mathematical operators to it to transform your data. let’s suppose that our standards were diluted by a factor of 10, we can add a new column dil_fct for this: mutate(FAES, dil_fct = 10) ## # A tibble: 15 × 7 ## ...1 type conc_Na units replicate signal dil_fct ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. 10 ## 2 2 blank 0 ppm 2 592. 10 ## 3 3 blank 0 ppm 3 581. 10 ## 4 4 standard 0.1 ppm 1 5656. 10 ## 5 5 standard 0.1 ppm 2 5654. 10 ## 6 6 standard 0.1 ppm 3 5667. 10 ## 7 7 standard 0.2 ppm 1 9393. 10 ## 8 8 standard 0.2 ppm 2 9363. 10 ## 9 9 standard 0.2 ppm 3 9332. 10 ## 10 10 standard 0.5 ppm 1 20187. 10 ## 11 11 standard 0.5 ppm 2 20141. 10 ## 12 12 standard 0.5 ppm 3 20153. 10 ## 13 13 standard 1 ppm 1 30798. 10 ## 14 14 standard 1 ppm 2 30837. 10 ## 15 15 standard 1 ppm 3 30790. 10 We can also create multiple columns in the same mutate() call: mutate(FAES, dil_fct = 10, adj_signal = signal * dil_fct) ## # A tibble: 15 × 8 ## ...1 type conc_Na units replicate signal dil_fct adj_signal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blank 0 ppm 1 502. 10 5023. ## 2 2 blank 0 ppm 2 592. 10 5918. ## 3 3 blank 0 ppm 3 581. 10 5815. ## 4 4 standard 0.1 ppm 1 5656. 10 56563. ## 5 5 standard 0.1 ppm 2 5654. 10 56536. ## 6 6 standard 0.1 ppm 3 5667. 10 56674. ## 7 7 standard 0.2 ppm 1 9393. 10 93934. ## 8 8 standard 0.2 ppm 2 9363. 10 93627. ## 9 9 standard 0.2 ppm 3 9332. 10 93320. ## 10 10 standard 0.5 ppm 1 20187. 10 201869. ## 11 11 standard 0.5 ppm 2 20141. 10 201405. ## 12 12 standard 0.5 ppm 3 20153. 10 201530. ## 13 13 standard 1 ppm 1 30798. 10 307977. ## 14 14 standard 1 ppm 2 30837. 10 308365. ## 15 15 standard 1 ppm 3 30790. 10 307898. A couple of things to note: Quotation marks are generally optional when creating a new variable in mutate(), but they become necessary if the variable name contains spaces, special characters, or starts with a number. For example, \"dil_fct\", dil_fct, and dil_fct1 are all valid, but if you had a variable name like \"dil fct\", \"dil-fct\", or \"2nd_fct\", the quotes would be required. The variables we’re referencing do not need to be in quotation marks; hence signal because this variable already exists. Note the order of precedence: dil_fct is created first so we can reference in the second column being added, we would get an error if we swapped the order. 12.5.1 Mutate with a condition In data analysis, there are often scenarios where we want to categorize or re-label values based on certain conditions. The case_when() function offers a versatile and readable solution for handling these multiple conditions. The syntax for case_when() is straightforward: for each condition, you specify the logical test followed by the tilde (~) operator, and then the value or expression to return if the condition is TRUE. A .default value can be provided for cases when none of the conditions are TRUE. With our FAES data, say you want to label each conc_Na as “Low”, “Medium”, or “High” based on its value. You can use case_when() within mutate() as follows: mutate(FAES, conc_Na_level = case_when( conc_Na &lt; 0.2 ~ &quot;Low&quot;, conc_Na &lt; 0.4 ~ &quot;Medium&quot;, .default = &quot;High&quot;)) ## # A tibble: 15 × 7 ## ...1 type conc_Na units replicate signal conc_Na_level ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 blank 0 ppm 1 502. Low ## 2 2 blank 0 ppm 2 592. Low ## 3 3 blank 0 ppm 3 581. Low ## 4 4 standard 0.1 ppm 1 5656. Low ## 5 5 standard 0.1 ppm 2 5654. Low ## 6 6 standard 0.1 ppm 3 5667. Low ## 7 7 standard 0.2 ppm 1 9393. Medium ## 8 8 standard 0.2 ppm 2 9363. Medium ## 9 9 standard 0.2 ppm 3 9332. Medium ## 10 10 standard 0.5 ppm 1 20187. High ## 11 11 standard 0.5 ppm 2 20141. High ## 12 12 standard 0.5 ppm 3 20153. High ## 13 13 standard 1 ppm 1 30798. High ## 14 14 standard 1 ppm 2 30837. High ## 15 15 standard 1 ppm 3 30790. High For those interested in exploring further, there’s a similar function called ifelse() which provides conditional transformations in R. You can learn more about it in R documentation found here. 12.5.2 Useful mutate function There are a myriad of functions you can make use of with mutate. Here are some of the mathematical operators available in R: Operator.or.Function Definition + addition - subtraction * multiplication / division ^ exponent; to the power of… log() returns the specified base-log; see also log10() and log2() 12.6 Group and summarize data summarize effectively summarizes your data based on functions you’ve passed to it. Looking at our FAES data we’d might want the mean and standard deviation of the triplicate signals. Let’s see what happens when we apply the summarize function straight up: summarise(FAES, mean = mean(signal), stdDev = sd(signal)) ## # A tibble: 1 × 2 ## mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13310. 11242. This doesn’t look like what we wanted. What we got was the mean and standard deviation of all of the signals, regardless of the concentration of the standard. Also note how we’ve lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to group our observations by a variable. We can do this by using the group_by() function. groupedFAES &lt;- group_by(FAES, type, conc_Na) summarise(groupedFAES, mean = mean(signal), stdDev = sd(signal)) ## `summarise()` has grouped output by &#39;type&#39;. You can override using the ## `.groups` argument. ## # A tibble: 5 × 4 ## # Groups: type [2] ## type conc_Na mean stdDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blank 0 559. 48.9 ## 2 standard 0.1 5659. 7.34 ## 3 standard 0.2 9363. 30.7 ## 4 standard 0.5 20160. 24.0 ## 5 standard 1 30808. 25.0 Here we’ve created a new data set, groupedFAES, that we grouped by the variables type and conc_Na so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. Depending on your dataset and the analysis you’re performing, you’ll need to decide how to group your data: the more variables you use, the smaller each group will be. 12.6.1 Useful summarize functions We’ve used the mean() and sd() functions above, but there are a host of other useful functions you can use in conjunction with summarize. See Useful Functions in the summarise() documentation (enter ?summarise) in the console. This is also discussed in more depth in the Summarizing Data chapter. 12.7 The Pipe: Chaining Functions Together Piping is a concept that allows you to chain functions together in a way that simplifies and clarifies your code. At its core, piping is similar to function composition in mathematics, where the output of one function becomes the input to the next. This helps you build complex operations in a readable and logical sequence. 12.7.1 Function Composition Example Consider a mathematical example of function composition: \\[ f(g(x)) \\] Here, the function \\(g(x)\\) is applied first, and its output is then passed as the input to the function \\(f(x)\\). In programming, this concept can be translated to chaining functions together. 12.7.2 Abstract Example of Piping The pipe operator %&gt;%, an incredibly useful tool for writing more legible and understandable code. The pipe basically changes how you read code to emphasize the functions you’re working with by passing the intermediate steps to hidden processes in the background. Now, consider the following abstract example of piping in R: result &lt;- data %&gt;% step1() %&gt;% step2() %&gt;% step3() Here’s what’s happening: step1(): Takes data as its input. step2(): Takes the result of step1() as its input. step3(): Takes the result of step2() as its input. This sequence of operations could be written without pipes by nesting function calls, but the use of pipes makes the flow of data more explicit and easier to read. 12.7.3 Simple Examples of Piping Let’s start with a simple, single use of piping: FAES %&gt;% nrow() In this case, the %&gt;% operator pipes the FAES dataset directly into the nrow() function, which returns the number of rows in the dataset. This is functionally equivalent to: nrow(FAES) Now, let’s consider an example where the function takes an additional argument: meanBlank &lt;- FAES %&gt;% filter(type == &quot;blank&quot;) This is functionally equivalent to: meanBlank &lt;- filter(FAES, type == &quot;blank&quot;) In both cases, piping might seem redundant because it only removes the need to specify the first argument explicitly. 12.7.4 Piping in Practice With the tools presented in this chapter without using pipe operator we could do a decent job analyzing our FAES data. Let’s say we wanted to subtract the mean of the blank from each standard signal and then summarize those results. It would look something like this: blank &lt;- filter(FAES, type == &quot;blank&quot;) meanBlank &lt;- summarize(blank, mean(signal)) meanBlank &lt;- as.numeric(meanBlank) paste(&quot;The mean signal from the blank triplicate is:&quot;, meanBlank) ## [1] &quot;The mean signal from the blank triplicate is: 558.5249&quot; stds_1 &lt;- filter(FAES, type == &quot;standard&quot;) stds_2 &lt;- mutate(stds_1, cor_sig = signal - meanBlank) stds_3 &lt;- group_by(stds_2, conc_Na) stds_4 &lt;- summarize(stds_3, mean = mean(cor_sig), stdDev = sd(cor_sig)) stds_4 ## # A tibble: 4 × 3 ## conc_Na mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 5101. 7.34 ## 2 0.2 8804. 30.7 ## 3 0.5 19602. 24.0 ## 4 1 30249. 25.0 If we use pipes, we can make this code much more legible and easier to understand. The code with pipes would look like this: meanBlank &lt;- FAES %&gt;% filter(type == &quot;blank&quot;) %&gt;% summarise(mean(signal)) %&gt;% as.numeric() paste(&quot;The mean signal from the blank triplicate is:&quot;, meanBlank) ## [1] &quot;The mean signal from the blank triplicate is: 558.5249&quot; stds &lt;- FAES %&gt;% filter(type == &quot;standard&quot;) %&gt;% mutate(cor_sig = signal - meanBlank) %&gt;% group_by(conc_Na) %&gt;% summarize(mean = mean(cor_sig), stdDev = sd(cor_sig)) stds ## # A tibble: 4 × 3 ## conc_Na mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 5101. 7.34 ## 2 0.2 8804. 30.7 ## 3 0.5 19602. 24.0 ## 4 1 30249. 25.0 While the initial code did its job, it’s certainly wasn’t easy to type and certainly not easy to read. At every step of the way we’ve saved our updated data outputs to a new variable (stds_1, stds_2, etc.). However, most of these intermediates aren’t important, and moreover the repetitive names clutter our code. As the code above is written, we’ve had to pay special attention to the variable suffix to make sure we’re calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. stds_1 &lt;- mutate(stds_1, ...)), but that doesn’t solve the issue of readability as there’s still redundant assigning. Things may look a bit different, but our underlying code hasn’t changed much. What’s happening is the pipe operator passes the output to the first argument of the next function. So the output of filter... is passed to the first argument of sumamrise..., and the argument we specified in summarise is actually the second argument it receives. You’re probably wondering how hiding stuff makes your code more legible, but think of %&gt;% as being equivalent to “then”. We can read our code as: “Take the FAES dataset, then filter for type == \"blank\" then collapse the dataset to the mean signal value and then convert to numeric value then pass this final output to the new variable meanBlank.” Not only is the pipe less typing, but the emphasis is on the functions so you can better understand what you’re doing vs. where all the intermediate values are going. 12.7.5 Notes on piping The pipe is great, but it does have some limitations: You can’t easily extract intermediate steps. So you’ll need to break up your pipping chain to output any intermediate steps you can. The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of what’s going on. Keep the piping short and thematically similar. Pipes are linear, if you have multiple inputs or outputs you should consider an alternative approach. 12.8 Further reading Chapter 5: Data Transformation of R for Data Science for a deeper breakdown of dplyr and its functionality. Chapter 18: Pipes of R for Data Science for more information on pipes. Syntax equivalents: base R vs Tidyverse by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but there’s no denying that certain base-R can be more elegant. Check out this write up to get a better idea. 12.9 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["ggplot-basic-visualizations.html", "Chapter 13 Ggplot Basic Visualizations 13.1 Building plots ups 13.2 Basic plotting 13.3 Changing plot labels 13.4 Small Multiples 13.5 Plotting subsets of data 13.6 Exercise", " Chapter 13 Ggplot Basic Visualizations ggplot2 is loaded by default with the tidyverse suite of packages. Let’s revisit our spectroscopy data we encountered in Tidying Your Data: library(tidyverse) atr_data &lt;- read_csv(&quot;data/ATR_plastics.csv&quot;) ## Rows: 28628 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sample ## dbl (2): wavenumber, absorbance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # First 50 rows of data DT::datatable(head(atr_data, 50)) 13.1 Building plots ups The gg in ggplot2 stands for the grammar of graphics, (H. Wickham 2009) and it’s a way to break down graphics (plots) into small pieces that can be discussed (hence grammar). We’ll take a look at this grammar via geoms (what kind of plot), aes (aesthetic choices), etc. For now, understand that this means we need to build up graphics/plots piece-by-piece and layer-by-layer. This extends beyond code to how we code. Plot often, and discard the useless ones. Take the time to pretty up your plot after you’re satisfied with the underlying data. 13.2 Basic plotting ggplot2 uses geoms to specify what type of plot to create. Different plots are used to tell different stories and have different strengths and weakness. We’ll explore these more in Visualizations for Env Chem, but for now we’ll focus on geom_point(), which simply plots data as points on the 2-D plane. In other words, a scatter plot. Let’s plot our tidied atr_data data: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance)) + geom_point() Let’s ignore the plot for now and look at our code below: ggplot() creates a ggplot object that contains specifications for the plot: We want to plot data from our atr_data dataset (data = atr_data). We specified our aesthetic mappings via aes(), which communicates how we want the plot to look. In this case, we’ve specified which values from atr_data are the x-axis values (x = wavenumber) and y-axis values (y = absorbance). We then add the geom_point() layer to create a scatter plot of (x,y) points. Now let’s look at our result. What we see is a point for every recorded absorbance measurements from our ATR analysis. We can clearly see the spectra of the different plastics in our data, however they’re all coloured the same. This is because we’ve only specified the x and y values. As far as ggplot() is concerned, these are the only values that matter, but we know different. Fortunately you can pass multiple variables to different aes() options to enhance our plot. For instance, we can pass the sample variable, which specifies which sample a spectrum originates from, to the colour option: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() Now we have a legend which clearly specifies which points are associated with each sample. But now the points are too large, potentially masking certain peaks. We can adjust the size of each point as follows: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point(size = 0.5) We specified size = 0.5 in the geom_point() call because it’s a constant. We can map size to any continuous variable, such as the absorbance: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance, colour = sample, size = absorbance)) + geom_point() Sometimes this makes sense (i.e. a bubble chart) but for our example, having the size of the points increase as the absorbance increases doesn’t provide any new information (it actually clutters our plot). 13.3 Changing plot labels By default ggplot uses the header of the columns you passed for the x and y aes() options. Because headers are written for code they’re often poor label titles for plots. We can specify new labels and plot titles as follows: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() + labs(title = &quot;ATR Spetra&quot;, subtitle = &quot;Courtesy of CHM317 student data&quot;, x = &quot;Wavenumber, cm^-1&quot;, y = &quot;Absorbance (arbitrary units)&quot;, caption = &quot;hi mom&quot;, colour = &quot;Plastic&quot;) Note how we changed the title of the legend with colour = \"Plastics\". This is because the legend is generated from our colour aesthetic (aes(..., colour = sample)). If our legend was based off of the size aesthetic, we would use size = \"New Title\" to change the title for the size legend. 13.4 Small Multiples Sometimes your plots become overwhelming, a phenomena called overplotting, which prevent your from comparing graphs or charts. A popular solution is small multiples, a series of similar plots using the same scale and axes. This is readily accomplished in R using facet_grid() (which creates a 2-D grid ) or facet_wrap() (a single 1d ribbon wrapped into 2D space). You simply specify which variable you want to differentiate your plots, for us it’s sample: ggplot(data = atr_data, aes(x = wavenumber, y = absorbance)) + geom_point() + facet_wrap(~sample) Note the use of the tilde (~) in facet_wrap(~sample); in this situation, it’s shorthand telling facet_wrap() to make small multiples off of the sample variable. 13.5 Plotting subsets of data Often you won’t want to plot everything in your dataset. Rather, you’ll want to plot a specific chemical, city, location, etc. To that end you want to plot a subset of your data. There are a couple of ways to handle this. You can subset your data on the fly using subset(). This way allows you to specify based off of Logical operators as such: ggplot(data = subset(atr_data, sample == &quot;EPDM&quot;), aes(x = wavenumber, y = absorbance)) + geom_point() or ggplot(data = subset(atr_data, wavenumber &gt;= 2500), aes(x = wavenumber, y = absorbance)) + geom_point() Another approach is to use filter() and pipe to ggplot(): atr_data %&gt;% filter((sample != &quot;EPDM&quot; &amp; wavenumber &lt;= 2500) | (sample == &quot;EPDM&quot; &amp; wavenumber &gt;= 3500)) %&gt;% ggplot(aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() There are pros and cons to either approach. subset() on the fly is best for simple task, like plotting a single city, whereas the piping approach is best for more complex sorting. 13.6 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["summarizing-data.html", "Chapter 14 Summarizing Data 14.1 Data to Play With 14.2 Summarizing Data by Group 14.3 Pretty Tables with Flextable 14.4 Exercise", " Chapter 14 Summarizing Data Summarizing data is what it sounds like. You’re reducing the number of rows in your dataset based on some predetermined method. Taking the average of a group of numbers is summarizing the data. Many numbers have been condensed to one: the average. In this chapter we’ll go over summarizing data, and some aesthetic changes we can make for publication ready tables. 14.1 Data to Play With For this section we’ll take a look at the 2018 hourly mean NO2 concentrations for the Atlantic provinces (New Brunswick, Prince Edward Island, Nova Scotia, and Newfoundland). The dataset is available in the R4EnvChem Project Template repository. Also if you’re keen, you can download any number of atmospheric datasets from Environment and Climate Change Canada’s (ECCC) National Airborne Pollution Program’s (NAPS) website here. We will learn pivot_longer function in the next chapter, Restructuring Your Data, but you can take the following tidying code for granted in this chapter. atlNO2 &lt;- read_csv(&quot;data/2018hourlyNO2_Atl_wide.csv&quot;, skip = 7, na = c(&quot;-999&quot;)) %&gt;% rename_with(~tolower(gsub(&quot;/.*&quot;, &quot;&quot;, .x))) %&gt;% pivot_longer(cols = starts_with(&quot;h&quot;), names_prefix = &quot;h&quot;, names_to = &quot;hour&quot;, names_transform = list(hour = as.numeric), values_to = &quot;conc&quot;, values_transform = list(conc = as.numeric), values_drop_na = TRUE) # First 50 rows of dataset DT::datatable(head(atlNO2, 50)) Note in our dataset that both Halifax NS and Saint John NB have three NAPS stations each. It won’t matter for our aggregation, but if we were exploring this data in more depth this is something we would want to take into account. 14.2 Summarizing Data by Group While we can readily summarize an entire dataset, we often want to summarize groups within our dataset. In our case, it’s [NO2] in each province. To this end, we need to combine thegroup_by() and summarize() functions. This approach allows us to specify which groups we want summarized, and how we want them summarized. We’ll talk more about the second point later on, for now, let’s look at point how we specify which groups to summarize. Let’s calculate the mean hourly NO2 concentrations in the 4 provinces in our dataset: sumAtl &lt;- atlNO2 %&gt;% group_by(p) %&gt;% summarize(mean = mean(conc)) sumAtl ## # A tibble: 4 × 2 ## p mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 NB 2.86 ## 2 NL 2.30 ## 3 NS 2.36 ## 4 PE 0.975 That’s it. 186339 unique rows summarized like that. Note that summarize produces a new data frame, so you’ll want to double check the outputted data types. Let’s break down what our code does: We’re creating a new data frame, so we store it in sumAtl. We then take our atlNO2 dataset and group it by province using group_by(p). We then summarize our grouped data by summarizing the NO2 concentration with summarize(mean = mean(conc)). Note that since we’re creating a new dataset, we need to create new columns. This is what mean = mean(conc) does. We’re creating a column called mean, which contains the numerical mean 1-hr NO2 values which were calculated using the mean() function. Let’s dig a little deeper. The Canadian Ambient Air Quality Standards stipulates that the annual mean of 1-hour means for NO2 cannot exceed 17.0 ppb in 2020, and 12.0 ppb in 2025. Let’s see if any city in our dataset violated these standards in 2018. To do this, we’ll group by province (p) and city (city). This will retain our provinces column that we might want to use later on. sumAtl &lt;- atlNO2 %&gt;% group_by(p, city) %&gt;% summarize(mean = mean(conc)) sumAtl ## # A tibble: 19 × 3 ## # Groups: p [4] ## p city mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 NB Bathurst 1.21 ## 2 NB Edmundston 4.52 ## 3 NB Fredericton 1.82 ## 4 NB Moncton 3.37 ## 5 NB Saint John 3.02 ## 6 NL Corner Brook 2.71 ## 7 NL Grand Falls-Windsor 0.918 ## 8 NL Labrador City 2.51 ## 9 NL Marystown 0.277 ## 10 NL Mount Pearl 1.53 ## 11 NL St Johns 5.33 ## 12 NS Halifax 3.44 ## 13 NS Kentville 0.841 ## 14 NS Pictou 1.19 ## 15 NS Port Hawkesburry 2.53 ## 16 NS Sydney 2.66 ## 17 PE Charlottetown 1.85 ## 18 PE Southampton 0.512 ## 19 PE Wellington 0.455 Looks like there aren’t any offenders. For tips on visualizing these results please see the Visualizations for Env Chem chapter. 14.2.1 Further Summarize Operations There are other options we can use to summarize out data. A handy list is provided on the summarize() help page. The most common ones you’ll need are: mean() which calculates the arithmetic mean, a.k.a. the average. median() which calculates the sample median, the value separating the higher 50% of data from the lower 50% of a data sample. sd() which calculates the sample standard deviation. min() and max() which returns the smallest and largest value in the dataset. n() which provides the number of entries in a group. Note you don’t specify an input variable for this function. Let’s see them in action: sumAtl &lt;- atlNO2 %&gt;% group_by(p, city) %&gt;% summarize(mean = mean(conc), sd = sd(conc), min = min(conc), max = max(conc), n = n()) sumAtl ## # A tibble: 19 × 7 ## # Groups: p [4] ## p city mean sd min max n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 NB Bathurst 1.21 1.91 0 27 8755 ## 2 NB Edmundston 4.52 4.82 0 45 8756 ## 3 NB Fredericton 1.82 4.04 0 39 8729 ## 4 NB Moncton 3.37 4.71 0 39 8749 ## 5 NB Saint John 3.02 4.04 0 41 26051 ## 6 NL Corner Brook 2.71 2.47 0 28 8505 ## 7 NL Grand Falls-Windsor 0.918 1.26 0 27 7746 ## 8 NL Labrador City 2.51 4.05 0 46 8612 ## 9 NL Marystown 0.277 0.635 0 11 7142 ## 10 NL Mount Pearl 1.53 2.82 0 68 8522 ## 11 NL St Johns 5.33 6.14 0 49 8670 ## 12 NS Halifax 3.44 4.19 0 39 17591 ## 13 NS Kentville 0.841 1.22 0 20 8640 ## 14 NS Pictou 1.19 1.52 0 20 8515 ## 15 NS Port Hawkesburry 2.53 4.29 0 59 8601 ## 16 NS Sydney 2.66 2.93 0 37 8675 ## 17 PE Charlottetown 1.85 2.87 0 35 8690 ## 18 PE Southampton 0.512 0.683 0 16 6799 ## 19 PE Wellington 0.455 0.747 0 9 8591 Note that the functions we pass to summarize adhere to rules of missing values. That is to say, if even one value in a group is an NA, the entire group defaults to NA. Consequently, if you’re confident this isn’t an issue, you can pass the argument na.rm = TRUE to any of the summarize functions, which would look like mean = mean(conc, na.rm = TRUE). This will ignore any NA values and return a numeric value like you probably expect. 14.3 Pretty Tables with Flextable While the summarize function does an excellent job of summarizing our data, the outputted dataset isn’t really fit for publication. This is doubly so if you used summarize as the last step of your chemical quantification and you want a nice and pretty table of mean sample concentration with standard deviations. To this end we’ll use the flextable package. For more details refer to flextable R package. There are other packages to make tables, but we’re using flextable as it’s consistent between HTML and PDF outputs. library(flextable) flextable(sumAtl) .cl-909b6b68{}.cl-9094d41a{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-909771d4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-909771de{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-909786a6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909786a7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909786b0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909786b1{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909786ba{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909786bb{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}pcitymeansdminmaxnNBBathurst1.21336381.90688650278,755NBEdmundston4.51895844.81933870458,756NBFredericton1.82437854.04449040398,729NBMoncton3.37021374.71261280398,749NBSaint John3.02264794.038227304126,051NLCorner Brook2.70840682.46963800288,505NLGrand Falls-Windsor0.91750581.26145040277,746NLLabrador City2.50627034.05408020468,612NLMarystown0.27667320.63498500117,142NLMount Pearl1.53414692.82499290688,522NLSt Johns5.32975786.13848170498,670NSHalifax3.43834924.187116403917,591NSKentville0.84108801.21860100208,640NSPictou1.19377571.51804860208,515NSPort Hawkesburry2.52540404.28565340598,601NSSydney2.66109512.93000390378,675PECharlottetown1.85132342.86694040358,690PESouthampton0.51154580.68317780166,799PEWellington0.45547670.7471108098,591 Perhaps that isn’t pretty enough for you. Doubtlessly your instructor will tell you to combine the mean and standard deviation into one value (i.e. \\(mean \\pm sd\\)). We’ll do this in two steps. Step 1: Use unite() to merge the mean and sd values together row-wise; values will be separated by ±. ± is a legit symbol, try Alt+241or copy and paste it from this book. Step 2: Pretty up our table to significant digits, and perform some aesthetic changes. 14.3.1 Uniting columns Firstly, our mean and sd columns contain way too many decimal places. We’ll need to round them down before we use unite() to paste together the two columns into one. During our unite() call, we’ll use sep = \" ± \" to separate the mean from sd values (otherwise they’d be pasted as one long number). prettySumAtl &lt;- sumAtl %&gt;% mutate(mean = sprintf(&quot;%.1f&quot;, mean), sd = sprintf(&quot;%.1f&quot;, sd)) %&gt;% unite(&quot;mean ± sd&quot;, c(&quot;mean&quot;, &quot;sd&quot;), sep = &quot; ± &quot; ) %&gt;% select(-n) # removing n column prettySumAtl ## # A tibble: 19 × 5 ## # Groups: p [4] ## p city `mean ± sd` min max ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NB Bathurst 1.2 ± 1.9 0 27 ## 2 NB Edmundston 4.5 ± 4.8 0 45 ## 3 NB Fredericton 1.8 ± 4.0 0 39 ## 4 NB Moncton 3.4 ± 4.7 0 39 ## 5 NB Saint John 3.0 ± 4.0 0 41 ## 6 NL Corner Brook 2.7 ± 2.5 0 28 ## 7 NL Grand Falls-Windsor 0.9 ± 1.3 0 27 ## 8 NL Labrador City 2.5 ± 4.1 0 46 ## 9 NL Marystown 0.3 ± 0.6 0 11 ## 10 NL Mount Pearl 1.5 ± 2.8 0 68 ## 11 NL St Johns 5.3 ± 6.1 0 49 ## 12 NS Halifax 3.4 ± 4.2 0 39 ## 13 NS Kentville 0.8 ± 1.2 0 20 ## 14 NS Pictou 1.2 ± 1.5 0 20 ## 15 NS Port Hawkesburry 2.5 ± 4.3 0 59 ## 16 NS Sydney 2.7 ± 2.9 0 37 ## 17 PE Charlottetown 1.9 ± 2.9 0 35 ## 18 PE Southampton 0.5 ± 0.7 0 16 ## 19 PE Wellington 0.5 ± 0.7 0 9 Note to round the numbers we used sprintf() This is because in the final publication it’s important to keep trailing zeros (i.e. 1.0 and not 1), but R’s round() will drop these. mean = sprintf(\"%.1f\", mean) takes the existing values in the mean column, rounds them to one digit, that’s what \"%.1f\" means (“%.2f” would be two digits and so on), and pastes them back into the mean column. Same situation for the sd column. 14.3.2 Pretty tables Now we’ll want to make a pretty table. Despite the emphasis on visualizations in this book, tables are an under appreciated means to convey information. Often when you’re only plotting a handful of numbers, a table would better serve the reader. So don’t overlook this point of your report. If you’ve distilled hours of your work to a handful of numbers, you best serve them up on a silver platter. Below is an example of how you might format your table for publication. ft &lt;- flextable(prettySumAtl) ft &lt;- set_header_labels(ft, p = &quot;province&quot;) ft &lt;- set_table_properties(ft, layout = &quot;autofit&quot;) ft &lt;- align(ft, j = &quot;mean ± sd&quot;, align = &quot;right&quot;, part = &quot;all&quot;) ft .cl-90b42fa4{table-layout:auto;}.cl-90ad9036{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-90afdc24{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-90afdc25{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-90afede0{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90afedea{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90afedeb{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90afedec{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90afedf4{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90afedf5{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}provincecitymean ± sdminmaxNBBathurst1.2 ± 1.9027NBEdmundston4.5 ± 4.8045NBFredericton1.8 ± 4.0039NBMoncton3.4 ± 4.7039NBSaint John3.0 ± 4.0041NLCorner Brook2.7 ± 2.5028NLGrand Falls-Windsor0.9 ± 1.3027NLLabrador City2.5 ± 4.1046NLMarystown0.3 ± 0.6011NLMount Pearl1.5 ± 2.8068NLSt Johns5.3 ± 6.1049NSHalifax3.4 ± 4.2039NSKentville0.8 ± 1.2020NSPictou1.2 ± 1.5020NSPort Hawkesburry2.5 ± 4.3059NSSydney2.7 ± 2.9037PECharlottetown1.9 ± 2.9035PESouthampton0.5 ± 0.7016PEWellington0.5 ± 0.709 14.4 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["restructuring-your-data.html", "Chapter 15 Restructuring Your Data 15.1 Long vs Wide Data 15.2 Making Data Longer 15.3 Making Data Wider 15.4 Summary 15.5 Further reading 15.6 Exercise", " Chapter 15 Restructuring Your Data In the realm of data analysis, the structure of your data can be just as crucial as the data itself. When dealing with complex datasets in R, or any other analytical environment, the way you organize and reshape your data can significantly impact the efficiency and clarity of your analyses. This chapter delves into the art of restructuring data in R, focusing on two powerful tidyverse functions: pivot_longer and pivot_wider. These tools are essential for transforming data into a format that aligns perfectly with your analytical objectives. Understanding and mastering these functions will equip you with the skills to seamlessly toggle between different data layouts. Whether you need to condense wide datasets into longer, more detailed formats using pivot_longer, or expand long datasets into a wider, more summarized form with pivot_wider, this chapter will guide you through each step with practical examples and insights. By the end of this chapter, you’ll not only be adept at manipulating your data’s structure in R but also appreciate how such transformations can unveil new perspectives and insights in your data analysis journey. 15.1 Long vs Wide Data Let’s revisit our ATR plastics data from previous chapters: atr_plastics &lt;- read.csv(&quot;data/ATR_plastics.csv&quot;) DT::datatable(head(atr_plastics, 50)) And here is the surprise: The data you see above was actually already transformed to be longer for your convenience in the earlier chapters! The original (wide) data looks like this: atr_plastics_wide &lt;- read.csv(&quot;data/ATR_plastics_original_wide.csv&quot;) # This just outputs a table you can explore within your browser DT::datatable(atr_plastics_wide) 15.1.1 Long Data In the long format, the data is structured with each observation occupying a single row, and different variables are stacked or melted into a single column. Take a look at the data atr_plastics. It contains absorbance measurements for different materials (EPDM, Polystyrene, Polyethylene, and a sample labeled “Sample: Shopping bag”) at various wavenumbers. Each row represents a combination of wavenumber, material, and absorbance value. This format is more conducive to data analysis and visualization, particularly when working with multiple variables or when conducting statistical analyses. It allows for easier manipulation and transformation of the data, as well as the application of statistical models that require data in long format, such as certain regression analyses. This is exactly why we have been providing you with this version of data in the previous chapters! 15.1.2 Wide Data On the other hand, in the wide format, the data (atr_plastics_wide) is organized with each variable occupying its own column. Here, we have a dataset containing the same absorbance measurements, but now each row represents a unique wavenumber, and the absorbance values for each material are presented in separate columns. This format is intuitive for viewing all measurements at a particular wavenumber simultaneously, allowing for quick comparisons between materials. However, as the number of materials or variables increases, the wide format may become unwieldy, especially if additional variables are introduced. This format makes intuitive sense when recording in the lab, and for working in Excel, but isn’t the friendliest with R. For example, when making plots with ggplot, we can only specify one y variable. In the example plot below it’s the absorbance spectrum of Polystyrene. However, if wanted to plot the other spectra for comparison, we’d need to repeat our geom_point call for each variable, which is not ideal to keep our code clean. # Plotting Polystyrene absorbance spectra ggplot(data = atr_plastics_wide, aes( x = wavenumber, y = Polystyrene)) + geom_point() # Plotting Polystyrene and Polyethylene absorbance spectra ggplot(data = atr_plastics_wide, aes(x = wavenumber, y = Polystyrene)) + geom_point() + geom_point(data = atr_plastics_wide, aes(x = wavenumber, y = Polyethylene)) While the code above works, it’s not particularly handy and undermines much of the utility of ggplot. On the other hand, the long format will better work to show all 4 materials’ absorbance and wavenumber scatterplot in one graph without having to repeat the ggplot call. ggplot(data = atr_plastics, aes(x = wavenumber, y = absorbance, colour = sample) ) + geom_point() 15.2 Making Data Longer Now that we learned the difference between long and wide formats, how did we actually transform the original wide format ATR plastic data into the longer format? We transformed the original wide format data into a longer format using pivot_longer() function. This function reshapes the data by stacking or melting multiple columns into a single column, making it easier to work with and analyze. By specifying the appropriate arguments such as cols, names_to, and values_to, we can control how the data is reshaped. Let’s look at the following code. # Transform the original (wide) data into long format atr_long &lt;- pivot_longer(atr_plastics_wide, cols = -wavenumber, names_to = &quot;sample&quot;, values_to = &quot;absorbance&quot;) DT::datatable(head(atr_long, 50)) Let’s break down the code we’ve executed via the pivot_longer function: cols = -wavenumber specifies that we’re selecting every other column but wave number. we could have just as easily specified each column individually using cols = c(\"EPDM\",...) but it’s easier to use - to specify what we don’t want to select. names_to = \"sample\" specifies that the column header (i.e. names) be converted into an observation under the sample column. values_to = \"absorbance\" specifies that the absorbance values under each of the selected headers be placed into the absorbance column. Beyond what we showed here, pivot_longer has many other features that you can take advantage of. For more details and the possibilities of this function you can read the examples listed on the pivot_longer page of the tidyverse. 15.3 Making Data Wider How about making a data wider? We can utilize pivot_wider function. Essentially, pivot_wider is used to spread key-value pairs across a dataset, transforming it from a long to a wide format. This is especially useful when you need your data structured in a wide matrix for certain analytical procedures or visual presentations. The subsequent code utilizes the pivot_wider function to convert the long format data (atr_long) back into the original wider format. Here, names_from specifies the column containing the variable names, and values_from specifies the column containing the variable values. Running head(atr_wide) allows us to inspect the first few rows of the resulting wider format data, confirming that the transformation successfully reverted the data to its original structure. # Transform the long format data back into the wider format (= original data) atr_wide &lt;- pivot_wider(atr_long, names_from = sample, values_from = absorbance) head(atr_wide) ## # A tibble: 6 × 5 ## wavenumber EPDM Polystyrene Polyethylene Sample..Shopping.bag ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.0746 0.000873 0.0236 ## 2 551. 0.212 0.0746 0.000834 0.0238 ## 3 551. 0.213 0.0745 0.000819 0.0239 ## 4 552. 0.213 0.0745 0.000825 0.0239 ## 5 552. 0.214 0.0745 0.000868 0.0240 ## 6 553. 0.214 0.0746 0.000949 0.0240 Here’s a breakdown of this code: names_from = sample: This argument specifies which column in our long data will be used to create new column headers in the wide format. Each unique value in the sample column becomes a separate column in the resulting wide dataset. values_from = absorbance: This tells R that the values filling these new sample columns should be taken from the absorbance column. The result is a more traditional, wide-format dataset where each column represents a different sample’s absorbance values, facilitating side-by-side comparisons. The pivot_wider function is not only useful for converting long data to wide but also for data summarization and creating formats suitable for reports or specific analyses. If you’re dealing with summarized data, such as averages or counts, spreading this data into a wide format can make it more interpretable and easier to analyze. Furthermore, pivot_wider can be an essential part of a more complex data transformation process. In many cases, data manipulation might require alternating between widening and lengthening to achieve the desired structure for your analysis. To learn more about the pivot_wider function, we recommend reading the documentation here. 15.4 Summary In summary, the relationship between wide and long data formats in R is inverse, meaning that transforming data from one format to the other and back again using functions like pivot_longer() and pivot_wider() is straightforward and reversible. This flexibility allows researchers to choose the most suitable format for their analysis needs. In general, the decision to use wide or long format depends on the specific characteristics of the data and the analytical tasks at hand. Here’s a recap of when to use each format: Use wide format when: Each variable has its own column. Observations are in rows, and variables are in columns. The data is more compact and intuitive for viewing multiple variables simultaneously. Use long format when: Multiple variables are stored in the same column. Each row represents a unique observation or measurement. The data is conducive to statistical analysis and visualization, especially when dealing with repeated measures or categorical variables. Understanding both pivot_longer and pivot_wider equips you with a versatile toolkit for shaping your data. Whether you’re preparing data for specific package requirements, like matrixStats or matrixTests, or simply need to restructure your dataset for clarity and analysis, these functions are invaluable in the R programming environment. By understanding the inverse relationship between wide and long data formats and knowing when to use each format, we can also effectively manage and analyze our data to derive meaningful insights and conclusions. 15.5 Further reading As always, the R for Data Science book goes into more detail on all of the elements discussed above. For the material covered here you may want to read Chapter 12: Tidy Data. 15.6 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["visualizations-for-env-chem.html", "Chapter 16 Visualizations for Env Chem 16.1 Discrete vs. Continuous variables 16.2 Prerequisites", " Chapter 16 Visualizations for Env Chem Visualizations have always been an important part of data science and chemistry. Good graphics illuminate trends and patterns you may have otherwise missed and allow us to quickly inspect thousands of values. R via the ggplot2 package is one of, if not the premier, data visualization language available. This chapter will formally introduce the ggplot2 package, explain a bit of the logic undergirding its operation, and give you some quick examples of how it works. Afterwards we’ll delve deeper into specific visualizations you’ll use and encounter in your studies culminating in preparing your plots for publication. We’ve already encountered and produced several types of plots to visualize our data. We’ve also gone over the theory and basic operations of ggplot() in the ggplot basic visualizations section. Now, we’ll expand on these and explicitly walk through the most common data visualization methods you’ll encounter in the field of environmental chemistry. Additionally, we’ll learn how to get your plots ready for publication. The plots we’ll be covering include: Bar Charts Box Plots Histograms Scatter Plots Interactive Plots These are only a smattering of the possible data visualizations you can perform in R. We’re focusing on them because of their ubiquity in our field, but they often won’t be the ideal visualizations you need to communicate your story. We highly recommend you check out the following resources. Not only are they a great source of inspiration, they provide example code to get you up and running. We consult them regularly. Data to viz which features a decision tree to help you decide on what plot would serve you best. ggplot2 extensions gallery which is the best repository to the plethora of ggplot2() extensions. If you need a specialized plot, check here. Odds are someone has a solution to your problem. Some great extensions include ggrepel for easy labelling of points; ggpmisc for statistical annotations; and ggpubr for publication ready plots, group wise comparisons, and annotation of statistical significance. The R Graph Gallery contains hundreds of charts made with R. While it’s not as easy to navigate as Data to viz, it does contain many more examples; it is definitely worth exploring. 16.1 Discrete vs. Continuous variables The type of plots available to you, and how they display, are dependent on the type of data. Namely, whether your data is discrete (i.e. can only take particular values) or continuous (is not restricted to defined separate values, but can occupy any value over a continuous range). So a variable consisting of cities would be discrete, whereas a variable like concentration of a chemical would be continuous. You can treat numeric data as categorical if you so choose. Understanding the difference between discrete and continuous data will shape how you plot your data. 16.2 Prerequisites Additionally, for this section we’ll mostly be using the atlNO2 and sumAtl datasets we created in the Summarizing data chapter. Now that we’ve seen how to turn data into a longer format, we’ll show you how atlNO2 was tidied from the raw (wide format) data. This atlNO2 is identical to the previous atlNO2 we saw. atlNO2 &lt;- read_csv(&quot;data/2018hourlyNO2_Atl_wide.csv&quot;, skip = 7, na =c(&quot;-999&quot;)) %&gt;% rename_with(~tolower(gsub(&quot;/.*&quot;, &quot;&quot;, .x))) %&gt;% pivot_longer(cols = starts_with(&quot;h&quot;), names_prefix = &quot;h&quot;, names_to = &quot;hour&quot;, names_transform = list(hour = as.numeric), values_to = &quot;conc&quot;, values_transform = list(conc = as.numeric), values_drop_na = TRUE) ## Rows: 8395 Columns: 31 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Pollutant//Polluant, City//Ville, P/T//P/T ## dbl (28): NAPS ID//Identifiant SNPA, Latitude//Latitude, Longitude//Longitud... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sumAtl &lt;- atlNO2 %&gt;% group_by(p, city) %&gt;% summarize(mean = mean(conc), sd = sd(conc), median = median(conc), min = min(conc), max = max(conc)) ## `summarise()` has grouped output by &#39;p&#39;. You can override using the `.groups` ## argument. "],["common-types-of-graphs.html", "Chapter 17 Common Types of Graphs 17.1 Bar Charts 17.2 Box Plots 17.3 Histograms 17.4 Scatter Plots 17.5 Exercise", " Chapter 17 Common Types of Graphs In this chapter, you will learn how to create some of the most commonly used graphs, including bar plots, box plots, histograms, and scatterplots. These powerful visualization tools will enable you to explore and communicate your data effectively, gaining valuable insights along the way. 17.1 Bar Charts Bar charts, also called column charts, represent categorical data with rectangular bars whose height/length is proportional to the values they represent. ggplot(data = sumAtl, aes(x = city, y = mean)) + geom_col() + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Mean~NO[2]~(ppb))) Not an overly exciting plot, but it gets the job done showing the mean NO2 concentrations in each of the Atlantic Canada provinces (remember this dataset was created in the Summarizing Data chapter). Let’s break down the code: ggplot() includes geom_col() and geom_bar(). While both can be used to make bar charts. geom_col() is used when you want to represent values in the data (i.e. the precalculated mean as shown above), whereas geom_bar() makes the height of the bar proportional to the number of cases in each group. To our aesthetic mappings we’ve specified which values from the sumAtl are supposed to be our x-axis (x = city) and y-axis (y = mean). We used coord_flip() to rotate our plot 90\\(^\\circ\\) therefore the supplied x option of city is now plotted on the vertical axis. This makes reading long categorical names (i.e. the names of cities) easier. coord_flip() doesn’t change anything else except the final orientation of the plot. We used labs() to provide clearer labels for our axes than those defined by the column titles. Note that because we flipped the axes the y-axis is the horizontal axis and the x-axis is the vertical axis. Also note the use of the expression() function within the y-axis label, this is because subscripts and superscripts are not possible in the labs() function. The way that in that the expression() function works is a bit strange, you specify a space between characters/words by using a tilde ~, a subscript is specified by square brackets [], and superscript by a caret ^. 17.1.1 Adding Error Bars Almost all measurements have associated uncertainty/variability. These values are expressed visually via error bars demarcating the minimum and maximum variability and give a general idea of how precise a measurement is. In the Further Summarize Operations section we used the standard deviation (sd) function to calculate the standard deviation of the mean NO2 concentration in each city in the sumAtl dataset as a measure of the variability around the mean. To include these standard deviation values as error bars we can use the geom_errorbar() and pass the min and max values we want the error bars to be. In our case, the lowest value would be ymin = mean - sd, and the highest would be ymin = mean + sd. Our plotted error bars now indicate plus or minus one standard deviation from the mean. ggplot(data = sumAtl, aes(x = city, y = mean)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Mean~NO[2]~(ppb))) Some of the error bars indicate we could get a negative concentration of NO2. This is physically impossible, but it does suggest we should evaluate the distribution of our data (see below). Note that since we’re calculating error bar ranges on the fly, we’ve had to specify new aesthetic arguments to geom_errorbar(). 17.1.2 Ordering Bar Charts Often with bar charts (and similar plots), it’s useful to order the bars to help tell a story or convey information. We can do this using fct_reorder(): ggplot(data = sumAtl, aes(x = fct_reorder(city, mean), y = mean)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Mean~NO[2]~(ppb))) So in our aesthetics call for geom_bar we specified the x variable should be city, but ordered based on their corresponding mean value. Doing this has helped shed some light on trends in NO2 levels. For one, despite Labrador City having lower mean [NO2], we can now easily see that it has a larger variation in [NO2] than Corner Brook. 17.1.3 Grouping Bar Charts Sometimes you’ll want to create groups within your bar charts. One example of this in the atldata dataset is to group the cities by province. To visualize trends within each province in the code below we decided to: Reorder the cities based on province using the fct_reorder() function that specifies which variable is being ordered (city) and what information is being used to order it (p column). Colour the bars based on province using fill = p. ggplot(data = sumAtl, aes(x = fct_reorder(city, p), y = mean, fill = p)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Mean~NO[2]~(ppb))) There are other ways to group your bar charts depending on the story you want to tell and the data you have. Please consult the Grouped, stacked and percent stacked barplot in ggplot2 page from the R-graph-gallery. 17.2 Box Plots Box plots give a summary of the distribution of a numeric variable through their quartiles. You’ve no doubt seen them before, but they’re often misinterpreted. Let’s create a box-plot using geom_boxplot() and our Atlantic hourly NO2 measurements, then we’ll break down how to interpret it. ggplot(data = atlNO2, aes( x = city, y = conc)) + geom_boxplot() + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Concentration~NO[2]~(ppb))) Let’s break down how to interpret one box before tackling the entire set. As previously mentioned, box plots describe data in their quartiles. Quartiles basically arrange the data from the lowest to highest value and split the data at three points: The first quartile (Q1) is halfway between the lowest value and the median (50%) of the data. In other words 25% of the data lies below Q1. The second quartile (Q2) is the median. 50% of the data lies below, and 50% lies above this point. The third quartile (Q3) is halfway between the median and the highest value in the data. In other words, 75% of the data lies below Q3. The box in box-plots represents the range between Q1 and Q3. This is known as the inter-quartile range (IQR) and 50% of the total data falls somewhere inside this box. You can estimate the distribution by the symmetry of the box. if Q1 to the median is smaller than the median to Q3, the data has a positive skew (right sided skew), and vice versa. Rounding it out, geom_boxplot() includes whiskers, the thin lines emanating out from the box. This is used to predict outliers and is calculated as \\(outliers = \\pm 1.5 \\times IQR\\). Anything outside the whiskers is considered an “outlier” or an extreme point, and is plotted individually. Putting this all together, let’s look at the [NO2] for St. Johns city: Note that we’ve plotted the actual distribution of the data. Prior to computers, this was incredibly difficult to do, hence the use of box plots which can be drawn knowing only five points. However, the simplicity in calculating box-plots means they can hide trends and observations of your data. A useful alternative to box-plot are Violin Plots, which are explored in the next section. 17.2.1 Violin Plots Violin plots are made using geom_violin(). It is similar to the box-plot, but instead of displaying the quartiles, it plots the density within each group and is a bit more intuitive than box-plots. While the example below isn’t the most convincing given the scale of the dataset, violin plots are useful for identifying underlying trends in the distribution of data. For example, in the plot below we can see that some towns such as Marystown has days where [NO2] = 0 ppb, whereas Grand Falls-Windsor has a large number of days with low, but measurable levels of NO2. This might be because of differences in regional ambient levels of NO2. ggplot(data = atlNO2, aes(x = city, y = conc, fill = p)) + geom_violin() + coord_flip() + labs(x = &quot;City in Atlantic Canada&quot;, y = expression(Concentration~NO[2]~(ppb))) 17.2.2 Statistical Comparisons Between Groups Often box-plots are used to show differences in distributions between two groups (i.e. population in Location A vs. Location B). How you determine this statistically is a different story, but packages such as ggpubr have many built-in functionalities to display the results of these outcomes. From our NO2 data, St. Johns appears to have the highest levels of NO2. Let’s apply a pairwise test against other Newfoundland communities to see if our observation is statistically significant based upon the results of a Wilcoxon test. nfld &lt;- atlNO2 %&gt;% filter(p == &quot;NL&quot;) # only Newfoundland stations # Code from ggpubr website ggpubr::ggviolin(nfld, x = &quot;city&quot;, y = &quot;conc&quot;) + ggpubr::stat_compare_means(ref.group = &quot;St Johns&quot;, method = &quot;wilcox.test&quot;, label = &quot;p.signif&quot;) Based on the results of our test, all other stations in Newfoundland have statistically significant differences in the median NO2 values. Note the validity of this statistical approach to this particular problem is called into question based on the distribution of the data etc. We’ve included it to demonstrate how to label significance on plots, rather than an explicit discussion on statistics. For more information on ggpubr, adding p-values and significance labels, and different pairwise statistical test please visit ggpubr: Publication Ready Plots. 17.3 Histograms Histograms are an approximate representation of the distributions of numerical data. They’re an approximation because you arbitrarily “bin” your data into groups and then count the number of values inside that bin. The frequency, or count, in each bin is represented by the height of a rectangle whose width equals that of the bin. geom_histogram() is used to create histograms: ggplot(data = filter(atlNO2, city == &quot;St Johns&quot;), aes(x = conc)) + geom_histogram() + labs(subtitle = &quot;Distribution of St. Johns&#39; NO2 levels in 2018&quot;, y = expression(Concentration~NO[2]~(ppb))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can alter the resolution of our histogram by modifying the width of the bins using the binwidth argument or by specifying the number of bins with the bins argument. The former is useful when you don’t know the range of your data, whereas the latter is useful is you do (i.e. numbers between 0 and 100). ggplot(data = filter(atlNO2, city == &quot;St Johns&quot;), aes(x = conc)) + geom_histogram(binwidth = 1) + labs(subtitle = &quot;Distribution of St. Johns&#39; NO2 levels in 2018, binwidth = 1&quot;) 17.3.1 Multiple histograms While you can overlap histograms, it gets difficult to read with more than a handful of datasets. If we wanted to plot histograms of all the cities in our dataset we would have to use a small multiple via the facet_grid() or facet_wrap() arguments. facet_grid() allows you to arrange many small plots on a grid defined by variables in your dataset (i.e. columns for provinces, and rows for different pollutants). In the example below we’ve used facet_wrap(~city) which creates a 2D layout of histograms of each cities NO2 values. Note the tilde , ~, preceding in ~city. ggplot(data = atlNO2, aes(x = conc, fill = p)) + geom_histogram(binwidth = 1, position = &quot;identity&quot;) + facet_wrap(~city) 17.4 Scatter Plots Scatter plots display values of two variables, one of which is a continuous variable. Each data point is plotted as an individual point. You’ve already learned the basics of scatter plots in Ggplot Basic Visualizations. Now, we’ll touch upon some things you can do to improve your scatter plots. 17.4.1 Marginal plots You can easily combine a scatter plot with marginal plot. This is useful to summarize one dimension of our scatter plot. With the Toronto air quality data that we’ve already familiarized with, we might want to know the distribution of concentrations of the individual pollutants. Using the ggExtra package and the ggMarginal() function we can get the following: torontoAir &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ## Rows: 507 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): city, p, pollutant ## dbl (4): naps, latitude, longitude, concentration ## dttm (1): date.time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # note we&#39;re storing our plot in the variable torPlot # and we&#39;re not plotting SO2 torPlot &lt;- ggplot(data = filter(torontoAir, pollutant != &quot;SO2&quot;), aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() + theme(legend.position = &quot;bottom&quot;) # We&#39;re passing our torPlot to the ggMarginal Function ggExtra::ggMarginal(torPlot, margins = &quot;y&quot;, groupColour = TRUE, groupFill = TRUE) We can now see the distributions of NO2 and O3 overlaid on the vertical axis. Note that ggMarginal() only works with scatter plots. There are plenty of other marginal options scattered about various packages. You can see many of them in action (with beautiful examples) at Tufte in R by Lukasz Piwek. 17.5 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["customizing-your-plots.html", "Chapter 18 Customizing your plots 18.1 Interactive Plots 18.2 Plot Themes 18.3 Legends 18.4 Modifying labels 18.5 Modifying Axis 18.6 Arranging plots 18.7 Exercise", " Chapter 18 Customizing your plots Up until now we haven’t paid much attention to the explicit aesthetics of plots beyond what we needed for our exploratory analysis. However, many journals, publications, instructors, etc. will want plots to adhere to certain aesthetic standards. There are scores of options to play with, so we recommend you consult the ggplot2 Cheat Sheet. 18.1 Interactive Plots Ultimately your visualizations will be printed to a static PDF document, but in the interim having an interactive plot can be helpful for data exploration. The plotly package magically makes most ggplots interactive with a simple command. Here’s an example with our Toronto air quality data: torontoAir &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ## Rows: 507 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): city, p, pollutant ## dbl (4): naps, latitude, longitude, concentration ## dttm (1): date.time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. torPlot2 &lt;- ggplot(data = torontoAir, aes(x = date.time, y = concentration, colour = pollutant)) + geom_line() plotly::ggplotly(torPlot2) This is also super useful when surveying spectroscopy data, although the large number of points in those datasets can take a while to render into interactive plotly plots. 18.2 Plot Themes Overall themes can be applied to ggplot. The simple and minimalist theme_classic() is satisfactory for most submissions, but you can peruse the available themes in ggplot here or you can explore many more themes in the ggthemes package. # generating example plot to modify p &lt;- ggplot(data = torontoAir, aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() # Default theme default &lt;- p + labs(title = &quot;Default theme&quot;) # Classic theme classic &lt;- p + theme_classic() + labs(title = &quot;Classic theme&quot;) # Arranging into grid gridExtra::grid.arrange(default, classic, ncol = 2) 18.3 Legends You can specify the position of the legend under the theme() option as such: bottom &lt;- p + theme(legend.position = &quot;bottom&quot;) inside &lt;- p + theme(legend.position = &quot;inside&quot;, legend.position.inside = c(.5, .5)) gridExtra::grid.arrange(bottom, inside, ncol = 2) Other legend positions include: \"left\", \"right\", \"bottom\", \"top\" and \"none\" (remove legend entirely). Use legend.position.inside with a two-element numeric vector to specify the location. For example, use c(0.95, 0.95) for inside the top-right corner and c(0.05, 0.05) for inside the bottom right corner. 18.4 Modifying labels The labels generated for the plots are derived from the variable names passed along to the ggplot() function. Consequently, variable names that are easy to code become ugly labels on the plot. You can modify labels using the labs() function. How to use the labs() function was also discussed in the Bar Charts section of the Common Types of Graphs chapter. Note that in this example we changed the legend’s title by specifying what aes() option we used to create the legend; in the example below it’s colour. p + labs(title = &quot;Toronto Air quality&quot;, subtitle = &quot;from Jan 1st to 8th, 2018&quot;, x = &quot;Date&quot;, y = &quot;Concentration (ppb)&quot;, colour = &quot;Pollutant&quot;) 18.5 Modifying Axis We’ve already talked about labeling axis titles in Modifying labels, and adding marginal plots in Scatter plots. So we’ll just briefly touch upon some simple axis modifications. 18.5.1 Transforming axis Transformations are largely related to continuous data, and are done using scale_y_continuous() or scale_x_continuous() functions. For example to scale the y-axis of our plot we’d do the following: p + scale_y_continuous(trans = &quot;log10&quot;) + labs(y = &quot;Log10(concentration)&quot;) Other useful transformations include “log2” for base-2 logs, “date” for dates, and “hms” for time. The latter two are useful if R hasn’t correctly interpreted your dataset. The data type for the data.time column of our dataset was correctly interpreted during our initial importation using read_csv(). Hooray for doing it right the first time. 18.5.2 Axis limits The limits of plots created with ggplot() are automatically assigned, but you can override these using the lims() function. For example we can specify the limits of our example plot to show from 0 to 100 ppb: p + lims(y = c(0, 100)) 18.5.3 Axis ticks/labels Sometimes when you are plotting, the length of the axis labels is unreadable. This is often the case with categorical data, such as the names of cities like we’ve encountered earlier. We addressed this earlier in Bar charts by rotating the plot 90\\(^\\circ\\) with the coord_flip() function. This is often the best solution as it’s how we read English. Another solution is to rotate the axis labels themselves: basePlot &lt;- ggplot(data = filter(sumAtl, p == &quot;NL&quot;), aes(x = city, y = mean)) + geom_col() default &lt;- basePlot + labs(title = &quot;default plot&quot;) flip &lt;- basePlot + coord_flip() + labs(title = &quot;coord_flip()&quot;) rotated &lt;- basePlot + theme(axis.text.x = element_text(angle = 45)) + labs(title = &quot;element_text(angle = 45)&quot;) rotatedHJust &lt;- basePlot + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;element_text(angle = 45, hjust = 1)&quot;) gridExtra::grid.arrange(default, flip, rotated, rotatedHJust, ncol = 2, nrow = 2) 18.6 Arranging plots We talked about how facets can be used to generate multiple plots from a dataset (small multiples), but sometimes you want to combine two or more different plots together. There are a couple of ways, but we’ve been using grid.arrange() from the gridExtra package (as demonstrated above). You can read up on gridExtra here. There is also the ggarrange function from the ggpubr package which, amongst other things, can easily create shared legends between plots. colchart &lt;- ggplot(data = sumAtl, aes(x = fct_reorder(city, mean), y = mean, fill = p)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() boxplot &lt;- ggplot(data = atlNO2, aes(x = city, y = conc, fill = p)) + geom_boxplot() + coord_flip() boxplot ggpubr::ggarrange(colchart, boxplot, ncol = 2, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;), common.legend = TRUE, legend = &quot;bottom&quot;) 18.7 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["modelling-linear-regression.html", "Chapter 19 Modelling: Linear Regression 19.1 Modelling Theory 19.2 Linear Regression in R 19.3 Visualizing your models 19.4 Accessing and using model outputs 19.5 Augmented, glanced, and tidied model outputs 19.6 Building Linear Regressions for Multiple Analytes 19.7 Conclusion 19.8 Further reading 19.9 Exercise", " Chapter 19 Modelling: Linear Regression Modelling is basically math used to describe some type of system, and they are a forte of R, a language tailor made for statistical computing. Every model has assumptions, limitations, and all around tricky bits to working. There is no shortage of modelling in a myriad of contexts, but in this chapter we’ll discuss and break down the most common model you’ll encounter, the linear regression model, in the most common context, the linear calibration model, using the most common function, lm. You have probably encountered the linear regression model under the pseudonym “trend-lines”, most likely generated by Excel’s “add trend-line option” (as in CHM135). While the models we’ll be constructing with lm work much the same mathematically, unlike Excel, R returns all of the model outputs. Correspondingly, it’s easy to get lost between juggling R code, the seemingly endless model outputs, and keeping yourself grounded in the real systems you’re attempting to model. 19.1 Modelling Theory The linear calibration model relates the response of an instrument to the value of the measurand. The measurand is simply what we’re measuring, often the concentration of an analyte. So we use the measurand, which we can control via preparation of standards from reference material as the independent variable, with the instrument output being the dependent variable (as instrument response varies with concentration). Altogether we’re: Measuring the instrument response of standards of known concentration and samples of unknowns concentration. Calculating the linear calibration model (i.e. line of best fit) through our standards. Using the measurement model to calculate the concentration in our unknown from their respective instrument response. This is summarized in the figure below. Figure 19.1: Linear calibration model; figure modified from Hibbert and Gooding (2006). Before we can calculate concentrations, we need a measurement model. In other words, an equation that relates instrument response to sample concentration (or other factors). For simple linear calibration, we use: \\[y = a + bx\\] Where: \\(y\\) is the instrument response \\(x\\) is the independent variable (i.e. sample concentration) \\(a\\) and \\(b\\) are the coefficients of the model; otherwise known as intercept and slope, respectively. We’ll gloss over some of the more technical aspects of modelling, and discuss other in more detail below. For now, know that: We’re assuming our linear model is correct (i.e. the instruments actually respond linearly to concentration). All uncertainties reside in the dependent variable \\(y\\) (i.e., no errors in preparation of the standards). The values of \\(a\\) and \\(b\\) are determined by minimizing the sum of the residuals squared. The residuals are the difference between the actual measured response and where it would be if it were on the calibration line. Once we have our line of best fit, we can calculate the concentration of our unknown sample \\(i\\), from its measured response \\(y_i\\) by: \\[x_i = \\frac{y_i - b}{a}\\] There is more going on under the hood than what we’re describing here, but this should be enough to get you up and running. If you would like a greater breakdown of linear calibration modelling, we suggest you read Chapter 5 of Data Analysis for Chemistry by Hibbert and Gooding. An online version is accessible via the University of Toronto’s Library. There is also no reason the instrument response must be linear. In fact, we spend a great deal of time arranging our experiment so that we land in the ‘linear range’. For details on developing non-linear calibration curves in R see Evaluating and Improving Model Performance. 19.2 Linear Regression in R Now that we have a rough understanding of what we’re trying to do, let’s go over how to calculate linear regression models in R. Note model is a general term, in this situation we’ll be calculating a calibration curve. All calibration curves are models, but not all models are calibration curves. For our example dataset we’ll import a dataset consisting of four analytical standards of sodium plus a calibration blank all run in triplicate. The standards were measured via flame atomic emission spectroscopy (FAES). Let’s import the FAES calibration results we saw in Transform: Data manipulation. As we’ve already seen, our data is composed of four standards and a blank analyzed in triplicate. Since we’re focusing on modelling, we’ll treat the blank as a standard in our model fitting. So let’s import our dataset. Note that to make our tables more readable in the HTML version of this textbook, we are generating our tables using the DataTables library using the format DT::datatable(): FAES &lt;- read_csv(file = &quot;data/FAES_original_wide.csv&quot;) %&gt;% pivot_longer(cols = -std_Na_conc, names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) %&gt;% separate(col = std_Na_conc, into = c(&quot;type&quot;, &quot;conc_Na&quot;, &quot;units&quot;), sep = &quot; &quot;, convert = TRUE) %&gt;% mutate(type = &quot;standard&quot;) DT::datatable(FAES) And let’s quickly plot our data. You should always visualize your data before modelling, especially for linear calibration modelling. Visualizing your data is an easy way to spot trends and gross errors in your data. ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() Let’s use the lm function to calculate the linear relationship between the signal as a function of conc_Na: lmfit &lt;- lm(signal ~ conc_Na, data = FAES) lmfit ## ## Call: ## lm(formula = signal ~ conc_Na, data = FAES) ## ## Coefficients: ## (Intercept) conc_Na ## 2615 29707 Reading the code above: We’re using the FAES data we created earlier: data = FAES. We’re comparing signal (the dependent variable) to conc_Na (the independent variable) via the tilde ~. The way to read this is: “signal depends on concentration”. We’re fitting a linear model for comparing these two variables indicated by lm() The model’s outputs are stored in the lmfit variable. As we can see, the model outputs are pretty brief and not much more than Excel’s outputs. We can use summary() to extract more information to better understand our model: summary(lmfit) ## ## Call: ## lm(formula = signal ~ conc_Na, data = FAES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2112.78 -1528.53 70.51 821.50 2718.20 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2615.1 665.2 3.931 0.00172 ** ## conc_Na 29707.2 1304.5 22.772 7.34e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1824 on 13 degrees of freedom ## Multiple R-squared: 0.9755, Adjusted R-squared: 0.9737 ## F-statistic: 518.6 on 1 and 13 DF, p-value: 7.341e-12 This summary provides much more information: Under ‘Residuals’, we can see some summary statistics on the residuals of our fit (we will discuss residuals much more in the next chapter) Under ‘Coefficients’, the fit value (called ‘Estimate’) for each variable, and their predicted standard errors are given. Note that these standard errors are an approximation, but they are good enough to use for most cases. The next two columns under ‘Coefficients’, ‘t value’ and ‘Pr(&gt;|t|)’, represent a statistical test of the significance of the model fit. Here, ‘Pr(&gt;|t|)’ describes the probability (or p-value) that the fit value for each variable is nonzero (i.e. that the variable contributes significantly to the model). This is described qualitatively by the significance codes - in this case, the intercept has a p-value &lt; 0.01, and the conc-Na (our independent variable) has a p-value of &lt;0.001. The residual standard error is the root mean square error of all of the residuals of the model. The \\(R^2\\) values represent the proportion of the variance in the dependent variable that can be explained by the independent variable(s). In general, it is better to use the ‘Adjusted R-squared’, over the ‘Multiple R-squared’, as the ‘Adjusted R-squared’ corrects for a bias in \\(R^2\\) introduced by adding additional variables. The F-statistic and its p-value compare the model you generated to a model where all variables are zeros (essentially a version of the t values above, but for the overall model). If your p-value is below your threshold of statistical significance (often 0.05), then the variables you included significantly improved the model. The information provided in the model summary can be used to evaluate how well your model describes your data (in our case, how effective the calibration curve we have generated is in describing the instrumental response to our compound) and to predict new values. 19.3 Visualizing your models While the statistical outputs above provide a lot of information about the quality of our model fit, often the best way to evaluate a model’s performance is visualization. R provides an easy set of tools for visualizing models within the ggplot framework. This can be applied to any model, not just linear regression, but we are showing it here for the calibration curve. To plot a model over existing data, we take our standard scatter plot generated using geom_point and we add a geom_smooth object on top of it. modelPlot &lt;- ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=FALSE) modelPlot The geom_smooth function can be used to plot a variety of trend lines over data. In our case, we specify method = 'lm' to ensure that the trend line plotted is a straight line. We also specify se = FALSE to stop the function from plotting error bands around the line, which it would otherwise do by default. We can also add the equation and \\(R^2\\) value of a model onto a ggplot object using a function from the ggpmisc library, stat_poly_eq as follows (note that we saved our previous plot as modelPlot, so we can just add the new layer on top): modelPlot &lt;- modelPlot + ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(after_stat(eq.label), after_stat(rr.label), sep = &quot;~~~&quot;)), parse = TRUE, size = 5) modelPlot The syntax for stat_poly_eq is a little complicated, but it is just a way of showing a polynomial equation based on statistical analysis of a dataset. The polynomial itself is specified by formula = y ~ x (using the names of the variables from the ggplot aes() call). The number of digits in the \\(R^2\\) value is given by rr.digits = 4, the label is given by aeslabel = paste(after_stat(eq.label), after_stat(rr.label), sep = \"~~~\"). Most of this is formatting - the important parts are after_stat(eq.label), which calculates the label for the fit equation after statistical analysis, and after_stat(rr.label), which is the same thing but for the \\(R^2\\) value. You can also change the size of the label by changing size = 5. 19.4 Accessing and using model outputs Once you have generated a model, you likely want to use it to make predictions. For simple models like calibration curves, the simplest way to do this is to access the coefficients. Let’s start by importing a dataset to make predictions on: FAESSamples &lt;- read_csv(file = &quot;data/FAESUnknowns.csv&quot;) %&gt;% pivot_longer(cols = -c(sample, `dilution factor`), names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) DT::datatable(FAESSamples) We can extract the coefficients of our model (recall we stored our calibration curve in the variable lmfit) using the coef function: coef(lmfit) ## (Intercept) conc_Na ## 2615.119 29707.170 Then, we use some base R indexing syntax in brackets to select which coefficient we want: intercept &lt;- coef(lmfit)[1] slope &lt;- coef(lmfit)[2] paste(&quot;The equation of our calibration curve is: y = &quot;, intercept, &quot; + &quot;, slope,&quot;*x&quot;, sep=&quot;&quot;) ## [1] &quot;The equation of our calibration curve is: y = 2615.11945030675 + 29707.1701380368*x&quot; We can apply this calibration curve to our new data directly within tidyverse using the mutate function. Recall that we are trying to calculate our dependent variable, concentration, so we solve the calibration curve equation for x: FAESSamples &lt;- FAESSamples %&gt;% mutate(&quot;instConc&quot; = (signal - intercept)/slope) DT::datatable(FAESSamples) Correcting for the supplied dilution factors and summarizing our data a little bit, we have easily calculated the concentrations from a selection of new samples in an automated fashion: FAESresults &lt;- FAESSamples %&gt;% mutate(&#39;SampleConc&#39; = instConc * `dilution factor`) %&gt;% group_by(sample) %&gt;% summarize(&quot;Mean Concentration Na (ppm)&quot; = mean(SampleConc), &quot;Standard Deviation (ppm)&quot; = sd(SampleConc), N = n()) DT::datatable(FAESresults) 19.5 Augmented, glanced, and tidied model outputs In most cases, the base lm function and summary output and will be sufficient for your needs, but there are other ways of presenting model outputs using the broom package, which may or may not be useful. Either way, it’s helpful to be aware of them. 19.5.1 Tidied outputs The function that is most likely to be useful to you is the tidied output, which we will use in the next section. Tidied model outputs allow you to easily access model fit parameters in a tidyverse style. This can be accessed by calling the broom function tidy on the model you have generated. This creates a dataframe (or rather, a tibble) which we can visualize as normal: library(broom) tidied &lt;- tidy(lmfit) DT::datatable(tidied) As you can see, the output table has each variable in a row with their respective fit values (called estimate again here), standard errors, t-values, and p-values. These are the same outputs as were given in from the summarize function, but tabulated for easy access. Similar to using the coef function, you can extract model fits to predict new values relatively easily using this format with base R indexing via bracket notation as below. In the next section, we will use this format to automatically apply multiple different calibration curves to a dataset. tidyIntercept &lt;- tidied[1,2] tidySlope &lt;- tidied[2,2] paste(&quot;The equation of our calibration curve is: y = &quot;, tidyIntercept, &quot; + &quot;, tidySlope,&quot;*x&quot;, sep=&quot;&quot;) ## [1] &quot;The equation of our calibration curve is: y = 2615.11945030675 + 29707.1701380368*x&quot; 19.5.2 Glanced outputs The ‘Glanced’ output is a way of summarizing the quality of the model fit in a single row, so that it can be easily included in a dataframe. The glanced output again generates a tibble using the broom function glance: glanced &lt;- glance(lmfit) DT::datatable(glanced, options = list(scrollX = TRUE)) The glanced output has most of the same outputs as summary, plus some extra. Variables described previously include the two \\(R^2\\) values (remember we want to use the adjusted R-squared in almost all cases), sigma, which is the estimated standard error of the residuals, and statistic, p.value, and df, which are the F-statistic, its p-value, and the number of degrees of freedom used in the numerator of the F-statistic. New outputs include deviance, logLik, AIC, BIC, the number of observations, and the number of residual degrees of freedom. Deviance is the sum of the squares of the residuals of the model. The logLik is the log-likelihood of the model used to calculate AIC and BIC, the Akaike information criterion and Bayesian information criterion, which are estimations of the model quality. For additional information on these values, you will have to delve into the statistics literature. 19.5.3 Augmented outputs Finally, the augmented output is a complete summary of the fit behavior of the model, generating a tibble using the broom function augment: augmented &lt;- augment(lmfit) DT::datatable(augmented, options = list(scrollX = TRUE)) Here, you can see the dependent variable (here signal), the independent variable (here conc_Na), the fitted model response (.fitted), the residual for that fit value (.resid), the diagonal of the hat matrix (.hat), the model standard error when the listed observation is dropped from the model (‘.sigma’), the Cooks distance of that observation (.cooksd), and the standardized residuals (.std.resid). It is unlikely that you will use most of these values, but it is useful to know how to calculate them, and we will revisit the residuals in the next chapter. 19.6 Building Linear Regressions for Multiple Analytes In many cases in analytical chemistry, we find ourselves with multiple analytes, each of which will have their own calibration curves. We can easily use the power of R to calculate these calibration curves, store them in a logical way, and apply them to new data in an automated fashion. 19.6.1 Dataset The dataset metalConc contains information on metal concentrations measured in parts per million, or ppm, and corresponding signal values (counts per seconds, or cps) from the inductively coupled plasma optical emission spectrometer (ICP-OES) in ANALEST for 4 metals: Aluminum, Magnesium, Calcium, and Potassium. We will also import a dataset of samples run on the same ICP-OES to apply our new calibration curves. Take a look at the calibration data before proceeding with our analysis (we have done some pre-analysis to get it in the right format here): metalConc &lt;- read_csv(&quot;data/metal_concentration.csv&quot;) metalSignals &lt;- read_csv(&quot;data/metal_signals.csv&quot;) %&gt;% mutate(samplenumber = c(1,2,3,4,5,6)) %&gt;% pivot_longer(cols = -samplenumber, names_to = &quot;metal&quot;, values_to = &quot;Signal&quot;) DT::datatable(metalConc) 19.6.2 Building a Linear Regression for Each Analyte Now, we will build separate linear regression models for each metal to predict signal values based on metal concentrations. This can be done either in base R using a for loop or in Tidyverse using the mutate function. We will show both methods. 19.6.2.1 Tidyverse The Tidyverse approach is generally preferred in R, as the code is easier to read and faster to implement, and the results are accessible automatically. Our data is not currently in tidy format, so let’s pivot it into a tidy format first. We use the pivot_longer function to turn our separate metal columns into two columns, one containing the metal identity for each sample, and the other containing the signal intensity for that metal: tidiedMetals &lt;- metalConc %&gt;% pivot_longer(cols = -Concentration, names_to = &quot;metal&quot;, values_to = &quot;Signal&quot;) DT::datatable(tidiedMetals) Now we can construct calibration curves for each metal separately all within the pipe, and extract the fit parameters using the broom tidy function. This is a little complicated, so let’s go through it step-by-step. 1. We group the data by metal using the group_by function to allow us to create separate calibration curves for each metal 2. We nest the data using the nest function, which stores the entire dataset for each metal into its own column, “nesting” those dataframes within our larger dataframe. 3. We can then build models off of those datasets using the mutate function combined with the map function. map tells R that we want to apply a function (in this case lm) to data that we specify. We assign the generated models to a new column, model, using mutate. 4. Within the same mutate call, we use map to call the tidy function on each model individually and assign it to a new column called tidied, which stores the entire dataframe in the same column 5. We call unnest to extract the dataframes contained within tidied 6. We generate a data table to view our newly analyzed data tidyCalCurve &lt;- tidiedMetals %&gt;% group_by(metal) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(Signal ~ Concentration, data = .x)), tidied = map(model, tidy)) %&gt;% unnest(tidied) DT::datatable(tidyCalCurve, options = list(scrollX = TRUE)) We now have a tidy version of our calibration curves with the models still accessible under the ‘model’ column, and the values, standard errors, and p-values of our slope and intercept. If we instead wanted to evaluate how well each calibration curve worked, we would use the glance function, like so, which is doing exactly the same thing but with the glance function instead of the tidy function. glancedCalCurve &lt;- tidiedMetals %&gt;% group_by(metal) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(Signal ~ Concentration, data = .x)), glance = map(model, glance)) %&gt;% unnest(glance) DT::datatable(glancedCalCurve, options = list(scrollX = TRUE)) You can feel free to do whatever data manupulation you want to get the right combination of estimate values and measurements of goodness of fit. For example, say you wanted slope, intercept, and adjusted R-squared in columns for a report. You could perform the following functions. There’s a lot going on here, so we’ll go through this step-by-step as well. 1. This time we have generated both tidied and glanced dataframes for each model and nested them as above. 2. This time, we unnest glance and select only the information we want, in this case retaining the metal, adj.r.squared, and tidied columns 3. Then, we unnest tidied and use pivot_wider to convert the tidyverse format tidy output into wide format results for legibility, setting the column names based off of the tidy term column and the results from the estimate column 4. We rename the terms for legibility 5. We select only the results we want 6. We generate a readable data table combinedCalCurve &lt;- tidiedMetals %&gt;% group_by(metal) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(Signal ~ Concentration, data = .x)), tidied = map(model, tidy), glanced = map(model, glance)) %&gt;% unnest(glanced) %&gt;% select(metal, adj.r.squared, tidied) %&gt;% unnest(tidied) %&gt;% pivot_wider(id_cols = c(metal, adj.r.squared), names_from = &#39;term&#39;, values_from = &#39;estimate&#39;) %&gt;% rename(slope = Concentration, intercept = `(Intercept)`, adjRsquared = adj.r.squared) %&gt;% select(metal, slope, intercept, adjRsquared) DT::datatable(combinedCalCurve) You can always directly access the values in any of these formats using the select and filter functions. Here’s how to do that with the above table, accessing the slope of the Ca calibration curve as an example. The double square brackets access the specific value instead of returning a dataframe with only one element. CaSlope &lt;- combinedCalCurve %&gt;% filter(metal == &#39;Ca&#39;) %&gt;% select(slope) CaSlope[[2]] ## [1] 377609.7 If we want to use these values to calculate new concentrations in the metalSignals dataframe, we just have to do a little bit of code using tidyverse. Here is one way to do it by joining the calibration dataset with our new samples using the full_join function. This allows us to add one dataframe (here tidyCalCurve) to another (metalSignals), by comparing the values of one of the columns (here metal): #Start by creating our calibration curves in wide format, with the metal in one column, the slope in the second, and the intercept in the third. tidyCalCurve &lt;- tidiedMetals %&gt;% group_by(metal) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(Signal ~ Concentration, data = .x)), tidied = map(model, tidy)) %&gt;% unnest(tidied) %&gt;% pivot_wider(id_cols = c(metal), names_from = &#39;term&#39;, values_from = &#39;estimate&#39;) %&gt;% rename(slope = Concentration, intercept = `(Intercept)`) %&gt;% select(metal, slope, intercept) #Then, we join the two datasets with &#39;metal&#39; as the common column between them tidyResults &lt;- full_join(metalSignals, tidyCalCurve, by = &#39;metal&#39;) #Finally, we calculate the concentrations for each of our samples. tidyResults &lt;- tidyResults %&gt;% mutate(concentration = (Signal - intercept)/slope) #And output our results in a readable data table DT::datatable(tidyResults, options = list(scrollX = TRUE)) As you can see, many of our samples have concentrations close to zero or even negative. Negative concentration values can arise when signal values fall below the intercept of the linear regression model, indicating a deviation from the model’s assumptions or that the concentration of the analyte is below the limit of detection of an instrument, where instrument noise or background signal may influence measurements. Negative concentrations lack meaningful interpretation in chemical analysis and are often considered artifacts of the model’s limitations. In the next chapter, we will discuss how to manage these types of issues through calculating limits of detection, applying nonlinear calibrations, and weighting our calibration curves to increase the accuracy of our calibration models. 19.6.2.2 Base R Here is one way to perform the same procedure to calculate linear regressions for each analyte in base R: # Create lists to store metal names, slope, intercept, and R^2 for each metal metals &lt;- colnames(metalConc)[2:5] slope &lt;- c() intercept &lt;- c() R2 &lt;- c() # Iterate over each metal for (metal in metals) { # Build linear regression model model &lt;- lm(metalConc[[metal]] ~ Concentration, data = metalConc) # Extract slope, intercept, and R squared slope[metal] &lt;- coef(model)[2] intercept[metal] &lt;- coef(model)[1] R2[metal] &lt;- summary(model)$r.squared } #Combine the resulting lists into a tibble metalResults &lt;- tibble(metal = metals, slope, intercept, R2) #Generate a datatable to output the DT::datatable(metalResults, options = list(scrollX = TRUE)) The code above uses a for loop, that was discussed in the Loops section of this resource. The loop will move through each metal in the dataset and then store the slope, intercept and \\(R^2\\) values in their own vectors. We then combine these vectors into a table. This table can be accessed and used exactly as in the Tidyverse methods. 19.6.3 Visualizing Linear Regressions for Each Analyte After constructing linear regression models for each metal, visualizing these results can provide valuable insights into the relationships between metal concentrations and signal values. Below, we will create a single scatterplot for four selected metals along with their regression lines. This visualization will help in assessing the fit of the models and understanding the variance in signal values as a function of concentration. To make this plot, we will be using the tidied version of the dataset, produced using pivot_longer above. # Create a scatterplot for each metal with its regression line ggplot(tidiedMetals, aes(x = Concentration, y = Signal, color = metal)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(color = metal)) + labs(title = &quot;Regression Lines for Selected Metals&quot;, x = &quot;Concentration&quot;, y = &quot;Signal&quot;) + theme_minimal() + scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;)) Based on this plot, we can see that our instrument is much more sensitive to calcium than the other metals, and has very low sensitivity to aluminum, which may explain some of the odd concentrations we observed previously. 19.7 Conclusion In conclusion, while linear regression serves as a powerful tool for analyzing the relationship between chemical concentrations and instrumental signals, its applicability may be limited by the assumption of linearity. Negative concentration values observed in linear regression analyses highlight the need for robust statistical techniques and critical evaluation of instrument responses. In the next chapter, we will explore some alternative modeling approaches as well as some techniques for evaluating the effectiveness of the models produced. 19.8 Further reading As previously stated, we highly recommend reading Chapter 5: Calibration from Data Analysis for Chemistry by Hibbert and Gooding for a more in-depth discussion of linear calibration modelling. The book can be accessed online via the University of Toronto’s Library. For a greater discussion on modelling in R, see Modelling in R for Data Science. 19.9 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["evaluating-and-improving-model-performance.html", "Chapter 20 Evaluating and Improving Model Performance 20.1 Residuals for Model Appropriateness 20.2 Nonlinear Calibration Curves 20.3 Weighted Calibration Curves 20.4 Removing outliers 20.5 Evaluating Model Performance 20.6 Using Calibration Curves to Estimate LOD and LOQ 20.7 Inverse Calibration Curves 20.8 Exercise", " Chapter 20 Evaluating and Improving Model Performance In this chapter, we will be discussing methods to evaluate model performance and eventually generate more effective models in R, again in the context of calibration curves in analytical chemistry. As in the previous chapter, we will be using the FAES dataset to demonstrate these concepts. Here is a reminder of what that data looks like: library(tidyverse) library(broom) #Load the data file FAES &lt;- read_csv(file = &quot;data/FAES_original_wide.csv&quot;) %&gt;% #Pivot the data into long (i.e. tidy) format pivot_longer(cols = -std_Na_conc, names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) %&gt;% #Clean up the data columns separate(col = std_Na_conc, into = c(&quot;type&quot;, &quot;conc_Na&quot;, &quot;units&quot;), sep = &quot; &quot;, convert = TRUE) %&gt;% mutate(type = &quot;standard&quot;) #Output a html-friendly data table DT::datatable(FAES) And, as always we visualize the data before attempting any modelling: ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() 20.1 Residuals for Model Appropriateness The residuals of a model are defined as the difference between a measured and fitted value at each datapoint. We briefly showed them last chapter, but they are actually far more important to this modelling framework than we have given them credit for. In the least-squares method we are using for linear modelling, the goal of the model is actually to minimize the sum of the squares of the residuals. However, the residuals also have some additional uses beyond defining the goal of our model fitting algorithm, which we will explore here. The basic idea is that the residuals can show patterns in the underlying data that the model we use doesn’t capture. If a model is missing that pattern, that might mean that our model is not the best option. To assess this, we visualize our residuals after making our models. Below are some examples of what these kinds of visualization will look like in the context of a linear calibration curve. Figure 20.1: Example residual patterns; figure adapted from Hibbert and Gooding (2006). A good linear (or any) model will have residuals normally distributed about zero, shown in the upper left-hand corner below. The remaining examples show other types of patterns you might see when plotting residuals. The upper right-hand example shows residuals with a quadratic trend, indicating that there is nonlinearity in our data that the model used does not capture. The lower left-hand example shows increasing magnitude of the residuals, which is indicative of heteroscedasticity in our data, meaning the magnitude of the uncertainty in each data point is changing as the concentration changing This is a problem because one of the assumptions of the model fitting process is homoscedasticity (i.e. the magnitude of error is the same for each data point). Finally, in the lower right-hand corner, we can identify an outlier in the data based on the residual plot. We will discuss how to fix the other two issues in later sections, but first let’s generate a residual plot for our own data. We start by developing our linear model as we did last chapter: #Generate a linear model between conc_Na and signal lmfit &lt;- lm(signal ~ conc_Na, data = FAES) lmfit ## ## Call: ## lm(formula = signal ~ conc_Na, data = FAES) ## ## Coefficients: ## (Intercept) conc_Na ## 2615 29707 We can extract the residuals of this model two different ways. First is the built-in method using the resid function: lmresid &lt;- resid(lmfit) lmresid ## 1 2 3 4 5 6 ## -2112.78035 -2023.34265 -2033.66065 70.50554 67.72194 81.60224 ## 7 8 9 10 11 12 ## 836.86322 806.13232 775.49632 2718.19878 2671.80698 2684.28108 ## 13 14 15 ## -1524.59609 -1485.76569 -1532.46299 However, as always Tidyverse has a nicer way of doing it. Recall the augment function within the broom library that we discussed in the previous chapter. Along with some other outputs, it provides a well-organized dataframe containing residuals and standardized residuals. Let’s look at that output again here: #Generate the augmented dataframe augmented &lt;- augment(lmfit) #Output an html-friendly table of the augmented results DT::datatable(augmented, options = list(scrollX = TRUE)) It is useful to have this information pre-organized into a dataframe, as it makes for easy plotting. Let’s try plotting the residuals (stored as .resid in the augmented dataframe) against our independent variable, the known concentration of Na in these standards (stored as conc_Na), along with a straight horizontal line at zero for comparison. We are doing this with the residuals themselves, but you could also plot the standardized residuals (this just means they are normalized by the mean and standard deviation of the residuals) or the Cooks Distance - each is a different but valid way of representing the model error as a function of the independent variable. We also draw the calibration curve itself for comparison: #Plot the residuals of the linear model (in .resid) vs concentration resid &lt;- ggplot(data = augmented, aes(x = conc_Na, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + labs(x = &quot;Na Concentration (ppm)&quot;, y = &quot;Residual&quot;) + #theme_classic() makes the plot look a little nicer theme_classic() #Plot the calibration data with a calibration curve overlaid on top cal &lt;- ggplot(data = augmented, aes(x = conc_Na, y = signal)) + geom_point() + labs(x = &quot;Na Concentration (ppm)&quot;, y = &quot;Signal&quot;) + theme_classic() + geom_smooth(method = &#39;lm&#39;, se=FALSE) + ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(after_stat(eq.label), after_stat(rr.label), sep = &quot;~~~&quot;)), parse = TRUE, size = 2) gridExtra::grid.arrange(cal, resid, ncol = 2, nrow = 1) Based on this plot, we can see that our data has a pretty clear nonlinear trend - the residuals look like a parabola. This indicates that we might have some nonlinearity in our standard curve, likely due to nonlinearity in the instrument response. We can also see how much easier it is to spot that trend in the residual plot than it is in the original calibration curve plot. It is always a good idea to plot residuals after making a model to check how well your model is performing and to see if there are ways you could improve your results. Next, we will discuss ways to modify our calibration models to improve their performance. 20.2 Nonlinear Calibration Curves If you plot your residuals for a linear model and they have a clear curvature (see the top right example at the start of the section 20.1), the next step is to try a nonlinear model, the most common and useful of which is the second-order polynomial. Many instruments in analytical chemistry have a nonlinear response profile once you reach a certain concentration, at which point the response follows a quadratic curve. This is particularly common in LC-MS analysis as so much of an analyte is being ionized and detected that it suppresses some ionization and/or reduces the detector response. It is easy to create quadratic calibration curves in R - in fact, we can use exactly the same function as we used for a linear model, with just a slight adjustment: #Generate the quadratic model using the lm function quadFit &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAES) quadFit ## ## Call: ## lm(formula = signal ~ conc_Na + I(conc_Na^2), data = FAES) ## ## Coefficients: ## (Intercept) conc_Na I(conc_Na^2) ## 717.6 47698.6 -17613.3 Here, we must put the second-order term (conc_Na^2) within the I function, which tells R to read the term conc_Na^2 “as-is”, allowing R to understand that we mean we want this term to be squared instead of treating it as code. The output coefficients have names that are a little difficult to read, but the (Intercept) is the intercept, conc_Nais the linear term, andI(conc_Na^2)` is the quadratic term. As with the linear model, we can plot the fitted model over our data and extract the residuals. Let’s do that now to compare our results. Note that this time, we have to provide the formula for geom_smooth to use, and for the purposes of stat_poly_eq, we have to use the poly(x,2,raw=TRUE) syntax instead of using the I function to allow it to interpret the equation correctly. Also, when we extract the residuals using the broom augment function, we resupply the original dataset FAES. This is because augment will otherwise try to reconstruct the original dataset, splitting conc_NA into a linear and quadratic term, which will get messy. #Generate the augmented summary of the quadFit model on the FAES dataset quadAugmented &lt;- augment(quadFit, FAES) #Plot the calibration curve with a quadratic model overlaid on top of it cal &lt;- ggplot(data = quadAugmented, aes(x = conc_Na, y = signal)) + geom_point() + labs(x = &quot;Na Concentration (ppm)&quot;, y = &quot;Signal&quot;) + theme_classic() + geom_smooth(method = &#39;lm&#39;, formula = y ~ conc_Na + I(conc_Na^2), se=FALSE) + ggpmisc::stat_poly_eq(formula = y ~ poly(x, 2, raw=TRUE), # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(after_stat(eq.label), after_stat(rr.label), sep = &quot;~~~&quot;)), parse = TRUE, size = 2) #Plot the residuals of the quadratic model resid &lt;- ggplot(data = quadAugmented, aes(x = conc_Na, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + labs(x = &quot;Na Concentration (ppm)&quot;, y = &quot;Residual&quot;) + theme_classic() gridExtra::grid.arrange(cal, resid, ncol = 2, nrow = 1) This is definitely looking better! The magnitude of the residuals has decreased by an order of magnitude. To use a quadratic calibration curve to calculate new concentration values, we cannot just solve for x in the calibration curve equation - we must plug in our ‘y’ value, subtract it from our intercept to get an expression equal to zero and use the quadratic equation to solve for the roots of x. Alternatively, an inverse calibration curve can be constructed - this will be discussed in a later section. We will discuss more about how to decide whether a quadratic or linear model is better in a later section, but for now, notice how the residuals still have a shape - let’s try to fix that using another modeling trick. 20.3 Weighted Calibration Curves If you run into residuals with a ‘cone’ shape, such as in the bottom-left of the examples in section 20.1, you have data that are heteroscedastic. The best solution to this issue is to develop a weighted model. Weighting a calibration curve (or any model) is the process of applying a multiplicative factor to each data point during the modeling process. The goal of this process (assuming data collected perfectly with normally distributed, predictable errors) is to normalize the residuals of each data point collected, such that no data point has undue influence over the model. There are actually two situations where weighting a model is particularly helpful: The first is what we just discussed, as weighting can solve the problem of heteroscedasticity by reducing the reliance of the model on data with larger uncertainties. If a the weighting scheme is chosen such that: \\[w = \\frac{1}{\\sigma^2}\\] Where \\(w\\) is the weight and \\(\\sigma\\) is the standard deviation of our dependent variable (recall that standard deviation is the square root of the variance), then the error associated with each data point is normalized out and the model can be fit accurately. Practically, the variance of data is rarely known, so instead we use one of several weighting schemes related to our observations, such as: \\[w = \\frac{1}{y^2} \\ \\ \\ OR \\ \\ \\ w = \\frac{1}{x^2} \\ \\ \\ OR \\ \\ \\ w = \\frac{1}{x} \\ \\ \\ OR \\ \\ \\ w = \\frac{1}{s(y)^2}\\] Where \\(x\\) and \\(y\\) are the independent and dependent variables of the model respectively, and \\(s(y)\\) is the experimental standard deviation of the dependent variable. The first three options assume that the standard deviation or variance of the dependent variable (in this case instrument signal response) is proportional to either the dependent or independent variables. The final option does not make this assumption, but it requires averaging replicates, resulting in fewer data points for regression. The second area where weighting can help is when a calibration curve spans a large range of concentrations. When this happens, the fitting algorithm based on the squares of the residuals starts to consider higher concentrations more than lower concentrations because the residuals of those values tend to be higher if the model does not pass close to them. To correct for this issue, the \\(1/x\\) weighting scheme is generally quite effective. There is no all-purpose answer for what weighting scheme to use - this is an area where you will have to use your judgement, and maybe try multiple options to see what gives you the best-performing model. Generally speaking, if you are more concerned about heteroscedasticity in your results (maybe you plotted your residuals and the magnitude of the residuals is increasing), then the \\(1/x^2\\) or \\(1/y^2\\) (or \\(1/s(y)^2\\) if you have a measurement of standard error) options are good. If you just want to correct for your calibration crossing multiple orders of magnitude, use the \\(1/x\\) scheme. For the purposes of our quadratic calibration curve example, the magnitude of the residuals is not increasing with respect to concentration - it looks more like the model is doing a better job at higher concentrations than lower ones. That sounds like the second case where weighting is useful, so in practice the \\(1/x\\) scheme is probably our best bet. However, for illustrative purposes we’ll try all of the first three weighting schemes to compare them. Here is how you add weights to a model in R. Note that we have to remove concentrations of 0, as a 1/x weight would be infinite for these points, and we also will recalculate the unweighted calibration curve without these points for comparison. We also use the cbind function, which binds together a set of columns into a dataframe: #Removing standard with concentrations = 0 (our blanks) FAESweighted &lt;- FAES %&gt;% filter(conc_Na &gt; 0) #Re-calculating an unweighted quadratic model unweightedCal &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAESweighted) #Weighted quadratic model with a 1/x scheme weighted1overxCal &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAESweighted, weights = 1/conc_Na) #Weighted quadratic model with a 1/x^2 scheme weighted1overx2Cal &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAESweighted, weights = 1/conc_Na^2) #Weighted quadratic model with a 1/y^2 scheme weighted1overy2Cal &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAESweighted, weights = 1/signal^2) #Bind together a &#39;term&#39; column and the coefficients of the different models produced above as separate columns with specified names based on their weighting scheme compareFits &lt;- cbind(&quot;term&quot; = c(&quot;Intercept&quot;, &quot;x&quot;, &quot;x^2&quot;), &quot;unweighted&quot; = tidy(unweightedCal)$estimate, &quot;1/x&quot; = tidy(weighted1overxCal)$estimate, &quot;1/x^2&quot; = tidy(weighted1overx2Cal)$estimate, &quot;1/y^2&quot; = tidy(weighted1overy2Cal)$estimate) DT::datatable(compareFits, options = list(scrollX = TRUE)) You can see how the results, particularly in the quadratic term, are somewhat different depending on the weighting scheme. The weighted models are deemphasizing the higher concentration points, resulting in a lower magnitude of the quadratic term. Finally, let’s look at the residuals for these three weighting schemes vs. without a weighting scheme: We can see that the weighted residual plots are a little more normally distributed than the unweighted curve and that with the stronger weights (\\(1/x^2\\) and \\(1/y^2\\)), the residuals of the lowest concentration points are closer to zero. However, we will need to do additional analysis to determine whether weighting has meaningfully improved our model performance. We will talk about that next. There is one major caveat when using weighted fits with approximate weights as we are doing here: The standard errors reported by the model outputs become inaccurate. This is because when you include weights, they are incorporated into the equations used to calculate standard errors of the model parameters. Therefore, if your weights are estimated, your standard errors become unreliable. Keep that in mind when using weighted fits - this means that you cannot use a weighted fit to calculate results such as LOD and LOQ. 20.4 Removing outliers Finally you visually identify an outlier within a calibration curve (or any model) by plotting your residuals as in the example in the bottom right of the plot in section 20.1, you can test whether that outlier is statistically outside the range of the model using a basic statistical test on the residuals. The dataset we’ve been working with doesn’t have any outliers (generally these are removed and rerun prior to making calibration curves), so let’s work with a simple toy dataset similar to the one used to generate the example outlier residual plot: #Generating a toy dataset residData &lt;- data.frame(&quot;x&quot; = c(1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10), &quot;y&quot; = c(1.0,1.1, 2.0,2.1, 3.3,3.2, 4.2, 4.3, 5.2, 2.3, 6.3,6.4, 7.3,7.4, 8.4, 8.5, 9.3,9.5, 10.4,10.6)) ggplot(data = residData, aes(x = x, y = y)) + geom_point() + theme_classic() We can see we have a pretty clear outlier at x = 5, but otherwise our data seems pretty linear. Here’s what it would look like if we added a linear model to this data: #Generating a linear model on the toy dataset lmWithOutlier &lt;- lm(y ~ x, data = residData) #Plotting the generated model over the data ggplot(data = residData, aes(x = x, y = y)) + geom_point() + theme_classic() + geom_smooth(method = &#39;lm&#39;, se=FALSE) + ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(after_stat(eq.label), after_stat(rr.label), sep = &quot;~~~&quot;)), parse = TRUE, size = 5) We can see that the linear model isn’t fitting any of our datapoints that well and has a relatively low \\(R^2\\) value. This is because it’s being pulled down or leveraged by the outlier, which is called a high-leverage point. We can see this well in the residual plot: ggplot(data = augment(lmWithOutlier), aes(x = x, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + labs(x = &quot;Independent variable&quot;, y = &quot;Residual&quot;) + theme_classic() Again, we can clearly see the outlier at x = 5. To test whether this point is really an outlier, we can use studentized residuals, which are called standardized residuals when calculated using the broom augment function. These studentized residuals are calculated by normalizing to the standard error of the model with the data point being calculated removed, and they are often used to identify outliers in datasets used for regression. The rule of thumb is if the absolute value of the studentized residual is greater than 3, then that data point is likely an outlier. If we plot the studentized residuals here: #Plotting studentized residuals of the model ggplot(data = augment(lmWithOutlier), aes(x = x, y = .std.resid)) + geom_point() + geom_hline(yintercept = 0) + labs(x = &quot;Independent variable&quot;, y = &quot;Residual&quot;) + theme_classic() We see that our studentized residual for our suspected outlier is less than -4, so we can conclude it is a likely outlier, and it is probably better to remove it when creating our calibration curve. 20.5 Evaluating Model Performance Often, we will run into a situation where we have multiple possible models we can create, and we want to decide which of them is the best for our purposes. For calibration curves, this might mean that we want to compare a linear vs. nonlinear calibration curve or a weighted vs. unweighted calibration curve. So far, we have done so by comparing the shapes of the resulting residual plots, but this can become subjective and unclear in many cases. By performing the modeling in R, you have access to a variety of quantitative tools to compare models. A few of these options will be discussed in this section. 20.5.1 Choosing the best model Let’s start by performing this analysis of which model is best on our example dataset. Say we want to start by deciding whether our linear model or quadratic model is the better choice. We will also add in a cubic function for illustrative purposes. We start by constructing these models, and then we will make use of the broom glance function to compare several goodness-of-fit measurements between the two models. We also use the rbind function to bind together the single-rowed glanced outputs into a single multi-row table with defined row names: #Generating a linear model linear &lt;- lm(signal ~ conc_Na, data = FAES) #Generating a quadratic model quadratic &lt;- lm(signal ~ conc_Na + I(conc_Na^2), data = FAES) #Generating a cubic model cubic &lt;- lm(signal ~ conc_Na + I(conc_Na^2) + I(conc_Na^3), data = FAES) #Binding together the glanced outputs of the three models rowwise into a single dataframe glanced &lt;- rbind(&quot;linear&quot; = glance(linear), &quot;quadratic&quot; = glance(quadratic), &quot;cubic&quot; = glance(cubic)) DT::datatable(glanced, options = list(scrollX = TRUE)) Let’s go through what these results tell us. - We will ignore the first value, the multiple \\(R^2\\), as this value will strictly increase as we add more parameters into the model. - A more useful measure of goodness of fit is the adjusted \\(R^2\\) value, which is a measurement of how much of the variance in our dependent variable this model captures. We can see that from the linear to quadratic model, there is a significant improvement, while there is actually a slight decrease in adjusted \\(R^2\\) between the quadratic and cubic model. This evidence supports using the quadratic model. - Comparing the F-statistic and p-value of the three functions, we can see that the quadratic fit has the lowest p-value, followed by the cubic and then the linear model. Again, this supports using the quadratic model. - Finally, the AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are parameters that evaluate the relative quality of different models applied to the same set of data. Technically speaking, they are measurements of the amount of information lost in the models we are generating relative to underlying the process used to generate the data. A lower AIC/BIC indicates that less information is lost when using this model. For our models, we can see that the quadratic model has the lowest AIC and BIC, followed by the cubic model, then the linear model. These parameters also support using the quadratic model. There is one other test you can try to compare different models like this, which is specifically useful when deciding whether to include an extra term in a model or not - the p-value of each specific fit parameter. These are accessible directly from the summary function or in the broom tidy function. Recall the tidy format of models in R for the cubic model: #Generate the tidied summary of the cubic model tidycubic &lt;- tidy(cubic) %&gt;% #replace the &#39;term&#39; column with more legible values mutate(term = c(&quot;Intercept&quot;, &quot;x&quot;, &quot;x^2&quot;, &quot;x^3&quot;)) DT::datatable(tidycubic, options = list(scrollX = TRUE)) Looking at this table in the p.value column, we can see the p-values of a t-test of whether each parameter is statistically significantly different from zero. The intercept, first, and second-order terms have very low p-values. However, the cubic term is around 0.5, much too high, indicating that the fit value of the cubic term is statistically indistinguishable from zero. This alone is grounds to reject the cubic model over the quadratic and linear models. Let’s look at the p-values of the quadratic model just to be sure: #Generating the tidied summary of the quadratic model tidyquadratic &lt;- tidy(quadratic) %&gt;% #replace the &#39;term&#39; column with more legible values mutate(term = c(&quot;Intercept&quot;, &quot;x&quot;, &quot;x^2&quot;)) DT::datatable(tidyquadratic, options = list(scrollX = TRUE)) Sure enough, the p-values for all three of the terms of the quadratic model are very low, indicating that it is likely these terms are statistically nonzero in the best form of the model. Keep in mind that this test is not perfect - just because a parameter has a p-value less than some cutoff (0.05, 0.01, or even lower) does not necessarily mean that it is a useful parameter to include in your model. The best approach in model selection is always to assemble a weight of evidence, like we have done here: The residual plot, adjusted \\(R^2\\), F-test p-value, AIC and BIC, and t-test p-value for the individual parameters all indicate that the quadratic model is the best fit for our data, so we can be reasonably sure that the quadratic model is the best choice in this case. The linear model appears to miss some of the nonlinearity in the dataset, while the cubic model seems to overfit the results slightly. However, there will be situations where these parameters do not all agree, and you will have to make a judgement call about which model you want to use. A good additional factor to consider in these cases is what you want to use the model for: Are you using it to describe your data effectively to learn something fundamental about your results (say, if you want to calculate LOD and LOQ)? Or, are you more interested in using the model to best predict new outcomes? In the first case, you likely want to use the simplest possible model that explains most of your results. In the second choice, you may be okay with using a more complex model that can capture more of the nuances in your data. Therefore, the choice of the best model may vary depending on your goals. 20.5.2 To weight or not to weight? We can also use some of these parameters describing model goodness-of-fit to compare different weighting schemes quantitatively to choose between them. Let’s try this with the four different weighted models we created earlier: #Generate a table by binding the glanced outputs of models with different weighting schemes into a single dataframe weightedGOF &lt;- rbind(&quot;unweighted&quot; = glance(unweightedCal), &quot;1/x&quot; = glance(weighted1overxCal), &quot;1/x^2&quot; = glance(weighted1overx2Cal), &quot;1/y^2&quot; = glance(weighted1overy2Cal)) DT::datatable(weightedGOF, options = list(scrollX = TRUE)) Notice how some of the parameters such as sigma and deviance have completely different orders of magnitude depending on the weighting scheme - this is because the weights are directly used to calculate these values. However, we can still compare AIC and BIC between models, and between adjusted \\(R^2\\) values if we use caution. The AIC and BIC here are unambiguious - the addition of weights does not significantly increase the information gain within this model. Adjusted \\(R^2\\) values are a little more complicated, because they mean something different in a weighted fit vs. an unweighted fit. Recall that an unweighted adjusted \\(R^2\\) describes the proportion of variance in the dependent variable explained by the model regressing on the independent variable. In a weighted regression model, adjusted \\(R^2\\) describes the proportion of variance in the weighted dependent variable explained by the model regressed on the weighted independent variable. As such, use caution when comparing adjusted \\(R^2\\) between unweighted and weighted models. That being said, in this case the adjusted \\(R^2\\) value agrees with the AIC and BIC that the quadratic model creates the best fit on the data. With this in mind, we would likely choose not to include weights in this particular calibration curve. However, if the results of these goodness-of-fit measurements were more ambiguous and the heteroscedasticity in the unweighted regression residuals were more obvious, you might have to use your judgement when deciding whether to weight a fit. It is often okay to add a weighting scheme to a fit even if the goodness-of-fit measurements do not support it if you believe it improves the predictive power of your calibration curve. Generally speaking, if a weighting scheme is unnecessary and you still include one, it will not significantly influence your concentrations calculated with a calibration curve, but if you need a weighting scheme and do not include one, low concentrations calculated with your calibration curve may be unreliable. Hence, many researchers choose to always include weighting schemes, even if it is not always strictly necessary. 20.5.3 Do I Need an Intercept? You might wonder, particularly if your instrument has very low background noise, whether it is worth including an intercept within your calibration curve. It is possible to create linear and polynomial models without an intercept in R - simply add -1 to the end of the function definition within the lm function. However, you must do so with extreme caution. It is almost always better to include an intercept in a calibration curve than to remove it. If you really want to try a model without an intercept, there are three things you need to check: 1. Do your blanks support the fact that your instrument response is zero when the concentration of your analyte is zero? 2. When you generate your model with the intercept, is the intercept statistically indistinguishable from zero? 3. When you generate your model without the intercept, does it improve the model goodness-of-fit over not having the intercept? Let’s test these three conditions for our data. We can easily test the first condition just by looking at our data: DT::datatable(FAES) Clearly, the signal is nonzero in our standards even when the concentration of our analyte is 0 ppm. Thus, we fail our first test. This would be enough to force us to include the intercept in our model fits normally, but we will test the other two conditions here for illustrative purposes. Next, let’s see what the p-value of our model intercept being statistically significantly different from zero says: DT::datatable(tidy(quadratic), options = list(scrollX = TRUE)) The p-value of our intercept here is around \\(10^-5\\), which is pretty definitive in indicating that we cannot ignore our intercept. Just to check, let’s create the model without the intercept: #Generate a quadratic model without the intercept by adding &#39;-1 to the equation quadraticnoint &lt;- lm(signal ~ conc_Na + I(conc_Na^2) -1, data = FAES) DT::datatable(tidy(quadraticnoint), options = list(scrollX = TRUE)) Now let’s compare the glanced versions of the model with and without an intercept: #Binding together the glanced outputs of the two models into a dataframe to compare interceptGOF &lt;- rbind(&quot;With intercept&quot; = glance(quadratic), &quot;No intercept&quot; = glance(quadraticnoint)) DT::datatable(interceptGOF, options = list(scrollX = TRUE)) We can see based on all of our model goodness-of-fit parameters that the quadratic model with an intercept is a better fit for our data than without the intercept. Therefore, we must include the intercept in our calibration curve for this dataset. 20.5.4 Model Selection: Conclusions So far this chapter, we have discussed a variety of tools you can use in R to improve and evaluate regression models. To improve a model, you can: Add or remove nonlinear terms to the model to capture additional complexities within the data Weight the model appropriately to minimize issues of heteroscedasticity and magnitude differences between different data points To evaluate a model, you can: Plot residuals of the model fit to look for additional patterns in the data, heteroscedasticity, or outliers Compare goodness-of-fit metrics for different models such as adjusted \\(R^2\\), AIC, BIC or F-statistic and its corresponding P-value. Test whether parameter fits are statistically different from zero to evaluate whether they improve the model Let’s pause for a moment and discuss the practicality of all of this. Using these model evaluation and improvement tools is a lot of work for something as simple as a calibration curve, and you might wonder whether it’s even worth it to go to all of this trouble to choose the correct model. In truth, many professional scientists don’t validate their calibration curves this rigorously, and in many cases you probably won’t need to either. You may just go ahead and try a linear fit, weighted linear fit, and a quadratic fit and see which one gives you the best adjusted \\(R^2\\) value. That being said, it’s worth knowing how to rigorously evaluate calibration curves so that if you ever need to do so, you can. All of the work we’ve done here is also broadly applicable beyond just calibration curves - if you need to create nonlinear models (like we will discuss next chapter) to capture real-world phenomena, these model development and evaluation tools can be far more important. In the end, it is up to you as the researcher to choose how rigorously you need to be when constructing your models, whether it be for a calibration curve or a large, multivariable regression. That’s part of the job of a scientist! With that, let’s move on to a couple of additional practicalities in model development that are specific to calibration curves. If you came to this chapter specifically for model development and evaluation, this is the end of that section of this chapter - feel free to skip to the conclusion. 20.6 Using Calibration Curves to Estimate LOD and LOQ One of the most useful applications of calibration curves (beyond their usage as a calibration curve of course) is calculating the limit of detection (LOD) and limit of quantification (LOQ) of the instrument you are using. The approach we will take is defined by the International Committee on Harmonization (of Technical Requirements for Pharmaceuticals for Human Use). There are two other approaches: one based on visual evaluation and another based on directly calculating signal to noise ratio. However, using calibration curves is broadly applicable, does not require complex evaluation of chromatograms themselves, and is not prone to operator bias like visual evaluation. Calculating LOD and LOQ from calibration curves is pretty easy! All we need is the slope and standard error of the slope of our calibration curve, both of which we already know how to calculate. The exact definitions are: \\[LOD = 3.3\\frac{\\sigma}{S} \\ \\ \\ \\ \\ \\ LOQ = 10\\frac{\\sigma}{S}\\] Where \\(\\sigma\\) is the standard deviation of the response and \\(S\\) is the slope of the calibration curve. \\(\\sigma\\) may be defined as either the standard deviation of the y-intercept of the calibration curve or the residual standard deviation of the calibration curve itself. In R, the standard deviation of the y-intercept is a standard output of the summary or tidy functions on a linear model. The residual standard deviation of the calibration curve can be accessed using the sigma function or in the output of the glance function under sigma. Note that in R, all model errors are called standard errors, however, in R, these standard errors actually refer to the estimated standard deviation of that variable. Hence, when the above equations call for residual standard deviation and standard deviation of the intercept, we use the values that R calls the residual standard error and standard error of the intercept. There are two caveats. First, the calibration curve must be linear and unweighted for this to work, and it should be a relatively good fit. Often, this means that you will have to cut off higher concentrations where instrument response becomes more nonlinear and remake the calibration curve specifically for the LOD/LOQ calculation. It’s okay to only use lower concentrations for this purpose because LOD and LOQ are defined by the lower limits of the instrument. Second, it is always best practice to cross-validate the calculated values of LOD and LOQ, ideally by running standards at the calculated concentrations and visually ensuring that you can see a signal outside of the noise of the instrument. Let’s try calculating LOD and LOQ for our example dataset now. First, we need to select the linear range that we will use to construct our calibration curve. The best way to do this is by looking at the data itself, so once again, let’s plot our original dataset with a linear calibration to see where the datapoints start deviating from linearity: ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=FALSE) At the very least, we need to remove the point at Na concentration = 1 ppm. Let’s remove that and check again: limitData &lt;- FAES %&gt;% #Filter out values with concentrations less than 1 ppm filter(conc_Na &lt; 1) ggplot(data = limitData, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=FALSE) That looks a lot better. We could also remove the point at 0.5 ppm, but even our lowest three concentrations look a little bit nonlinear, and we will get diminishing returns on the goodness of fit of the linear calibration curve as we remove more datapoints, so let’s just work with what we have above for now. Calculating LOD and LOQ now is as simple as creating our linear calibration curve and extracting the slope and residual standard deviation or standard deviation of the y-intercept. Using the broom tidy and glance outputs may be helpful here to get your information in workable form (this would be very helpful if you were generating many calibration curves for many analytes at once and want to calculate LOD and LOQ for all of the analytes). We will generate LOD and LOQ both ways here: #Generate the linear model on our filtered data and its glanced and tidied summaries limitsLM &lt;- lm(signal ~ conc_Na, data = limitData) limitsGOF &lt;- glance(limitsLM) limitsCoef &lt;- tidy(limitsLM) #Calculate the LOD based on the residual standard deviation LODresid &lt;- 3.3*limitsGOF[[3]]/limitsCoef[[2,2]] #Calculate the LOQ based on the residual standard deviation LOQresid &lt;- 10*limitsGOF[[3]]/limitsCoef[[2,2]] paste(&quot;For the FAES dataset, the LOD = &quot;, LODresid, &quot;ppm and the LOQ = &quot;, LOQresid, &quot;ppm when calculated using residual standard deviation.&quot;) ## [1] &quot;For the FAES dataset, the LOD = 0.0488676828127103 ppm and the LOQ = 0.148083887311243 ppm when calculated using residual standard deviation.&quot; #Calculate the LOD based on the y-intercept&#39;s standard deviation LODyint &lt;- 3.3*limitsCoef[[1,3]]/limitsCoef[[2,2]] #Calculate the LOQ based on the y-intercept&#39;s standard deviation LOQyint &lt;- 10*limitsCoef[[1,3]]/limitsCoef[[2,2]] paste(&quot;For the FAES dataset, the LOD = &quot;, LODyint, &quot;ppm and the LOQ = &quot;, LOQyint, &quot;ppm when calculated using y-intercept standard deviation.&quot;) ## [1] &quot;For the FAES dataset, the LOD = 0.0206503650239429 ppm and the LOQ = 0.0625768637089178 ppm when calculated using y-intercept standard deviation.&quot; The first thing you’ll notice is that the LOD and LOQ are quite different depending on which estimate of \\(\\sigma\\) we use! This is where testing and validation come in. Ideally, you would want to run standards near these two limits of detection and see which one is actually detectable on the instrument. You’ll notice that the LOQ using residual standard deviation is somewhat high - higher than our 0.1 ppm calibration point! This is likely because the quality of the calibration curve matters quite a lot when calculating LOD and LOQ. If you have nonlinearity or few calibration points like we have in this case, you could be artificially inflating your LOD and LOQ due to statistical, not instrumental uncertainty. If you were performing these experiments yourself, you might want to run additional standards in the 0 to 0.5 ppm region to improve the calibration curve both for better concentration calculations and to see if you end up with a lower LOD and LOQ, or just use the LOD and LOQ calculated using the standard deviation of the intercept instead if you are confident that those values are reasonable. That being said, it is always possible that you have higher than expected instrument noise, so you cannot just throw out high values of LOD and LOQ if you have a low-quality calibration curve. Instead, you would likely need to cross-validate with different method of calculating LOD and LOQ. The general practice in environmental analytical chemistry is to report values &lt; LOD directly as “&lt;LOD” - i.e. not to report a quantitative value, and values &lt; LOQ and half of the LOQ. Applying the quadratic calibration curve we calculated and evaluated earlier and the LOD and LOQ values we found here (let’s use the higher ones for illustrative purposes), we could properly calculate new concentrations for our sample dataset. First, let’s import the data and tabulate it to remind ourselves what it looks like: FAESsamples &lt;- read_csv(file = &quot;data/FAESUnknowns.csv&quot;) %&gt;% pivot_longer(cols = -c(sample, `dilution factor`), names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) ## Rows: 3 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sample ## dbl (4): dilution factor, reading_1, reading_2, reading_3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. DT::datatable(FAESsamples) First, we calculate a concentration column from our quadratic calibration curve using the quadratic formula. Remember that we can use the coef function to extract coefficients of a fit into a list, and we can use base R bracket notation to extract specific values #Extracting the coefficients of the quadratic fit x2 &lt;- coef(quadratic)[3] x &lt;- coef(quadratic)[2] intercept &lt;- coef(quadratic)[1] #Calculating new concentrations using the quadratic formula (solving the fit equation for x) FAESresults &lt;- FAESsamples %&gt;% mutate(instConc = (-x + sqrt(x^2-4*x2*(intercept-signal)))/(2*x2)) DT::datatable(FAESresults) Then, let’s filter out data that is below our LOD and LOQ as follows: FAESresultsfiltered &lt;- FAESresults %&gt;% #Set instConc to LOQ/2 when its value is &lt; LOQ and to 0 when its value is &lt; LOD mutate(instConc = case_when( instConc &gt; LOQresid ~ instConc, instConc &gt; LODresid ~ LOQresid/2, instConc &lt; LODresid ~ 0)) %&gt;% #Create a note that indicates whether the concentration is &lt; LOQ or &lt; LOD mutate(note = case_when( instConc &gt; LOQresid ~ &quot;N/A&quot;, instConc &gt; LODresid ~ &quot;&lt;LOQ&quot;, instConc &lt; LODresid ~ &quot;&lt;LOD&quot;)) DT::datatable(FAESresultsfiltered) Notice how it has taken the tap water values and set all but one to 1/2 the LOQ - this is because we are not confident in the actual concentrations. This is artificially lowering their values, and since they’re right on the edge of the LOQ in this case, it might be better to report them as-is instead of as 1/2 the LOQ, or to use the lower LOQ value we calculated ealier using the standard deviation of the intercept. If you were reporting these values in a publication, you would flag these values as &lt; LOQ for clarity. As before, before reporting these values you would want to correct for the dilution factor and summarize them. 20.7 Inverse Calibration Curves So far, we have exclusively been constructing conventional calibration curves - that is, the instrument response has been the dependent variable and the known concentrations have been the independent variable. It is also possible to construct a calibration curve by inverting these two variables - i.e. you use the concentration as the dependent variable and the instrument response as the independent variable. There are two main benefits to doing it this way: First, the least squares fitting algorithm is designed to minimize error along the dependent variable axis. If we are using a calibration curve to predict new concentrations then, an inverse calibration curve which uses concentration as the dependent variable should be minimizing error in that concentration better than a conventional calibration curve. Second, the equation for an inverse calibration curve can be used directly when calculating new concentrations. This is particularly important when using nonlinear calibration curves, as it means you don’t need to solve for roots of a polynomial - you just plug in your instrument response as \\(x\\). With this in mind, you might wonder why conventional calibration curves are, well, conventional. There are two main reasons. First, conventional calibration curves can be used to define and calculate LOD and LOQ as discussed above. You cannot use inverse calibration curves to calculate LOD and LOQ. Because of this fact, conventional calibration curves will always have a place in analytical chemistry. Second, when performing a least squares fit, one of the underlying assumptions is that there is no uncertainty in the independent variable, i.e. for a calibration curve the concentration of each standard is known exactly. Historically, this was generally true - analytical instrumentation tended to have larger errors than measurements made using analytical balances and glassware. In modern chemistry labs, this is not strictly true - modern instrumentation is sometimes now more precise than our solution preparation can be, and so this second factor becomes less important. Now that you know why inverse calibration curves are convenient and why we might choose to use them, let’s generate an inverse quadratic calibration curve from our dataset and use it to calculate new concentrations directly, instead of having to use the quadratic equation. Notice how all we do is switch the variables in the lm function. We’ll also add the values from our conventional calibration curve and the difference between them to compare: #Generate a model using conc_NA as the dependent variable and signal as the independent variable inverseCal &lt;- lm(conc_Na ~ signal + I(signal^2), data = FAES) #Extracting the coefficients using the `coef` function x2 &lt;- coef(inverseCal)[3] x &lt;- coef(inverseCal)[2] intercept &lt;- coef(inverseCal)[1] #Calculating new concentrations using the base fit equation inverseFAESResults &lt;- FAESsamples %&gt;% mutate(instConc = x2*signal^2 + x*signal + intercept, #Then adding a new column with the conventional calibration curve results instConcConventional = select(FAESresults, instConc)[[1]], #And a column with the difference between the two instConcDiff = instConc - instConcConventional) DT::datatable(inverseFAESResults, options = list(scrollX = TRUE)) You’ll notice that the new values of instConc are slightly different from those in the previous section. Generally speaking, inverse calibration curves will give slightly different results from a conventional calibration curve, but the differences are around 0.01 ppm, well within the uncertainty of the two models. ##Summary In this chapter we extended the concepts we learned in Chapter 19 beyond simple linear regressions, discussing a variety of topics related to improving and evaluating your models, including: - How to evaluate models using residuals to identify patterns in your data that your model might not be capturing. - Two new ways to improve models: nonlinear regression and weighted regression. - How to quantitatively decide which model provides the best fit for a given dataset. - Some practical methods related to using and interpreting regression models in the context of calibration curves. As mentioned earlier in the chapter, all of the model evaluation, improvement, and selection we have discussed is broadly applicable to regressions beyond calibration curves. There are many reasons why an environmental scientist may decide to create a regression model between two (or more) variables, and it is important to know how to rigorously evaluate those regressions, especially when they are being used to extract fundamental information about the system being studied. Keep this in mind as you move to the next chapter, Modelling: Non-Linear Regression. 20.8 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. "],["modelling-non-linear-regression.html", "Chapter 21 Modelling: Non-Linear Regression 21.1 Experimental Background 21.2 Modelling Sigmoid Curve 21.3 Summary 21.4 Exercise", " Chapter 21 Modelling: Non-Linear Regression We’ve touched upon the basics of modelling in R but it doesn’t have to stop there. This chapter will expand upon the contents of Modelling: Linear Regression to cover non-linear regressions. Since we can’t account for the myriad of models utilized throughout the field, we’ll work through a case-study. 21.1 Experimental Background For this chapter we’ll be using data obtained from an experiment in CHM317. In this experiment, students measure the fluorescence of the fluorescent dye acridine orange in the presence of sodium dodecyl sulfate (SDS). In, or near, the critical micelle concentration of SDS, there is a sharp change in absorbance and fluorescence of the solution. Tracking these changes in fluorescence, students can then estimate the CMC of SDS. Experimentally, students prepared solutions of a constant concentration of acridine orange and varying concentrations of SDS. The emission spectrum of each sample was recorded, and we want to take the maximal of each spectra as a data point to build our model. Let’s go ahead and import our data: library(tidyverse) sds &lt;- read_csv(&quot;data/CHM317/fluoro_SDSCMC.csv&quot;) %&gt;% pivot_longer(cols = !`Wavelength (nm)`, # select all columns BESIDES `Wavelength (nm)` names_to = c(&quot;conc&quot;, &quot;conc.units&quot;, &quot;chemical&quot;), names_pattern = &quot;(.*) (.) (.*)&quot;, values_to = &quot;intensity&quot;, names_transform = list(conc = as.numeric) ) %&gt;% rename(wavelength = &#39;Wavelength (nm)&#39;) # renaming column, less typing later on. head(sds) ## # A tibble: 6 × 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 500 0.001 M SDS 20.0 ## 2 500 0.0016 M SDS 18.6 ## 3 500 0.004 M SDS 7.02 ## 4 500 0.0048 M SDS 1.12 ## 5 500 0.0056 M SDS 5.48 ## 6 500 0.0064 M SDS 7.72 And a quick plot to visualize our data: ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() Alright, alright, alright. Things are looking like we’d expect with some well behaved data. By plotting each point individually, we can really see the noise inherent with each reading. For a more robust analysis we’d typically conduct several replicates and average out the spectra for each concentration or apply some kind of model to smooth each peak. But today, we’re just interested in getting the maximal fluorescence emission intensity from each reading. Let’s first annotate our plate to find the highest point, then go about extracting our data for analysis. 21.1.1 Annotating maximal values Annotating the maximal point on the plot will take a bit more code than actually obtaining it from the data. For this we’ll need to use the ggpmisc package which contains miscellaneous extensions for ggplot2, and ggrepel so our labels won’t overlap. library(ggpmisc) library(ggrepel) ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() + ggpmisc::stat_peaks(span = NULL, geom = &quot;text_repel&quot;, # From ggrepel mapping = aes(label = paste(..y.label.., ..x.label..)), x.label.fmt = &quot;at %.0f nm&quot;, y.label.fmt = &quot;Max intensity = %.0f&quot;, segment.colour = &quot;black&quot;, arrow = grid::arrow(length = unit(0.1, &quot;inches&quot;)), nudge_x = 60, nudge_y = 200) + facet_grid(rows = vars(conc)) By faceting the plot (i.e. arranging many smaller plots vs. one large one), we can easily see the increase in emission peak intensity as the concentration of SDS increases. Likewise, we can avoid the messy overlap of the max intensity annotations. This is only one way to plot this data, but this is sufficient because we’re simply inspecting our data at this point. And here we can see that the intensity all occur around a similar wavelength (~ 528 nm) 21.1.2 Extracting maximal values The plots we made above are great for inspecting our data, but what we really want is the maximal emission intensity value to calculate the CMC of SDS. We can see the maximal values on the plots, but there’s no way we’re typing those in manually. So let’s go ahead and get out maximal values from our dataset: sdsMax &lt;- sds %&gt;% group_by(chemical, conc.units, conc) %&gt;% filter(intensity == max(intensity)) %&gt;% ungroup() head(sdsMax) ## # A tibble: 6 × 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 520 0.0056 M SDS 28.1 ## 2 524 0.0072 M SDS 116. ## 3 527 0.0064 M SDS 65.3 ## 4 527 0.008 M SDS 768. ## 5 528. 0.012 M SDS 810. ## 6 528 0.0048 M SDS 22.0 All we did was tell R to take the row with the highest emission intensity value per group. We specified chemical, conc.units, and conc, in case we had more chemicals in our dataset. Our maximum values match those we see in our plot above. Let’s see how they stack up against each other: ggplot(data = sdsMax, aes(x = conc, y = intensity)) + geom_point() Figure 21.1: Plot of maximal fluoresence intensity at various concentrations of SDS. 21.2 Modelling Sigmoid Curve So we want to find the critical micelle concentration of SDS using the maximum fluorescence emission. The CMC is at the ‘midpoint of the sinusoid curve’. Which means we’ll need to a) plot a sinusoid curve and b) extract the midpoint. The ‘sinusoid’ or ‘S-shaped’ curve mentioned in the lab manual is known as a logistic regression. Logistic regressions are often used to model systems with a largely binary outcome. In other words, the system starts at point A, and remains there for awhile, before ‘quickly’ jumping up (or down) to level B and remain there for the remainder. Examples include saturation and dose response curves. For our CMC working data, the fluorescence intensity is low when the \\([SDS] &lt; CMC\\), as micelles are not able to form. However once \\([SDS] &gt; CMC\\), micelles form and the fluorescence intensity increases. We can see this trend in 21.1. There are different forms of logistic regression equations. The simplest form is the 1 parameter, or sigmoid, function which looks like \\(f(x) = \\frac{1}{1+e^{-x}}\\). The outputs for this function are between 0 and 1. We could apply this formula to our model if we somehow normalized our fluorescence intensity accordingly. An alternative is to use the four parameter logistic regression, which looks like: \\[f(x) = \\frac{a - d}{\\left[ 1 + \\left( \\frac{x}{c} \\right)^b \\right ]} + d\\] where: a = the theoretical response when \\(x = 0\\) b = the slope factor c = the mid-range concentration (inflection point) This is commonly referred to as the EC50 or LC50 in toxicology/pharmacology. d = the theoretical response when \\(x = \\infty\\) Why do we need such a complicated formula for our model? Well, looking again at 21.1 we see that the lower point is approximately 20, and not zero. Likewise, the upper limit appears to be around 825. The slope factor is necessary because the transition from the low to high steady state occurs over a small, but not immeasurable, concentration range. And lastly, by including the inflection point, we can calculate exactly for this value using R to get the CMC estimate. 21.2.1 Calculating Logistic Regression A strength of R is its flexibility in running various models, and logistic regression is no different. We can use a number of packages to reach these ends, specifically the drc package contains a plethora of functions for modelling dose response curves (hence drc). However, for this example we’ll use a more generalized approach. Earlier we talked about linear regression, where we adjust the slope and intercept of a linear equation to best fit our data (see Calibration Curves). Recall that this optimization is based on minimizing the distance between the model and all of the experimental points (least squares). Well the stats package has a function called nls that expands upon this to nonlinear models. Per the nls function description: “[nls] determine[s] the nonlinear (weighted) least_squared estimates of the parameters of a nonlinear model.” So we can create a formula in R based on the four-parameter logistic regression described above. After that, we’ll need to produce some starting details from which the model can build off of. If we don’t tell nls where to start, it can’t function, as the search space is too large. Looking at 21.1, the intensity appears to floor around 20; the intensity appears to max out around 820; the midpoint appears to be around 0.0075 M, and let’s say the slope is 1. Remember, these are starting values from which nls starts to optimize from, and not the actual values used to construct the model. So, let’s create our model logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 1, # slope c = 0.0075, # CMC d= 820) # max intensity ) ## Error in numericDeriv(form[[3L]], names(ind), env, central = nDcentral): Missing value or an infinity produced when evaluating the model … and we get an error message. Get used to these when modelling! Don’t worry about understanding it completely, error messages are often written with programmers in mind so they can be a bit cryptic. You can often copy and paste these directly into any search engine to get some more information, but this one is simple enough: we either have a missing value or an infinity produced. Well we have six input parameters in our model: a, b, c, d, our independent variable conc, and our dependent variable intensity. We’ve also supplied starting values to all of them via the list we created inside the function. Therefore, one of our starting values must be too far off from a plausible start point and is causing troubles in the nls function. They all look good except for the slope start value b = 1. The slope here is an approximation for the slope between the min value a and max value d. Looking at our data in 21.1, that slope may be a bit shallow considering the large jump in intensity. Let’s increase the value of b and try again: logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 10, # new slope c = 0.0075, # CMC d= 820) # max intensity ) Eh, no errors! Once you progress beyond simple linear regressions, modelling becomes more of a craft. If we were trying to apply this model to multiple datasets, we would probably want to shop around cran to find a package with self-starting models. This way we can circumvent having to supply starting parameters. Anyways, that’s for another day. For now, let’s take a look at our model outputs which are all stored in the logisModel variable. To this end, we’ll use the broom package discussed in Modelling: Linear Regression. Specifically, we’ll use tidy to get an output of our estimated model parameters (i.e. a,b,c, and d), and augment for a data frame of containing the input values, and the estimated intensity values. Let’s look at our fitted values: library(broom) augment &lt;- augment(logisModel) augment ## # A tibble: 9 × 4 ## intensity conc .fitted .resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28.1 0.0056 49.4 -21.3 ## 2 116. 0.0072 116. -0.123 ## 3 65.3 0.0064 49.7 15.6 ## 4 768. 0.008 768. 0.0977 ## 5 810. 0.012 810. -0.0861 ## 6 22.0 0.0048 49.4 -27.5 ## 7 93.0 0.001 49.4 43.5 ## 8 31.7 0.004 49.4 -17.7 ## 9 57.0 0.0016 49.4 7.51 What we can see here from augment are the intensity and conc values we inputted into R. .fitted are the intensity values for a given concentration fitted to out model, and .resid is the residuals, the difference between the actual and estimated values. Let’s go ahead and plot our actual and fitted values against each other. ggplot(augment, aes(x = conc, y = intensity, colour = &quot;actual&quot;)) + geom_point() + geom_line(aes(y = .fitted)) + geom_point(aes(y = .fitted, colour = &quot;fitted&quot;)) Looks pretty good, although it’s interesting how the baseline at lower concentrations doesn’t plateau like the model values. You’ll note that the line produced by geom_line will only draw a straight line between points. There are ways to address this, but we don’t need to for our needs right now. There doesn’t appear to be any gross outliers in our model, so it seems to have done a good job. We can verify this by checking the residuals(see [Plotting residuals]): ggplot(augment, aes(x = conc, y = .resid)) + geom_point() We can’t see any obvious patterns in the residuals (i.e. all are negative), so we can have further confidence in the fit of our model. 21.2.2 Extracting model parameters To extract the model parameters a, b, c, and d we can use the tidy function: library(broom) tidy &lt;- tidy(logisModel) tidy ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 49.4 11.1 4.44 0.00678 ## 2 b 49.0 9.65 5.07 0.00385 ## 3 c 0.00755 0.0000785 96.2 0.00000000230 ## 4 d 810. 27.3 29.7 0.000000808 Looking past the scientific notation, our model values are pretty similar to what we estimated. Specifically, c, our midpoint value is 0.0076 M. Not too bad from our original estimate. And recall that the midpoint of our curve corresponds to the critical micelle concentration of SDS, which we’ve estimated to be 0.0076M. Not too far from the literature value of 0.0081 M. 21.3 Summary In this chapter we reviewed non-linear modelling using a case study with four-parameter logistic regression. While the equation covered here might not be the one you need, the steps are identical: Tidy and visually inspect your data to see and patterns Determine which mathematical model you’ll be using Use the nls or other suitable package to calculate your model; you may need to tinker around with the starting values, estimate them from your data. Verify your model outputs (both fitted and residuals). Lastly, we’ve also touched upon labelling maximal values in a plot using the ggpmisc package. Notably useful for determining local peaks in spectroscopy data. 21.4 Exercise There is a set of exercises available for this chapter! Not sure how to access and work on the exercise Rmd files? Refer to Running Tests for Your Exercises for step-by-step instructions on accessing the exercises and working within the UofT JupyterHub’s RStudio environment. Alternatively, if you’d like to simply access the individual files, you can download them directly from this repository. Always remember to save your progress regularly and consult the textbook’s guidelines for submitting your completed exercises. Wickham, H. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer-Verlag. Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

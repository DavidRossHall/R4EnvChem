[["index.html", "R for Environmental Chemists R4EnvChem", " R for Environmental Chemists David Hall, Steven Kutarna, and Jessica Deon 2021-04-05 R4EnvChem Howdy, This website is more-or-less the living results of a collaborative project between the three of us. Our ultimate goal is not to be an exhaustive resource for all environmental chemist. Rather, were focused on developing broadly applicable data science course content (tutorials and recipes) based in R for undergraduate environmental chemistry courses. Note that none of this has been reviewed yet and is not implemented in any capacity in any curriculum. Check out the index for whats already live, and the list below for some of our proposals. Setup and workflows for working with R and R Studio Data tidying Best practices when recording data Tidying data either in R or Excel for downstream analysis in R Recipes for tidying common datasets (i.e. ECCC NAPS hourly measurements) Data Visualization Map, cartograms, and other related stuff Volcano plots Bar charts Data analysis Calibration curves and quantitative analysis Various statistical comparisons (t.test, ANOVA, ) Correlation plots Advice on beautifying visualizations for publication ## Questions: If you have any questions/comments/suggestions/concerns please email: Dave at davidross.hall@mail.utoronto.ca Steven at steven.kutarna@mail.utoronto.ca Dr. Deon at jessica.deon@utoronto.ca "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction more to come "],["software.html", "Chapter 2 Software 2.1 Microsoft Excel 2.2 R", " Chapter 2 Software 2.1 Microsoft Excel The demonstrations and instructions in this document are based on Microsoft Excel for Microsoft 365. Current U of T students with a UTmail+ account (i.e. you) can access the full desktop and mobile versions. For assistance downloading the software, consult the UofT libraries webpage. Microsoft Excel is available for both Windows and Mac operating systems. If you are using an different OS (i.e. Linux), you can still use Excel, and other Microsoft Office web programs, from the web. More details can be found here. Note youll still need to login using your UtorID. 2.2 R R is a computer language used for data analysis and visualization. It is extremely popular in academia, and is one of the most popular language for data sciences. While it might seem overkill to learn R to tackle your undergraduate work, it is well worth the time investment especially as your progress to more complicated projects. To know R is to constantly find uses for it. R is most easily written in the R Studio IDE. If you have not done so, download R from the comprehensive R archive network (CRAN), link here: http://cran.utstat.utoronto.ca/, and R Studio, link here: https://rstudio.com/products/rstudio/download/). Follow the listed instructions and you should be well on your way. You can also see the accompanying Working with RStudio document on Quercus for additional top tips. 2.2.1 R Markdown In a nutshell, R Markdown allows you to analyse your data with R and write your report in the same place (this document is written with R Markdown). This has loads of benefits including increased reproducibility, and streamlined thinking. No more flipping back and forth between coding and writing to figure out whats going on. Lets run some simple code as an example: # Look at me go mom x &lt;- 2+2 x ## [1] 4 What weve done here is write a snippet of R code, ran it, and printed the results (as they would appear in the console). While the above code isnt anything special, we can extend this concept so that our R markdown document contains any data, figures or plots we generate throughout our analysis in R. Pretty neat, eh? You might not think so, but lets imagine a scenario youll encounter soon enough. Youre about to submit your assignment, youve spent hours analyzing your data and beautifying your plots. Everything is good to go until you notice at the last minute you were supposed to subtract value x and not value y in your analysis. If you did all your work in Excel (tsk tsk), youll need to find the correct worksheet, apply the changes, reformat your plots, and import them into word (assuming everything is going well, which is never does with looming deadlines). Now if you did all your work in R markdown, you go to your one .rmd document, briefly apply the changes and re-compile your document. To further illustrate the versatility of R Markdown, this entire book was written with it! See the Appendix for more information. "],["importing.html", "Chapter 3 Importing", " Chapter 3 Importing This entire section will be revamped to talk about importing data in R, and not Excel. DH. "],["exploring-spectroscopy-data.html", "Chapter 4 Exploring Spectroscopy Data 4.1 Setting up data for efficient plotting 4.2 Plotting data 4.3 Creating interactive plots", " Chapter 4 Exploring Spectroscopy Data Youre bound to encounter some form of spectroscopy data during your chemistry career. Most of the instruments youll use to acquire spectroscopy data will also have software that allows you to explore and analyze your recorded spectra. However, this software is often proprietary, and as a student its extremely difficult to get your hands on a working copy, let alone a working copy for each instrument youll use. Fortunately, you can easily export your spectra as a .csv file containing all the data youll need to reproduce your spectra at home. This chapter outlines how you can use R to create easily create interactive plots, and how you can use these to plot your spectra data in an interactive format. 4.1 Setting up data for efficient plotting Alright, lets import an example ATR FT-IR dataset. Different programs will typically have their own export layout, but you can tidy this up in Excel during the lab. The example here is from an experiment in CHM 317 where students use ATR FT-IR to investigate the polymer compositions of consumer products against known plastics. library(tidyverse) spectrum &lt;- read_csv(&quot;./data/CHM317/ATR_plastics.csv&quot;) head(spectrum, n = 10) ## # A tibble: 10 x 11 ## wavenumber EPDM Neoprene Mylar PTFE PVC Polystyrene Polyethylene ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.296 0.0709 0.0417 0.0174 0.0746 0.000873 ## 2 551. 0.212 0.295 0.0709 0.0421 0.0174 0.0746 0.000834 ## 3 551. 0.213 0.295 0.0708 0.0424 0.0175 0.0745 0.000819 ## 4 552. 0.213 0.294 0.0707 0.0429 0.0175 0.0745 0.000825 ## 5 552. 0.214 0.294 0.0707 0.0436 0.0176 0.0745 0.000868 ## 6 553. 0.214 0.294 0.0706 0.0443 0.0177 0.0746 0.000949 ## 7 553. 0.215 0.293 0.0706 0.0453 0.0177 0.0746 0.00101 ## 8 553. 0.215 0.292 0.0705 0.0455 0.0178 0.0746 0.00103 ## 9 554. 0.216 0.292 0.0704 0.0453 0.0179 0.0746 0.00105 ## 10 554. 0.216 0.291 0.0703 0.0443 0.0179 0.0745 0.00107 ## # ... with 3 more variables: `Sample: eyeglass bag` &lt;dbl&gt;, `Sample: Gloves ## # (KC500)` &lt;dbl&gt;, `Sample: Shopping bag` &lt;dbl&gt; Notice how the data is organized here. Theres a column for wavelength, and then a column for the absorance readings for each plastic sample. Note that since the experiment uses the same method for each sample, the wavenumber steps are identical between runs, hence the lone wavenumber column. While this setup, in the wide format, is handy when recording data and organizing your spreadsheet, its not very efficient in R. So were going to transform it into a long format. spectrum &lt;- spectrum %&gt;% pivot_longer(cols = !&#39;wavenumber&#39;, names_to = &quot;sample&quot;, values_to = &quot;absorbance&quot;) head(spectrum, n = 10) ## # A tibble: 10 x 3 ## wavenumber sample absorbance ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 550. EPDM 0.212 ## 2 550. Neoprene 0.296 ## 3 550. Mylar 0.0709 ## 4 550. PTFE 0.0417 ## 5 550. PVC 0.0174 ## 6 550. Polystyrene 0.0746 ## 7 550. Polyethylene 0.000873 ## 8 550. Sample: eyeglass bag 0.0201 ## 9 550. Sample: Gloves (KC500) 0.0451 ## 10 550. Sample: Shopping bag 0.0238 In a long format, each column is a variable, and each row is an observation. So in this format you can read across and note that at 550.0952 cm^-1, EPDM had an absorbance of 0.212. This format makes working with the tidyverse family of functions much easier and intuitive once you understand the layout. 4.2 Plotting data Because our data is tidied (i.e. setup properly) we can easily plot everything all at once: fig &lt;- ggplot(spectrum, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() fig So here we see that each sample/plastic has its own spectra coloured, and we can easily compare them all to each other. However this plot is a bit ugly. Ugly plots have their place, most notably when youre just exploring your data and seeing what sticks to the wall. Now to present your plot in a lab report youll need to clean it up a bit. Lets give that a go: fig &lt;- ggplot(spectrum, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_path() + labs(title = &quot;ATR FT-IR spectra of various plastics&quot;, subtitle = &quot;Spectra recorded with a Thermo Scientific iS50&quot;, caption = &quot;(data from CHM 317 classe of 2019.)&quot;) + xlab(&quot; Wavenumber(cm^-1)&quot;) + ylab(&quot;Absorbance&quot;) + theme_classic() fig Note the use of geom_path() in our new plot. This will connect the individual points in a plot creating a smooth line. 4.3 Creating interactive plots Alright, so far what we showed isnt any different then what you could do in Excel or similar programs. An advantage of R is you can use packages such as plotly to easily create interactive graphs. Using interactive graphs when analyzing spectroscopy data is very powerful as it affords you the tools to easily zoom in and investigate small peaks, craw along the spectra and see the evolution of your samples absorbances, and to readily compare samples to each other. All that being said, lets load the plotly package and transform our above plot into an interactive plot. library(plotly) plotlyFig &lt;- ggplotly(fig) plotlyFig 4.3.1 Some notes about working with Plotly You dont have to worry too much about whats going on under the hood with Plotly, but you should be aware of the following: Interactive plotly plots can only work in an .html format. Obviously if you print them out as a PDF youll loose the interactive element. If you notice something neat when you zoom in, you can use the snapshot button to take a picture for your report. "],["calibration-curves-with-qqq-data.html", "Chapter 5 Calibration Curves with QQQ Data 5.1 Importing and tidying data 5.2 Normalizing QQQ Data 5.3 Calculating Calibration Curves 5.4 Quantifying sample concentrations", " Chapter 5 Calibration Curves with QQQ Data Note I didnt actually take the CHM410 course, so Jess will need to review this part. As well, i expect this entire chapter will get chopped up in later drafts of the book. As well, will review modelling stuff to find an easier pay. Probably with purrr and what not. -DH Sample prep is only half the fun when it comes to environmental chemistry. Eventually youll want to quantify whats in your samples and to do that youll need to construct calibration curves. This write up will use previously acquired triple-quadrupole LC-MS (hence QQQ) results from the 2019 CHM410 Field trip. This fieldtrip data is comprised of three datasets: lab4_biota.csv with the sampling information for biological samples, liab4_sediment.csv for sediment sampling information, and lab4_qqq.csv with the integrated peak areas of every analyzed sample and calibration standard. For this section youll only need lab4_qqq.csv as well only be calculating the concentration of the samples we injected in the instrument, and not back calculating the concentration in our original samples. Note Most of what well need is contained in the tidyverse family of packages, but you will also need the broom and ggrepel packages to make your lives easier. 5.1 Importing and tidying data First off lets import the peak area information for all of our analytes. Note I cleaned up the data in Excel resulting in the lab4_qqq.csv file used herein. This is mostly because of issues with merged cells and multiple sheets in the original dataset. I also took the opportunity to generated unique sampleID values for each sample by combining the sample name and the group letter. library(tidyverse) # for dplyr, readr, stringr, and ggplot QQQ &lt;- read_csv(&quot;data/CHM410/lab4_qqq.csv&quot;) head(QQQ) ## # A tibble: 6 x 29 ## group type sampleID PFHxA_peakArea PFHxA_RT `13C2PFHxA_peak~ `13C2PFHxA_RT` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 calC~ std 20MC 0 ~ 0 1.97 160000 3.65 ## 2 calC~ std 20MC 0.~ 28400 3.62 174000 3.62 ## 3 calC~ std 20MC 0.~ 45100 3.62 169000 3.61 ## 4 calC~ std 20MC 1 ~ 90900 3.61 168000 3.61 ## 5 calC~ std 20MC 2 ~ 184000 3.6 174000 3.6 ## 6 calC~ std 20MC 4 ~ 384000 3.61 141000 3.6 ## # ... with 22 more variables: PFHpA_peakArea &lt;dbl&gt;, PFHpA_RT &lt;dbl&gt;, ## # PFOA_peakArea &lt;dbl&gt;, PFOA_RT &lt;dbl&gt;, `13C4PFOA_peakArea` &lt;dbl&gt;, ## # `13C4PFOA_RT` &lt;dbl&gt;, PFNA_peakArea &lt;dbl&gt;, PFNA_RT &lt;dbl&gt;, ## # `13C5PFNA_peakArea` &lt;dbl&gt;, `13C5PFNA_RT` &lt;dbl&gt;, PFDA_peakArea &lt;dbl&gt;, ## # PFDA_RT &lt;dbl&gt;, `13C2PFDA_peakArea` &lt;dbl&gt;, `13C2PFDA_RT` &lt;dbl&gt;, ## # PFHxS_peakArea &lt;dbl&gt;, PFHxS_RT &lt;dbl&gt;, `13C4PFHxS_peakArea` &lt;dbl&gt;, ## # `13C4PFHxS_RT` &lt;dbl&gt;, PFOS_peakArea &lt;dbl&gt;, PFOS_RT &lt;dbl&gt;, ## # `13C4PFOS_peakArea` &lt;dbl&gt;, `13C4PFOS_RT` &lt;dbl&gt; Note how our data is in a wide format, with columns for the peak area and retention times for each targeted ion. This served the TA well when they wrote down the data from the LC-MS analysis, but lets tidy it up so its easier to work with in R. ### Find cleaner way of doing this... QQQ &lt;- QQQ %&gt;% pivot_longer(cols = -c(&quot;group&quot;, &quot;type&quot;, &quot;sampleID&quot;), names_to = c(&quot;cmpd&quot;, &quot;measurement&quot;), names_sep = &quot;_&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = measurement, values_from = value) head(QQQ) ## # A tibble: 6 x 6 ## group type sampleID cmpd peakArea RT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 calCurve std 20MC 0 ppb PFHxA 0 1.97 ## 2 calCurve std 20MC 0 ppb 13C2PFHxA 160000 3.65 ## 3 calCurve std 20MC 0 ppb PFHpA 0 2.26 ## 4 calCurve std 20MC 0 ppb PFOA 0 2.65 ## 5 calCurve std 20MC 0 ppb 13C4PFOA 244000 4 ## 6 calCurve std 20MC 0 ppb PFNA 0 3.29 Much better, each column is a variable, and every row an observation. However, since the Lake Niamco samples were analysed at a different time then the 20 Mile Creek samples, lets quickly annotate our data to differentiate the two. Since we used (overly) descriptive sample names (stored in the sampleID column), we can create a new column to specify the location by searching for matching string values. QQQ &lt;- QQQ %&gt;% mutate(location = case_when( str_detect(sampleID, regex(&quot;20MC&quot;, ignore_case=TRUE)) ~ &quot;20MC&quot;, str_detect(sampleID, regex(&quot;NIA&quot;, ignore_case=TRUE)) ~ &quot;NIA&quot;, TRUE ~ &quot;NA&quot;)) The code above will search through every row in the sampleID column. If it finds the string of characters 20MC, which we used to denote samples from 20 Mile Creek, it will record this in a new column called location. Same with NIA. If neither 20MC or NIA are detected, it returns NA. Note, if we had more complex sampleID names, we could expand our case_when arguments accordingly. Lets make a quick plot to verify everything is looking alright: library(RColorBrewer) # because i&#39;m colour blind... ggplot(QQQ, aes(x = RT, y = peakArea, colour = group)) + geom_point() + facet_grid(cols = vars(cmpd), rows = vars(location)) + theme(legend.position = &quot;bottom&quot;) + scale_color_brewer(palette = &quot;Paired&quot;) Alright, busy plot, but lets see what we got. First off, this is a small multiple, basically a grid of small, individual, plots that share a common axis. So each small plot is our retention time (RT) on the x-axis vs. integrated peak area (peakArea) on the y-axis. Now our small multiple is organized in a grid, with the columns of the grid corresponding to the ions we analyzed, and the rows of the grid being the location grouping. So, the top-right plot shows the peak area vs. retention time of PFOS from the 20 Mile Creek samples. Lastly, the colour of a point corresponds to the group to which that value belongs. So we see multiple calCurve values at the same RT for a given compound, but with vastly different peak areas. This makes sense, as these are our standards from which well construct our calibration curve later on. Now that we understand what were looking at, lets inspect our data. Here are some things I noted: Some compounds have a RT and peakArea of 0. Now their actual concentration isnt necessarilly 0 ppb, rather the vendor software used to calculate peak areas will return 0 if a given ion wasnt detected. However, R will interpret this number literally, so well need to address this later on. Most compounds elute at approximately 4 minutes. The grouping of retention times makes sense because of the structural similarity of our targeted compounds, and the short chromtagraphy gradient. However some compounds appear to elute earlier. Often these outliers have a low peakArea, so they may be the result of the vendors algorithm integrating noise, and mislabelling it as a legitimate peak. Well need to review this later on. The internal standards all appear to have similar peak areas values. This makes sense, as weve spiked in the same amount of internal standard for each sample. No signal is greater than a calCurve signal, this is good as it means all of our unknowns should fall within our calibration curve; baring matrix effects 5.2 Normalizing QQQ Data Now that were all organized in terms of importing our LC-MS results, lets being the work of actually quantifying the samples we injected into the LC-MS before back calculating and quantifying our actual field samples. First thing, well need to normalize our peak areas to account for matrix effects. To achieve this, well need to pair each analyte of interest with its assigned internal standard. Recall however that we did not have an exact isotopic standard for each compound. The pairing, from the lab manual, is below: Analyte Full name Carboxylic acid? Sulfonic acid? Number of Carbons Number of perfluorinated carbons Internal Standard to use PFHxA Perfluorohexanoic acid x 6 5 13C4 PFHxA PFHpA Perfluoroheptanoic acid x 7 6 13C4 PFOA PFHxS Perfluorohexane sulfonic acid x 6 6 13C4 PFHxS PFOA Perfluorooctanoic acid x 8 7 13C4 PFOA PFNA Perfluorononanoic acid x 9 8 13C5 PFNA PFOS Perfluorooctane sulfonic acid x 8 8 13C4 PFOS PFDA Perfluorodecanoic acid x 10 9 13C2 PFDA So according to the table above, both PFHpA and PFOA use 13C4PFOA as an internal standard. Given 5.2.1 Assessing Internal Stds For fun, lets gauge how much our internal standards varied between samples. After all, theyre all supposed to be the same Well create a new column to annotate which compounds are from our internal standard (i.e. those with \\(^{13}C\\)). For that, well recycle a bit of code from above to search for the 13C string. Then we can compare the peak areas of the internal standards and our analytes of interest. As for pairing, theres a couple of ways we could do this. Lets just create a new column where we remove the 13C. string, so we get the same compounds. Some things to note about the string search: The regex 13C., will look up any string with 13C and one additional wildcard character (noted by the .). This way we can account for the different numbers of 13C in the name, such as 13C4 and 13C2. PFHpA will not have an isotopic pair. Well need to resolve this later by subtracting the 13C4PFOA values from it. # annotating internal and external standards QQQ &lt;- QQQ %&gt;% mutate(stdType = case_when( str_detect(cmpd, regex(&quot;13C&quot;, ignore_case=TRUE)) ~ &quot;internal&quot;, TRUE ~ &quot;external&quot;)) # pairing analytes w/ internal standard QQQ &lt;- QQQ %&gt;% mutate(pair = str_remove(cmpd, regex(&quot;13C.&quot;))) ggplot(data = QQQ, aes(x = stdType, y = peakArea)) + geom_violin() + geom_jitter(aes(colour = type), position=position_jitter(0.2), alpha = 0.75) + scale_y_continuous(trans=&quot;log10&quot;) + facet_grid(cols = vars(pair), rows = vars(location)) + theme(legend.position = &quot;bottom&quot;) + ylab(&quot;log10 Peak Area&quot;) Boy howdy lets break this down. This is small multiple of violin plots that shows some neat trends. For those not in the know, a violin plot is similar to a box plot, but the width of the bar is a function of the density of the data around that point. In other words, the more points at a given value, the wider the plot; weve also plotted the individual points themselves to help convey this. Violin plots help us better visualize groupings of data, and can shed some light if a grouping of data might actually be many smaller groupings. Anyways, lets see what else this says about our data. First, we can see that theres much less variation between internal standards (the internal column) compared to the non-isotopically labelled analytes (theexternal\" column). Makes sense, all the internal standards are supposed to be the same concentration. However, even then there is still at least an order of magnitude variation in peak area for a given internal standard. Some of this is due to matrix effects, and is why we added the internal standards in the first place. However, looking at the internal standard peak areas, it appears that they cluster into two groups, albeit with some overlap. We might have missed this with a boxplot, but the violin plot helps us see it. You might imagine that the clustering is due to the differences between the external calibration curve samples, and real samples, with the latter having lowered internal standard peak areas due to matrix effects. However, looking at the colouring of the dots, we see that while the std internal standard peak areas are generally the highest (no matrix effect), they dont account for all of the upper cluster. So there might be another reason for this. This clustering around two points might be the result of people using two different pipettes to spike in their internal standard. The variation between pipettes could account for the clustering of the internal standard peak areas. Speaking of two groups, again, we can see a large number of the external were not detected, as denoted by their peak area value of 0. These are probably from the samples originating from the clean reference site. Lastly, between internal standards, we can clearly see that sulfonic acids have a weaker instrumental response then carboxylic acid. This is most likely due to difference in ionizability between the aforementioned functional groups. 5.2.2 Normalizing peak areas Moving onward, the entire point of spiking the same internal standard is to use those values to normalize our external measurements. For this well divide the external peak area by the internal peak area for a given sample. Note this is how I was told CHM410 did it, holler it its wrong. # Note removal of PFHpA, because it doesn&#39;t have an isotope pair # Also note RTDif column, which is difference in retention time between internal and external standard QQQNorm &lt;- QQQ %&gt;% filter(pair != &quot;PFHpA&quot;) %&gt;% group_by(sampleID, pair) %&gt;% mutate(normPeakArea = peakArea[stdType == &#39;external&#39;] / peakArea[stdType == &#39;internal&#39;]) %&gt;% mutate(RTdif = RT[stdType == &#39;external&#39;] - RT[stdType == &#39;internal&#39;]) # Normalizing PFHpA seperatly; not removal of 13C4PFOA data at end QQQPFHpA &lt;- QQQ %&gt;% filter(cmpd %in% c(&quot;PFHpA&quot;, &quot;13C4PFOA&quot;)) %&gt;% group_by(sampleID, ) %&gt;% mutate(normPeakArea = peakArea[cmpd == &#39;PFHpA&#39;] / peakArea[cmpd == &#39;13C4PFOA&#39;]) %&gt;% mutate(RTdif = RT[cmpd == &#39;PFHpA&#39;] - RT[cmpd == &#39;13C4PFOA&#39;]) %&gt;% filter(cmpd %in% c(&quot;PFHpA&quot;)) # Rejoining data and dropping internal standard values as they&#39;re no longer needed. QQQNorm &lt;- QQQNorm %&gt;% bind_rows(QQQPFHpA) %&gt;% filter(stdType == &quot;external&quot;) So the above code did double duty. First we normalized the external peak areas by the internal standard peak areas. This has a couple of consequence; If a compounds wasnt detected, it has a peak area of 0. When divided by the internal peak area, the result will be zero. If an internal standard wasnt detected, the external peak area will be divided by 0 resulting in Inf, a value of infinity, because in R, 1/0 = Inf Lets see how our internal and external standards match up by comparing internal and external retention times. ggplot(QQQNorm, aes(x = RT, y = RTdif, colour = cmpd)) + geom_point() + xlab(&quot;External RT (min)&quot;) + ylab(&quot;External RT - Internal RT (min)&quot;) Alright, it appears that most of our compounds are clustering around themselves, and theres little variation among the RTdif axis, meaning there isnt a large difference between the retention times of the internal and external peaks. This means our peak picking algorithm chose peaks at the correct retention time. Of course, the clustering around RT = 0 is from compounds that werent detected, but whose internal standards were; this is fine. However, there appears to be some outliers somewhere between these two clusters. These are probably all from the same sample. Lets annotate our plot to see if this is true. ggplot(QQQNorm, aes(x = RT, y = RTdif, colour = cmpd, label = sampleID)) + geom_point() + xlab(&quot;External RT (min)&quot;) + ylab(&quot;External RT - Internal RT (min)&quot;) + ggrepel::geom_text_repel(aes(label=ifelse(RTdif &gt; 0.25 | (RTdif &lt; -0.25 &amp; RT &gt; 0), as.character(sampleID),&#39;&#39;)),hjust=0,vjust=0) So something strange happened during the acquisition/processing of the 20MC 0 ppb run. Maybe it was the first one of the day, and the instrument was exceptionally noisy, leading to sloppy peak picking by the algorithm. After all, there shouldnt be any signal as the concentration of the external standard for this sample should be 0 ppb. Likewise, we dont see this effect with the NIA calibration curve standards. Something to keep in mind as we move forward. 5.3 Calculating Calibration Curves All this and were only here? Yup, its important to play around with your data because you never know what youll find. Already weve talked about issues with our internal spiking, and one of our external standard solutions. Lets move onwards to calculating our calibration curves. First we need to get the actual concentrations from our standards to make our cal curves. You can make a data frame and match up your concentrations using inner_join, or you can simply extract the concentration value from the calCurve group sampleIDs, as the numerical value is located between two spaces. Just remember to convert using as.numeric so R knows to treat the extracted strings as numerical values and not as characters (i.e. 0.1 and not \"0.1\"). QQQstds &lt;- QQQNorm %&gt;% filter(group %in% c(&quot;calCurve&quot;)) %&gt;% mutate(conc = as.numeric(str_extract(sampleID, &quot;(?&lt;=\\\\s)(.*)(?=\\\\s)&quot;))) Great, now we can simply group our standards by location and compound to compute a linear regression model for each. R has a plethora of built-in modelling functions, but oftentimes the output is less then intuitive. Since well be using the base R lm model to calculate our calibration curves, lets import the broom package, which is useful for cleaning up Rs modelling outputs (hence broom). library(broom) calCurves &lt;- QQQstds %&gt;% group_by(location, cmpd) %&gt;% nest() %&gt;% mutate(fit = map(data, ~lm(normPeakArea ~ conc, data = .x)), tidied = map(fit, tidy), glanced = map(fit, glance) ) Breaking down the above code, we grouped all calibration standards by the compound and location. This way we can get a linear regression for each grouping. Now, withing the mutate function, weve created three columns: fit, tidied and glanced. The first contains the raw output from the linear regression model lm in the form of a list. The linear model are calculated for normPeakArea as a function of conc. This is exceptionally messy, hence why we used the tidy, and glance function from the broom package . map just means were applying the function tidy to the individual output list created by lm and stored in the fit column. Note that the tidy and glanced outputs are tibbles. So we now have a tibble containing values (i.e. location), lists (i.e. fit), and tibbles (tidied). This is known as **nested data*. Were no longer in Kansas anymore Anyways, lets take a look at our model results. The glanced tibble contains a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire mode1 calCurves %&gt;% unnest(glanced) ## # A tibble: 14 x 17 ## # Groups: cmpd, location [14] ## cmpd location data fit tidied r.squared adj.r.squared sigma statistic ## &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PFHxA 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.996 0.995 1.05 1803. ## 2 PFOA 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.944 0.937 1.89 134. ## 3 PFNA 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.998 0.998 0.556 4994. ## 4 PFDA 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.994 0.993 0.946 1305. ## 5 PFHxS 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.999 0.999 0.447 9476. ## 6 PFOS 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.999 0.999 0.376 11307. ## 7 PFHxA NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.997 0.997 0.788 2742. ## 8 PFOA NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.946 0.939 1.94 140. ## 9 PFNA NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.996 0.995 0.843 1953. ## 10 PFDA NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.996 0.996 0.762 2085. ## 11 PFHxS NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.999 0.999 0.719 7035. ## 12 PFOS NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.996 0.996 1.01 2038. ## 13 PFHpA 20MC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.954 0.948 1.35 166. ## 14 PFHpA NIA &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.945 0.938 2.05 137. ## # ... with 8 more variables: p.value &lt;dbl&gt;, df &lt;dbl&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; What you see here is a bit more than what youd get from Excels line-of-best fit output. See the section on Modelling for a better breakdown of what everything means. But for now, we can see that our r.squared of each calibration curve is pretty good, and the p.value indicates each model is significant. the adj.r.squared is the same as r.squared in this situation. This is because r.squared will always increase if we add more exploratory variables to our model; the adj.r.squared accounts for the number of exploratory variables used in the model. However, in our case we only have one exploratory variable, hence theyre the same. But what about the slope and the intercept? After all, thats what we need to calculate the concentration in our unknowns. Lets take a look at the tidied from the tidy function which constructs a tibble that summarizes the models statistical findings. This includes coefficients and p-values for each term in a regression2 # storing because we&#39;ll use it later on. tidied &lt;- calCurves %&gt;% unnest(tidied) tidied ## # A tibble: 28 x 10 ## # Groups: cmpd, location [14] ## cmpd location data fit term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PFHxA 20MC &lt;tib~ &lt;lm&gt; (Int~ 0.729 0.388 1.88 9.66e- 2 ## 2 PFHxA 20MC &lt;tib~ &lt;lm&gt; conc 0.473 0.0111 42.5 1.04e-10 ## 3 PFOA 20MC &lt;tib~ &lt;lm&gt; (Int~ 1.24 0.694 1.78 1.12e- 1 ## 4 PFOA 20MC &lt;tib~ &lt;lm&gt; conc 0.231 0.0199 11.6 2.82e- 6 ## 5 PFNA 20MC &lt;tib~ &lt;lm&gt; (Int~ 0.289 0.205 1.41 1.95e- 1 ## 6 PFNA 20MC &lt;tib~ &lt;lm&gt; conc 0.415 0.00587 70.7 1.79e-12 ## 7 PFDA 20MC &lt;tib~ &lt;lm&gt; (Int~ 0.592 0.348 1.70 1.27e- 1 ## 8 PFDA 20MC &lt;tib~ &lt;lm&gt; conc 0.361 0.00999 36.1 3.78e-10 ## 9 PFHxS 20MC &lt;tib~ &lt;lm&gt; (Int~ 0.298 0.164 1.82 1.07e- 1 ## 10 PFHxS 20MC &lt;tib~ &lt;lm&gt; conc 0.459 0.00472 97.3 1.38e-13 ## # ... with 18 more rows, and 1 more variable: glanced &lt;list&gt; Again, a lot more to unpack compared to Excel. Thats because the lm function in R calculates a generalized linear model. lm performs a linear regression model, which we normally think of as an equation of the form \\(y= mx+b\\). But, regression models can be expanded to account for multiple variables (hence multiple linear regression) of the form \\[y = \\beta _{0} + \\beta _{1} x_{1} + \\beta _{2} x_{2} ... \\beta _{p} x_{p}\\]] where, \\(y\\) = dependent variable \\(x\\) = exploratory variable; theres no limit how many you can input \\(\\Beta _{0}\\) = y-intercept (constant term) \\(\\Beta _{p}\\) = slope coefficient for each explanatory variable In our situation, we only have one input variable for our model (conc), so the above formula collapses down to \\(y = \\beta _{0} + \\beta _{1} x_{1}\\). So looking at our results above, each row corresponds to a model parameter for a given compound and location. For each modelling parameter, were provided an estimate of its numerical value (estimate, the values well use to calculate concentration). The other parameters are useful to understand but not necessary at this point (again, check out the Modelling section). 5.3.1 Plotting regression curves Its always a good idea to visualize our models fit, and its definitely expected when it comes to calibration curves. So lets go ahead and plot ours: ggplot(QQQstds, aes(x = conc, y = normPeakArea, colour = cmpd)) + geom_point() + facet_grid(cols = vars(location)) + geom_smooth(method=&#39;lm&#39;) + ggpmisc::stat_poly_eq(formula = y ~ x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE, size = 3) + theme_classic() Note the grey area around each linear model fitting is the predicted 95% confidence interval for that model. In other words, 95% of our peak areas should fall inside those lines. Also note the difference in instrument response for different compounds. This is why true quantification requires authentic standards. Lets extract what we need and move on: terms &lt;- tibble(cmpd = tidied$cmpd, location = tidied$location, term = tidied$term, estimate = tidied$estimate) %&gt;% pivot_wider(names_from = &quot;term&quot;, values_from = &quot;estimate&quot;) %&gt;% rename(intercept = `(Intercept)`) %&gt;% rename(slope = conc) head(terms) ## # A tibble: 6 x 4 ## cmpd location intercept slope ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PFHxA 20MC 0.729 0.473 ## 2 PFOA 20MC 1.24 0.231 ## 3 PFNA 20MC 0.289 0.415 ## 4 PFDA 20MC 0.592 0.361 ## 5 PFHxS 20MC 0.298 0.459 ## 6 PFOS 20MC 0.316 0.422 5.4 Quantifying sample concentrations Lets pair up each compound with its calibration curve terms, so we can quantify each sample: unknowns &lt;- QQQNorm %&gt;% filter(type != &quot;std&quot;) %&gt;% inner_join(terms, by = c(&quot;cmpd&quot;, &#39;location&#39;)) %&gt;% mutate(conc = (normPeakArea - intercept)/slope) ggplot(unknowns, aes ( x = cmpd, y = conc, colour = group, label = sampleID)) + geom_jitter() + ggrepel::geom_text_repel(aes(label=ifelse(conc &gt; 100 , as.character(sampleID),&#39;&#39;)),hjust=0,vjust=0) Thanks to our annotation with ggrepel we know from which samples our outliers came from. Note Im using outlier here to mean anything outside our calibration curve, and not a statistical outlier, well get to that. Investigating our original data we see that: Neither 13C2PFDA or 13C4PFHxS were detected in Grp D NIA Shrimp #1 (Replicate 2). Further inspection shows that all of the internal standards are extremely low compared to the other samples. This indicates a missloading of the internal standard, hence the out of whack normalized peak area, and subsequent concentration. - Recall in R,1/0 = Inf, so an undetected internal standard is reported as0`, leading to an infinite concentration. This entire sample should be removed from further analysis. Similar situation with Grp E NIA Bluegill Sunfish #2 (Replicate 1), and although all internal standards where detected, their peak areas are close to the instrument cutoff (~1000 counts). Again, similar story with Grp F NIA Pumpkinsee fish #3 (Replicate 1) For these samples, it may be the result of some serious matrix effects, but I doubt it. Lets just saw well remove them, and see whats left: unknowns &lt;- unknowns %&gt;% filter(conc &lt; 120) ggplot(unknowns, aes ( x = cmpd, y = conc, colour = group, label = sampleID)) + geom_jitter() So we can see many samples have slightly negative concentrations. These are the result of the back calculation, and the errors in our model. We can establish an instrumental cutoff (i.e. anything less than X becomes a flat value). It is interesting how the PFOA and PFHxA results are substantially more negative then the others two. This may be due to the 13C4PFOA internal standard, which was also used to calculate the PFHxA concentrations. Anyways, always something to do From the broom package vignette. From the broom package vignette. "],["logistic-regression-modelling.html", "Chapter 6 Logistic Regression Modelling 6.1 Visually inspecting our data 6.2 Extracting maximal values 6.3 Modelling Sigmoidal Curve", " Chapter 6 Logistic Regression Modelling For this tutorial well be using data obtained from an experiment in CHM317. In this experiment, students measure the fluorescence of the fluorescent dye acridine orange in the presence of sodium dodecyl sulfate (SDS). In, or near, the critical micellular region of SDS, there is a sharp change in absorbance and fluorescence of the solution. Tracking these changes in fluorescence, students are to estimate the CMC of SDS. The setting of the fluorometer for this experiment are: Instrumental Settings Instrument Name LS50-B Excitation Wavelength 480 nm Emission Wavelength Range 500 to 650.5 nm Excitation Slit Width 2.5 nm Emission Slit Width 3 nm Scan Speed 250 nm/min Lets go ahead and import our data: library(tidyverse) sdsWide &lt;- read_csv(&quot;data/CHM317/fluoro_SDSCMC.csv&quot;) head(sdsWide) ## # A tibble: 6 x 10 ## `Wavelength (nm~ `0.001 M SDS` `0.0016 M SDS` `0.004 M SDS` `0.0048 M SDS` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 500 20.0 18.6 7.02 1.12 ## 2 500. 27.3 13.9 5.45 5.46 ## 3 501 27.0 12.5 8.13 5.89 ## 4 502. 29.7 12.5 8.17 5.81 ## 5 502 32.6 15.7 4.58 6.69 ## 6 502. 32.8 19.4 5.94 5.33 ## # ... with 5 more variables: `0.0056 M SDS` &lt;dbl&gt;, `0.0064 M SDS` &lt;dbl&gt;, ## # `0.0072 M SDS` &lt;dbl&gt;, `0.008 M SDS` &lt;dbl&gt;, `0.012 M SDS` &lt;dbl&gt; Looking at our data headers we can see the familiar wide format, with a wavelength column corresponding to the emission wavelength and the remainder accounting for the emission intensity at various concentration of SDS. Note that the intensity columns contains two pieces of information: 1) the concentration in moles per liter and 2) the identity of the chemical, SDS in this case. So when we tidy our data well need to split these column headers up so we get a column corresponding to the numerical value of the concentration, and another with the identity of the column. sds &lt;- sdsWide %&gt;% pivot_longer(cols = !`Wavelength (nm)`, # select all columns BESIDES `Wavelength (nm)` names_to = c(&quot;conc&quot;, &quot;conc.units&quot;, &quot;chemical&quot;), names_pattern = &quot;(.*) (.) (.*)&quot;, values_to = &quot;intensity&quot;, names_transform = list(conc = as.numeric) ) %&gt;% rename(wavelength = &#39;Wavelength (nm)&#39;) # renaming column, less typing later on. head(sds) ## # A tibble: 6 x 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 500 0.001 M SDS 20.0 ## 2 500 0.0016 M SDS 18.6 ## 3 500 0.004 M SDS 7.02 ## 4 500 0.00480 M SDS 1.12 ## 5 500 0.0056 M SDS 5.48 ## 6 500 0.0064 M SDS 7.72 The key bit of code here is names_to and names_pattern. The first part creates three new columns, and the second part searches and subsequently breaks up those headers. Recall our original headers looked like this: 0.001 M SDS, where we had the concentration, a space (which is a character!), the concentration units, another space, and finally the chemical. What names_pattern = \"(.*) (.) (.*).is searching for three chunks of characters separated by a space. We specify the chunk of characters in the parentheses. So the first bit, (.*), means look for any character (. in this context is a placeholder for any character) and the chunk of characters can be any length (as denoted by *). So extending this, we see our code looks for three chunks of characters, delimited by a space between them. The first can be any length, the second is 1 character long, and the third can be any length. You could have specified to look for M or SDS explicitly, but if we had different chemicals or units in our dataset these would be lost. Lastly note names_transform. We split up our original headers to populate rows. However our original headers were stored as characters, and when w split them up we created three separate strings of characters, so R will treat out conc values as characters rather than numbers. By using names_transform we tell R to treat conc values as numbers. Oh and we renamed our original Wavelength (nm) column to wavelength using the rename function. Its always a good idea to use the simplest column names you can (and no simpler!). A good practice is to remove any spaces (you can use snake_case or camelCase instead) as well as removing special character such as parentheses. 6.1 Visually inspecting our data Lets make a quick plot of our fluorescence intensity data and see what we have. ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() Alright, alright, alright. Things are looking like wed expect with some well behaved data. By plotting each point individually, we can really see the noise inherent with each reading. For a more robust analysis wed typically conduct several replicates and average out the spectra for each concentration or apply some kind of model to smooth each peak. But today, were just interested in getting the maximal fluorescence emission intensity from each reading. Lets first annotate our plate to find the highest point, then go about extracting our data for analysis. 6.1.1 Annotating maximal values Annotating the maximal point on the plot will take a bit more code then actually obtaining it from the data. For this well need to use the ggpmisc package which contains miscellaneous extensions for ggplot2, and ggrepel so our labels wont overlap. library(ggpmisc) library(ggrepel) ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() + ggpmisc::stat_peaks(span = NULL, geom = &quot;text_repel&quot;, # From ggrepel mapping = aes(label = paste(..y.label.., ..x.label..)), x.label.fmt = &quot;at %.0f nm&quot;, y.label.fmt = &quot;Max intensity = %.0f&quot;, segment.colour = &quot;black&quot;, arrow = grid::arrow(length = unit(0.1, &quot;inches&quot;)), nudge_x = 60, nudge_y = 200) + facet_grid(rows = vars(conc)) By facetting the plot (i.e. arranging many smaller plots vs. one large one), we can easily see the increase in emission peak intensity as the concentration of SDS increases. Likewise, we can avoid the messy overlap of the max intensity annotations. This is only one way to plot this data, but this is sufficient because were simply inspecting our data at this point. And here we can see that the intensity all occur around a similar wavelength (~ 528 nm) 6.2 Extracting maximal values The plots we made above are great for inspecting our data, but what we really want is the maximal emission intensity value to calculate the CMC of SDS. We can see the maximal values on the plots, but theres no way were typing those in manually. So lets go ahead and get out maximal values from our dataset: sdsMax &lt;- sds %&gt;% group_by(chemical, conc.units, conc) %&gt;% filter(intensity == max(intensity)) %&gt;% ungroup() head(sdsMax) ## # A tibble: 6 x 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 520 0.0056 M SDS 28.1 ## 2 524 0.0072 M SDS 116. ## 3 527 0.0064 M SDS 65.3 ## 4 527 0.008 M SDS 768. ## 5 528. 0.012 M SDS 810. ## 6 528 0.00480 M SDS 22.0 All we did was tell R to take the row with the highest emission intensity value per group. We specified chemical, conc.units, and conc, in case we had more chemicals in our dataset. Are maximum values match those we see in our plot above. Lets see how they stack up againt each other: ggplot(data = sdsMax, aes(x = conc, y = intensity)) + geom_point() Figure 6.1: Plot of maximal fluoresence intensity at various concentrations of SDS. 6.3 Modelling Sigmoidal Curve So we want to find the critical micellular concentration of SDS using the maximum fluorescence emission. The CMC is at the midpoint of the sigmoidal curve. Which means well need to a) plot a sigmoidal curve and b) extract the midpoint. The sigmoidal or S-shaped curve mentioned in the lab manual is known as a logistic regression. Logistic regressions are often used to model systems with a largely binary outcome. In other words, the system starts at point A, and remains there for awhile, before quickly jumping up (or down) to level B and remain there for the remainder. Examples include saturation and dose response curves. For our CMC working data, the fluorescence intensity is low when the \\([SDS] &lt; CMC\\), as micelles are not able to form. However once \\([SDS] &gt; CMC\\), micelles form and the fluorescence intensity increases. We can see this trend in 6.1. There are different forms of logistic regression equations. The simplest form is the 1 parameter, or sigmoid, function which looks like \\(f(x) = \\frac{1}{1+e^{-x}}\\). The outputs for this function are between 0 and 1. We could apply this formula to our model if we somehow normalized our fluoresence intensity accordingly. An alternative is to use the four parameter logistic regression, which looks like: \\[f(x) = \\frac{a - d}{\\left[ 1 + \\left( \\frac{x}{c} \\right)^b \\right ]} + d\\] where: a = the theoretical response when \\(x = 0\\) b = the slope factor c = the mid-range concentration (inflection point) This is commonly referred to as the EC50 or LC50 in toxicology/pharmacology. d = the theoretical response when \\(x = \\infty\\) Why do we need such a complicated formula for our model? Well, looking again at 6.1 we see that the lower point is approximately 20, and not zero. Likewise, the upper limit appears to be around 825. The slope factor is necessary because the transition from the low to high steady state occurs over a small, but not immeasurable, concentration range. And lastly, by including the inflection point, we can calculate exactly for this value using R to get the CMC estimate. 6.3.1 Calculating Logistic Regression A strength of R is its flexibility in running various models, and logistic regression is no different. We can use a number of packages to reach these ends, specifically the drc package contains a plethora of functions for modelling dose response curves (hence drc). However, for this example well use a more generalized approach. Earlier we talked about linear regression, where we plot adjust the slope and intercept of a linear equation to best fit our data (see Calibration Curves). Recall that this optimization is based on minimizing the distance between the model and all of the experimental points (least squares). Well the stats package has a function called nls that expands upon the this to nonlinear models. Per the nls function description: [nls] determine[s] the nonlinear (weighted) least_squared estimates of the parameters of a nonlinear model. So we can create a formula in R based on the four-parameter logistic regression described above. After that, well need to produce some starting details from which the model can build off of. If we dont tell nls where to start, it cant function, as the search space is too large. Looking at @ref{fig:sdsMaxPlot}, the intensity appears to floor around 20; the intensity appears to max out around 820; the midpoint appears to be around 0.0075 M, and lets say the slope is 1. Remember, these are starting values from which nls starts to optimize from, and not the actual values used to construct the model. So, lets create our model logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 1, # slope c = 0.0075, # CMC d= 820) # max intensity ) ## Error in numericDeriv(form[[3L]], names(ind), env): Missing value or an infinity produced when evaluating the model  and we get an error message. Get used to these when modelling! Dont worry about understanding it completely, error messages are often written with programmers in mind so they can be a bit cryptic. You can often copy and paste these directly into any search engine to get some more information, but this one is simple enough: we either have a missing value or an infinity produced. Well we have six input parameters in our model: a, b, c, d, our independent variable conc, and our dependant variable intensity. Weve also supplied starting values to all of them via the list we created inside the function. Therefore, one of our starting values must be too far off from a plausible start point and is causing troubles in the nls function. They all look good except for the slope start value b = 1. The slope here is an approximation for the slope between the min value a and max value d. Looking at our data in @ref{fig:sdsMaxPlot}, that slope may be a bit shallow consider the large jump in intensity. Lets increase the value of b and try again: logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 10, # new slope c = 0.0075, # CMC d= 820) # max intensity ) Ey, no errors! Once you progress beyond simple linear regressions, modelling becomes more of a craft. If we were trying to apply this model to multiple datasets, we would probably want to shop around cran to find a package with self-starting models. This way we can circument having to supply starting parameters. Anyways, thats for another day. For now, lets take a look at our model outputs which are all stored in the logisModel variable. To this end, well use the broom package which cleans up the default model outputs in R. Specifically, well use tidy to get an output of our estimated model parameters (i.e. a,b,c, and d), and augment for a data frame of containing the input values, and the estimated intensity values. Lets look at our fitted values: library(broom) augment &lt;- augment(logisModel) augment ## # A tibble: 9 x 4 ## intensity conc .fitted .resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28.1 0.0056 49.4 -21.3 ## 2 116. 0.0072 116. -0.123 ## 3 65.3 0.0064 49.7 15.6 ## 4 768. 0.008 768. 0.0977 ## 5 810. 0.012 810. -0.0861 ## 6 22.0 0.00480 49.4 -27.5 ## 7 93.0 0.001 49.4 43.5 ## 8 31.7 0.004 49.4 -17.7 ## 9 57.0 0.0016 49.4 7.51 What we can see here from augment are the intensity and conc values we inputted into R. .fitted are the intenisty values for a given concentration fitted to out model, and .resid is the residuals, the difference between the actual and estimated values. Lets go ahead and plot our actual and fitted values against each other. ggplot(augment, aes(x = conc, y = intensity, colour = &quot;actual&quot;)) + geom_point() + geom_line(aes(y = .fitted)) + geom_point(aes(y = .fitted, colour = &quot;fitted&quot;)) Looks pretty good, although its interesting how the baseline at lower concentrations doesnt plateau like the model values. Youll note that the line produced by geom_line will only draw a straight line between points. Theres ways to address this, but we dont need to for our needs right now. Looking again at our model results, there doesnt appear to be any gross outliers, so our model seems to have done a good job. We can verify this by checking the residuals: ggplot(augment, aes(x = conc, y = .resid)) + geom_point() We cant see any obvious patterns in the residuals (i.e. all are negative), so we can have further confidence in the fit of out model. 6.3.2 Extracting model parameters To extrac the model parameters a, b, c, and d we can use the tidy function: library(broom) tidy &lt;- tidy(logisModel) tidy ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 49.4 11.1 4.44 0.00678 ## 2 b 49.0 9.65 5.07 0.00385 ## 3 c 0.00755 0.0000785 96.2 0.00000000230 ## 4 d 810. 27.3 29.7 0.000000808 Looking past the scientific notation, our model values are pretty similar to what we estimated. Specifically, c, our midpoint value is 0.0076 M. Not too bad from our original estimate. And recall that the midpoint of our curve corresponds to the critical micellular concentration of SDS, which weve estimated to be 0.0076M. Not too far from the literature value of 0.0081 M. "],["appendix.html", "Chapter 7 Appendix 7.1 First off, what is R Markdown? 7.2 How do I get started with R markdown? 7.3 So now what do I do with R Markdown?", " Chapter 7 Appendix 7.1 First off, what is R Markdown? In a nutshell, R Markdown allows you to analyse your data with R and write your report in the same place (this document is written with R Markdown). This has loads of benefits including increased reproducibility, and streamlined thinking. No more flipping back and forth between coding and writing to figure out whats going on. Lets run some simple code as an example: # Look at me go mom x &lt;- 2+2 x ## [1] 4 What weve done here is write a snippet of R code, ran it, and printed the results (as they would appear in the console). While the above code isnt anything special, we can extend this concept so that our R markdown document contains any data, figures or plots we generate throughout our analysis in R. For example: library(tidyverse) library(knitr) airPol &lt;- read_csv(&quot;./data/Toronto_60433_2018_Jan2to8.csv&quot;, na = &quot;-999&quot;) kable(airPol[1:5, ], caption = &quot;Example table of airborne pollutant levels used for Figure 1.&quot;) Table 7.1: Example table of airborne pollutant levels used for Figure 1. temperature pollutant concentration date -11.7 NO2 41 2018-01-01 19:00:00 -11.7 O3 2 2018-01-01 19:00:00 -11.3 NO2 28 2018-01-01 20:00:00 -11.3 O3 14 2018-01-01 20:00:00 -11.6 NO2 20 2018-01-01 20:59:59 ggplot(airPol, aes(date, concentration, colour = pollutant)) + geom_line() + theme_classic() Figure 7.1: Time series of 2018 ambient atmospheric O3 and NO2 concentrations (ppb) in downtown Toronto Pretty neat, eh? You might not think so, but lets imagine a scenario youll encounter soon enough. Youre about to submit your assignment, youve spent hours analyzing your data and beautifying your plots. Everything is good to go until you notice at the last minute you were supposed to subtract value x and not value y in your analysis. If you did all your work in Excel (tsk tsk), youll need to find the correct worksheet, apply the changes, reformat your plots, and import them into word (assuming everything is going well, which is never does with looming deadlines). Now if you did all your work in R markdown, you go to your one .rmd document, briefly apply the changes and re-compile your document. 7.2 How do I get started with R markdown? As youve already guessed, R markdown documents use R and are most easily written and assembled in the R Studio IDE. If you have not done so, download R from the comprehensive R archive network (CRAN), link here: http://cran.utstat.utoronto.ca/, and R Studio, link here: https://rstudio.com/products/rstudio/download/). Follow the listed instructions and you should be well on your way. You can also see the accompanying Working with RStudio document on Quercus for additional top tips. Once setup with R and R Studio, well need to install the rmarkdown and tinytex packages. In the console, simply run the following code: install.packages(&quot;rmarkdown&quot;) # downloaded from CRAN install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() # install TinyTeX The rmarkdown package is what well use to generate our documents, and the tinytex package enables compiling documents as PDFs. Theres a lot more going on behind the scenes, but you shouldnt need to worry about it. Now that everything is setup, you can create your first R Markdown document by opening up R Studio, selecting FILE -&gt; NEW FILE -&gt; Rmarkdown. A dialog box will appear asking for some basic input parameters for your R markdown document. Add your title and select PDF as your default output format (you can always change these later if you want). A new file should appear thats already populated with some basic script illustrating the key components of an R markdown document. 7.2.1 Great, now whats going on with this R markdown document? Your first reaction when you opened your newly created R markdown document is probably that it doesnt look anything at all like something youd show your TA. Youre right, what youre seeing is the plain text code which needs to be compiled (called knit in R Studio) to create the final document. Lets break down what the R markdown syntax means then lets knit our document. When you create a R markdown document like this in R Studio a bunch of example code is already written. You can compile this document (see below) to see what it looks like, but lets break down the primary components. At the top of the document youll see something that looks like this: --- title: &quot;Untitled&quot; author: &quot;David Hall&quot; date: &quot;24/08/2020&quot; output: pdf_document --- This section is known as the preamble and its where you specify most of the document parameters. In the example we can see that the document title is Untitled, its written by yours truly, on the 24th of August, and the default output is a PDF document. You can modify the preamble to suit your needs. For example, if you wanted to change the title you would write title: \"Your Title Here\" in the preamble. Note that none of this is R code, rather its YAML, the syntax for the documents metadata. Apart from whats shown you shouldnt need to worry about this much, just remember that indentation in YAML matters. Reading further down the default R markdown code, youll see different blocks of text. In R markdown anything you write will be interpreted as body text (i.e .the stuff you want folks reading like this) in the knitted document. To actually run R code youll need to see the next section. 7.2.2 How to run R code in R Markdown Theres two ways to write R code in markdown: Setup a code chunk. Code chunks start with three back-ticks like this: ```{r}, where r indicates youre using the R languauge. You end a code chunk using three more backticks like this ```. Specify code chunks options in the curly braces. i.e. ```{r, fig.height = 2} sets figure height to 2 inches. See the Code Chunk Options section below for more details. Inline code expression, which starts with `r and ends with ` in the body text. Earlier we calculated x &lt;- 2 + 2, we can use inline expressions to recall that value (ex. We found that x is 4) A screenshot of how this document, the one youre reading, appeared in R Studio is shown in Figure 2. To actually run your R code you have two options. The first is to run the individual chunks using the Run current chunk button (See figure 2). This is a great way to tinker with your code before you compile your document. The second option is to compile your entire document using the Knit document button (see Figure 2). Knitting will sequentially run all of your code chunks, generate all the text, knit the two together and output a PDF. Youll basically save this for the end. Note all the code chunks in a single markdown document work together like a normal R script. That is if you assign a value to a variable in the first chunk, you can call this variable in the second chunk; the same applies for libraries. Also note that every time you compile a markdown document, its done in a fresh R session. If youre calling a variable that exist in your working environment, but isnt explicitly created in the markdown document youll get an error. How this document, the one youre reading, appeared in RStudio; to see the final results scroll up to Figure 1. Note the knit and run current chunk buttons. 7.2.3 How do I go from R markdown to something I can hand-in To create a PDF to hand in youll need to compile, or knit, your entire markdown document as mentioned above. To knit (or compile) your R markdown script, simply click the knit button in R Studio (yellow box, Figure 2). You can specify what output you would like and R Studio will (hopefully) compile your script. If you want to test how your code chunks will run, R Studio shows a little green play button on the top right of every code chunk. this is the run current chunk button, and clicking it will run your code chunk and output whatever it would in the final R markdown document. This is a great way to tweak figures and codes as it avoids the need to compile the entire document to check if you managed to change the lines from black to blue in your plot. 7.3 So now what do I do with R Markdown? You do science and you write it down! In all seriousness though, this document was only meant to introduce you to R markdown, and to make the case that you should use it for your ENV 316 coursework. A couple of the most useful elements are talked about below, and there is a wealth of helpful resources for formatting your documents. Just remember to keep it simple, theres no need to reinvent the wheel. The default R markdown outputs are plenty fine with us. 7.3.1 R Markdown resources and further reading Theres a plethora of helpful online resources to help hone your R markdown skills. Well list a couple below (the titles are links to the corresponding document): Chapter 2 of the R Markdown: The Definitive Guide by Xie, Allair &amp; Grolemund (2020). This is the simplest, most comprehensive, guide to learning R markdown and its available freely online. The R markdown cheat sheet, a great resource with the most common R markdown operations; keep on hand for quick referencing. Bookdown: Authoring Books and Technical Documents with R Markdown (2020) by Yihui Xie. Explains the bookdown package which greatly expands the capabilities of R markdown. For example, the table of contents of this document is created with bookdown. 7.3.2 R code chunk options You can specify a number of options for an individual R code chunk. You include these at the top of the code chunk. For example the following code tells markdown youre running code written in R, that when you compile your document this code chunk should be evaluated, and that the resulting figure should have the caption Some Caption. A list of code chunk options is shown below: ```{r, eval = FALSE, fig.cap = &quot;Some caption&quot;} # some code to generate a plot worth captioning. ``` option default effect eval TRUE whether to evaluate the code and include the results echo TRUE whether to display the code along with its results warning TRUE whether to display warnings error FALSE whether to display errors message TRUE whether to display messages tidy FALSE whether to reformat code in a tidy way when displaying it fig.width 7 width in inches for plots created in chunk fig.height 7 height in inches for plots created in chunk fig.cap NA include figure caption, must be in quotation makrs (\"\") 7.3.3 Inserting images into markdown documents Images not produced by R code can easily be inserted into your document. The markdown code isnt R code, so between paragraphs of bodytext insert the following code. Note that compiling to PDF, the LaTeX call will place your image in the optimal location, so you might find your image isnt exactly where you though it would be. A quick google search can help you out if this is problem. ![Caption for the picture.](path/to/image.png){width=50%, height=50%} Note that in the above the use of image atributes, the {width=50%, height=50%} at the end. This is how youll adjust the size of your image. Other dimensions you can use include px, cm, mm, in, inch, and %. 7.3.4 Generating Tables Theres multiple methods to create tables in R markdown. Assuming you want to display results calculated through R code, you can use the kable() function. Please consult Chapter 10 of the R Markdown Cookbook for additional support. Alternatively, if you want to create simple tables manually use the following code in the main body, outside of an R code chunk. You can increase the number of rows/columns and the location of the horizontal lines. To generate more complex tables, see the kable() function and the kableExtra package. Header 1 | Header 2| Header 3 ---------|---------|---------| Row 1 | Data | Some other Data Row 2 | Data | Some other Data ---------|---------|---------| Header 1 Header 2 Header 3 Row 1 Data Some other Data Row 2 Data Some other Data 7.3.5 Spellcheck in R Markdown While writing an R markdown document in R studio, go to the Edit tab at the top of the window and select Check Spelling. You can also use the F7 key as a shortcut. The spell checker will literally go through every word it thinks youve misspelled in your document. You can add words to it so your spell checkers utility grows as you use it. Note that the spell check with also check your R code; be wary of changing words in your code chunks because you may get an error down the line. 7.3.6 Quick reference on R markdown syntax Inline formatting; which is used to format your text. Numbered lists Normal lists Lists Block-level elements, i.e. youre section headers Example R markdown syntax used for formatting shown above: - **Inline formatting**; *which* is ~used~ to ^format^ `your text`. 1. Numbered lists - Normal lists - Lists - **Block-level elements**, i.e. your section headers # Headers ## Headers ### Headers "]]

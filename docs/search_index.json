[["index.html", "R for Environmental Chemistry Preface Providing Feedback Authors Acknowlegements", " R for Environmental Chemistry David Hall, Steven Kutarna, Kristen Yeh, Hui Peng and Jessica Deon Last built on: 2021-09-08 Preface Howdy, This website is more-or-less the living result of a collaborative project between us. Were not trying to be an exhaustive resource for all environmental chemists. Rather, were focusing on developing broadly applicable data science course content (tutorials and recipes) based in R chemistry courses and research. This book will is broken up into four sections: Section 1: Getting Started in R is a general guide for the complete novice that will help you install, setup, and run R code. It features a useful tutorial exercise to make sure you have a working script before starting courses. Section 2: Data Wrangling introduces data analysis workflows and showcases how you can use R and the tidyverse to import and clean up your data into a consistent format to tackle the vast majority of the data science/analysis problems youll encounter in undergraduate environmental chemistry courses. Section 3: Data Analysis Toolbox provides code and theory behind the most common data analysis practices in environmental chemistry. These include linear regression analysis, a myriad of visualizations, etc. Section 4: Notes for Env. Chem. Labs consist of chapters specific to individual laboratory experiments. They rely upon knowledge from the previous three sections to introduce concepts unique to individual labs. We recommend that you read through Sections 1 and 2 in sequential order. These provide the foundation for the consistent data analysys workflow used throughout Sections 3 and 4. Providing Feedback If you notice an error, mistake or if you have suggestions for adding features or improving the book, please reach out to us or flag an issue on GitHub. Dave at davidross.hall@mail.utoronto.ca Jessica Deon at jessica.deon@utoronto.ca Authors David Hall/ Steven Kutarna Kristen Yeh Hui Peng Jessica Deon Acknowlegements Additionally, we would like to thank Jeremy Gauthier, Andrew Folkerson, Mark Panas, and Stephanie Schneider for all of their comments, suggestions, and hard work integrating the concepts of this book into the CHM410 Laboratory curriculum. "],["installing-r.html", "Chapter 1 Installing R 1.1 Prerequisite software 1.2 Running R Code 1.3 Customizing RStudio 1.4 Where to get help 1.5 Summary", " Chapter 1 Installing R You may have heard about the coding or the R programming language, but figuring out how to get started can be a hurdle; at least it was for us. This chapter will cover installing the software youll need for coding in R. 1.1 Prerequisite software Before we get started, youll need to download the following open source and free software: R RStudio tidyverse suite of packages Read on for instructions on downloading all three. 1.1.1 R R is the programming language well code in. R is hosted on the Comprehensive R Archive Network (CRAN) and is one of the most popular programming languages for statisticians and scientist alike. You can download the latest build for your operating system here. A quick aside, but dont be intimidated by the term coding. Coding is simply writing instructions for the computer to execute. The only catch is has to be in a language that both we, humans, and the computer can understand. For our needs were using R, and like any language, R has its own syntax, rules, and quirks which well cover in later chapters. 1.1.2 Downloading RStudio IDE RStudio is the integrated development environment of choice when working with R. Its where youll actually be typing your code and interacting with R. Again, R is a language, and you need somewhere to write it down to make use of it. Writing in English can be done with a pencil and notepad or a word processor filed with useful tools to help you write. RStudio is the later for coding in R. You can download the latest version of RStudio here. Once you have R and RStudio installed, go ahead and open up RStudio. Once you open RStudio, youll be greeted with an interface divided into numerous panes. Weve highlighted the major ones in the image below: The RStudio interface with annottated regions Each pane serves a specific role: The console allows you to directly type and run your code. It also provides messages, warnings, and errors from any code you run. The environment window lists all variables, data, and functions youve created since the start of your coding session. The viewer shows your outputs, help documents, etc. which each has their own tab. 1.1.3 Installing packages Packages are previously written snippets of code that extend the capabilities of base R. Typically packages are created to address specific issues or workflows in different types of analysis. This book will make frequent use of a family of packages called the tidyverse. These packages all share a common thought process and integrate naturally with one another. You can download the entire suite of tidyverse packages by simply copy and pasting the following code into the console and pressing enter. install.packages(&quot;tidyverse&quot;) Youll see a flurry of lines printed to the console indicating the status of the installation. Once installed you wont be able to use these functions until you load it with library(). Enter the code below into the console to load the tidyverse package. library(tidyverse) The output shows us which packages are included in the tidyverse() and their current version numbers, as well as conflicts (where functions from different packages share the same name). Dont worry about these for now. 1.2 Running R Code As weve already seen, you can run bits of R code directly from the console. Throughout the book, code you can copy and run will look like this: 2 + 2 ## [1] 4 Noticed that both the code (the first part) and what the code outputs (the second part) are shown. Throughout this book code outputs will be proceeded by ##. You can run code directly from the console. Its handy for short and sweet snippets of code, something that can be typed in a single line. Examples of this is the install.packages() function, or to use R as a calculator: 2 * 3 ## [1] 6 pi * (10/2) ## [1] 15.70796 However, working like this isnt very useful Imagine printing a book one sentence at a time, you couldnt really go back and edit earlier work because its already printed. Thats why we write out code in scripts. Scripts are similar to recipes, in that theyre a series of instructions that R evaluates from the top of the script to the bottom. More importantly, writing your code out in a script makes it more readable to humans (presumably this includes you). Dont undervalue the usefulness of legible code. Your code will evaluate in seconds or minutes whereas it may take you hours to understand what it does. Lets open up a new script in RStudio by going to File-&gt;New File-&gt;R Script, or by clicking on the highlighted button in the image below. Figure 2.5: Opening a new script in RStudio. This should open up a new window in the RStudio interface, as shown in the following image. Figure 2.6: Scripts window in RStudio. You can copy and paste the code above into the script, save it, edit it, etc. and ultimately run specific lines of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on Mac), or by clicking the Run button in the top right corner of the Scripts window. Whenever you copy code blocks from this website (or other online sources). If youre reading this book online, you can easily copy an entire block of code using the copy button in the top right corner of the code block. Well dive into the basics of coding in R in the next chapter. 1.3 Customizing RStudio As many of us spend an absurd amount of time staring at bright screens, some of you may be interested in setting your RStudio to Dark Mode. You can customize the appearance of your RStudio interface by clicking Tools-&gt;Global Options, or RStudio-&gt;Preferences on Mac, then clicking Appearance on the left. Select your preferred Editor Theme from the list. Figure 2.4: RStudio Appearance customization window. 1.4 Where to get help While its often tempting to contact your TA or Professor at the first sign of trouble, its better to try and resolve your issues on your own. Given the popularity of R, if youve run into an issue, someone else has too and theyve complained about it and someone else has almost certainly solved it! An often unappreciated aspect of coding/data science is knowing how to get help, how to search for it, and how to translate someones solutions to your unique situation. Places to get help include: Google, Stack Overflow, etc. When in doubt Google it. Using built-in documentation (?help) reference books such as the invaluable R for Data Science, which inspired this entire project. And yes, when all else fails, holler at your TA/profs. 1.5 Summary In this chapter weve covered: Installing and running R and RStudio Running R code from the console Installing the tidyverse() package, the basis of the subsequent code in this book How to customize the appearance of RStudio so you dont burn out your eyes at night In the next chapter well break down how to setup your work in R for legibility, simplicity, and reproducibility. After all, the person cursing any of your sloppy work will invariably be you, so be kind to yourself, and do it right the first time. "],["rstudio-projects.html", "Chapter 2 RStudio Projects 2.1 Paths and directories 2.2 Importing a project 2.3 Summary 2.4 Creating an RStudio project", " Chapter 2 RStudio Projects Youre probably eager to start coding, but an equally important aspect is understanding the structure of your work. Knowing how to organize the files needed for your analysis and how to access them quickly is critical. Learning this early on will save you plenty of time and heartache down the line. So lets hold off on coding and consider where were working on your computer. Because we believe in it so much, well say it up top: Always work inside an RStudio Project, and use a unique project for each lab/experiment. 2.1 Paths and directories Before you get started with running your code, it is good to know where your analysis is actually occurring, or where your working directory is. The working directory is the folder where R looks for files that you have asked it to import, and the folder where R stores files that you have asked it to save. RStudio displays the current working directory at the top of the console, as shown below, but can also be printed to the console using the command getwd(). By default, R usually sets the working directory to the home directory on your computer. The ~ symbol denotes the home directory, and can be used as a shortcut when writing a file path that references the home directory. You can change the working directory using setwd() and an absolute file path. Absolute paths are references to files which point to the same file, regardless of what your working directory is set to. In Windows, absolute paths begin with \"C:\", while they begin with with a slash in Mac and Linux (i.e., \"/Users/Vinny/Documents\"). It is important to note that absolute paths and setwd() should never be used in your scripts because they hinder sharing of code  no one else will have the same file configuration as you do. If you share your script with your TA or Prof, they will not be able to access the files you are referencing in an absolute path. Thus, they will not be able to run the code as-is in your script. In order to overcome the use of absolute paths and setwd(), we strongly recommend that you conduct all work in RStudio within an R project. When you create an R project, R sets the working directory to a file folder of your choice. Any files that your code needs to run (i.e., data sets, images, etc.) are placed within this folder. You can then use relative paths to refer to data files in the project folder, which is much more conducive to sharing code with colleagues, TAs, and Profs. 2.2 Importing a project While you can create a project from scratch (discussed below), weve created a draft project template. Download it, and youll have a working RStudio project that you can use as you follow along with the code in the rest of this chapter and the tutorial exercise. Downloading the template project from the GitHub repository here; there are instructions on downloading at the bottom of the repositories webpage. Unzip the project folder somewhere useful/that makes sense to you (i.e. a folder for schoolwork). From RStudio click File -&gt; Open Project... and open the R4EnvChem-ProjectTemplate.Rproj file. If youve followed the steps above you should have successfully downloaded and opened an RStudio project, and it should look like this: Note how the project name is displayed on the top right. You can quickly switch between projects here which is useful if youll be using R for many different labs/courses. As well, take note that the working directory has changed to the one where the RStudio project is located. Since youve downloaded the entire project, the working directory for the project includes the example scripts and data files youll need to continue along with the remainder of this book. If you open the project folder (or access it from the Files tab) it should look like this: R4EnvChem-ProjectTemplate  R4EnvChem-ProjectTemplate.Rproj  Rscript-example.R | README.md | Rmarkdown-example.rmd  data  2018-01-01_60430_Toronto_ON.csv  2018-07-01_60430_Toronto_ON.csv | ...  images  DHall_TorontoPano.jpg With the R4EnvChem-ProjectTemplate.Rproj file located in the main folder, this is important as well be able to readily look for files we stored in project subfolders such as data and images. As you can see, the R4EnvChem-ProjectTemplate.Rproj file is located in the main folder, which RStudio will now treat as the working directory. Essentially it means well be able to quickly access files in project subfolders such as data and images without having to find out what the full file path is for your own computer. Youll appreciate this as you progress through this book. In the future you can create your own projects from scratch, but it behooves you to follow the template layout. Having consistently named folders youll use in every project will help simplify your life down the road. 2.3 Summary In this chapter weve covered: Importing the R4EnvChem Project Template so we have access to data for the tutorial (amongst other things) The concept of paths and directories and how relative referencing withing a project greatly simplify this Well further sermonize about the usefulness of projects later on, but now that weve gotten a working project with some data, lets move onto the R Coding Basics to start coding. 2.4 Creating an RStudio project Weve provided instructions on creating your own RStudio project from scratch, but you can always copy the template project folder above (or any for that matter) to re-purpose it as you see fit. To create a new project: go to File-&gt;New Project, or click the button highlighted in the image below. Click New Directory, then New Project. You may want your project directory to be a sub-folder of an existing directory on your computer which already contains your data sets. If this is the case, click Existing Directory instead of New Directory at the previous step, and then select the folder of your choice. Next, youll be asked to choose a sub-directory name and location. Enter your selected name and choose an appropriate location for the folder on your computer. Click Create Project, and you should now see your chosen file path displayed in the Files tab of the Viewer pane: When working on assignments for coursework, it is good practice to create a new R project for each assignment you work on. You should store the data, images, and any other files required for that assignment within the folder for the designated R project. You can create sub-folders for data and images, however, you may want to avoid making too many nested sub-folders, as this will make your paths long and tiresome to type. For a hypothetical course with 5 Labs (cough CHM410 cough), your coursework would look like this: CHM410 |  Project 1 | | | | project1.Rproj | | project1WriteUp.Rmd | data |  ... | images | ... |  Project 2 | | project2.Rproj | project2WriteUp.Rmd data  ... images ... ... With a separate folder for each experiment, and within each folder is an RStudio project, data, images, and other files required for that specific project. You shouldnt have nested R studio project as their is no benefit to this approach. Keep everything you need in one location, and no more. "],["r-coding-basics.html", "Chapter 3 R Coding Basics 3.1 Variables 3.2 Data types 3.3 Data structures 3.4 Other data structures 3.5 R packages and functions 3.6 Summary", " Chapter 3 R Coding Basics Now that you know how to navigate RStudio and have a working project, well take a look at the basics of R. As were chemist first, and not computer programmers, well try and avoid as much of the nitty-gritty underneath the hood aspects of R. However, a risk of this approach is being unable to understand errors and warnings preventing your code from running. As such, well introduce the most important and pertinent aspects of the R language to meet your environmental chemistry needs. 3.1 Variables Weve already talked about how R can be used like a calculator: (1000 * pi) / 2 ## [1] 1570.796 (2 * 3) + (5 * 4) ## [1] 26 But managing these inputs and outputs is simplified with variables. Variables in R, like those youve encountered in math class, can only have one value, and you can reference or pass that value along by referring the variable name. And, unlike the variables in math classes, you can change that value whenever you want. Another way to think about it is that a variable is a box in which you store your value. When you want to move (reference) your value, you move the box (and whatever is inside of it). Then you can simply open the box somewhere else without having to worry about the hassle of whats inside. You can assign the a value to a variables using &lt;-, as shown below. x &lt;- 12 x ## [1] 12 In addition to reading code top to bottom, you often read it from right to left. x &lt;- 12 would be read as take the value 12 and store it into the variable x. The second line of code, x, simply returns the value stored inside x. Note that when a variable is typed on it own, R will print out its contents. You can now use this variable in snippets of code: x ## [1] 12 x &lt;- x * 6.022e23 x ## [1] 7.2264e+24 Remember, R evaluates from right to left, so the code above is taking the number 6.022e23 and multiplying it by the value of x, which is 12 and storing that value back into x. Thats how were able to modifying the contents of a variable using its current value. You can also overwrite the contents of a variable at anytime (i.e. x &lt;- 25). Note that variable names are case sensitive, so if your variable is named x and you type X into the console, R will not be able to print the contents of x. Variable names can consist of letters, numbers, dots (.) and/or underlines (_). Here are some rules and guidelines for naming variables in R: Variable Name Requirements as dictated by R names must begin with a letter or with the dot character. var and .var are acceptable. Variable names cannot start with a number or the . character cannot be preceded by number. var1 is acceptable, 1var and .1var are not. Variable names cannot contain a space. var 1 is interpreted as two separate values, var and 1. Certain words are reserved for R, and cannot be used as variable names. These include, but are not limited to, if, else, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NA, and NaN Good names for variables are short, sweet, and easy to type while also being somewhat descriptive. For example, lets say you have an air pollution data set. A good name to assign the data set to would be airPol or air_pol, as these names tell us what is contained in the data set and are easy to type. A bad name for the data set would be airPollution_NOx_O3_June20_1968. While this name is much more descriptive than the previous names, it will take you a long time to type, and will become a bit of a nuisance when you have to type it 10+ times to refer to the data set in a single script. Please refer to the Style Guide found in Advanced R by H. Wickham for more information. Lastly, R evaluates code from top-to-bottom of your script. So if you reference a variable it must have already been created at an earlier point in your script. For example: y + 1 ## Error in eval(expr, envir, enclos): object &#39;y&#39; not found y &lt;- 12 The code above returns the object 'y' not found error because were adding + 1 to y which hasnt been created yet, its created on the next line. These errors also pop up when you edit your code without clearing your workplace. All variables created in a session are stored in the working environment so you can call them, even if you change your code. This means you can accidentally reference a variable that isnt reproduced in the latest iteration of your code. Consequently, a good practice is to frequently clear your work-space suing the broom button in the environment pane. This will help you to ensure the code youre writing will is organized in the correct order; see Saving R scripts for why this is important. 3.2 Data types Data types refer to how data is stored and handled by and in R. This can get complicated quickly, but well focus on the most common types here so you can get started on your work. Firstly, here are the data types youll likely be working with: character: \"a\", \"howdy\", \"1\", is used to represent string values in R. Basically its text that youd read. Strings are wrapped in quotation marks. For example, \"1\", despite being read as number by us, is stored as a character and treated as such by R. numeric is any real or decimal number such as 2, 3.14, 6.022e23. integer such as 2L, note the L tells R this is an integer. logical is a Boolean logic value, they can only be TRUE or FALSE Sometimes R will misinterpret a value as the wrong data type. This can hamper your work as you cant do arithmetic on a string! So lets look at some helpful functions to test the data type of a value in R, and how to fix it. x &lt;- &quot;6&quot; x / 2 ## Error in x/2: non-numeric argument to binary operator non-numeric argument to binary operator is a commonly encountered error, and its simply telling you that youre trying to do math on something you cant do math on. You might think if x is 6, why cant I divide it by 2? Lets see what type of data x is: is.numeric(x) # test if numeric ## [1] FALSE is.logical(x) # test if logical ## [1] FALSE is.integer(x) # test if integer ## [1] FALSE is.character(x) # test if character ## [1] TRUE So the value of x is a character, in other words R treats it as a word, and we cant do math on that (think, how would you divide a word by a number?). So lets convert the data type of x to numeric to proceed. x ## [1] &quot;6&quot; x &lt;- as.numeric(x) is.numeric(x) ## [1] TRUE x ## [1] 6 x / 2 ## [1] 3 So weve converted our character string \"6\" to the numerical value 6. Keep in mind there are other conversion functions which are described elsewhere, but you cant always convert types. In the above example we could convert a character to numeric because it was ultimately a number, but we couldnt do the same if the value of x was \"six\". x &lt;-&quot;six&quot; x &lt;- as.numeric(x) ## Warning: NAs introduced by coercion x ## [1] NA NAs introduced by coercion means that R didnt know how to convert six to a numeric value, so it instead turned it into an NA, representing a missing value. 3.3 Data structures Data structures refers to how R stores data. Again, its easy to get lost in the weeds here so well focus on the most common and useful data structure for your work: data frames. Data frames consist of data stored in rows and columns. If youve ever worked with a spreadsheet (i.e. Excel), its essentially that with the caveat that all data stored in a column must be of the same type. Again, different columns can have different data types, but within a column all the data needs to be the same type. R will convert your data otherwise to make it all the same. A common error is a single character in a column of numerical values leading to the entire column to be interpreted as character values; similar to what we discussed above. Errors like this most often stem from mistakes in recording and importing your data so be careful! From the R4EnvChem-ProjectTemplate, downloaded in Importing a project, lets import some real data as follows by typing the following into the console: # Install and open tidyverse if you haven&#39;t already # install.packages(&quot;tidyverse&quot;) # library(tidyverse) airPol &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## naps = col_double(), ## city = col_character(), ## p = col_character(), ## latitude = col_double(), ## longitude = col_double(), ## date.time = col_datetime(format = &quot;&quot;), ## pollutant = col_character(), ## concentration = col_double() ## ) read_csv() is a useful function from the tidyverse which, as you might guess from its name, can read a .csv file and convert it into a data frame. The data we just imported contains air quality data measured in downtown Toronto around January 2018. The Column specification summary printed to the console is a useful feature of read_csv(). It tells you what data type was determined for each column when it was imported. Note that double is simply another term for the numeric data type. Some of the variables are: naps, city, p, latitude, longitude to tell you where the data was measured. data.time for when the measurements were taking. Note this is a datetime, which is a subset of numeric data. The values contained herein correspond to time elements such as year, month, data, and time. pollutant for the chemical measured concentration for the measured concentration in parts-per-million (ppm). Weve assigned it to the variable: airPol. This is so we can reference it and make use of it later on (see below). If we didnt do this our data would simply be printed to the console which isnt helpful. Lets take a look at the data itself: # a variable by itself will return it&#39;s contents airPol ## # A tibble: 507 x 8 ## naps city p latitude longitude date.time pollutant ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 60430 Toronto ON 43.7 -79.5 2018-01-01 00:00:00 O3 ## 2 60430 Toronto ON 43.7 -79.5 2018-01-01 00:00:00 NO2 ## 3 60430 Toronto ON 43.7 -79.5 2018-01-01 00:00:00 SO2 ## 4 60430 Toronto ON 43.7 -79.5 2018-01-01 01:00:00 O3 ## 5 60430 Toronto ON 43.7 -79.5 2018-01-01 01:00:00 NO2 ## 6 60430 Toronto ON 43.7 -79.5 2018-01-01 01:00:00 SO2 ## 7 60430 Toronto ON 43.7 -79.5 2018-01-01 02:00:00 O3 ## 8 60430 Toronto ON 43.7 -79.5 2018-01-01 02:00:00 NO2 ## 9 60430 Toronto ON 43.7 -79.5 2018-01-01 02:00:00 SO2 ## 10 60430 Toronto ON 43.7 -79.5 2018-01-01 03:00:00 O3 ## # ... with 497 more rows, and 1 more variable: concentration &lt;dbl&gt; Here we see that the data is stored in a tidy format, which is to say each column is a variable and each row is an observation. So reading the first row, we know that the Toronto 60430 station on 2018-07-01 at midnight measured ambient O3 concentrations of 46 ppm (Note the concentration column isnt printed due to width). The concept of tidy data is important and is integral to working in R. Its discussed further in Tidying Your Data. Lastly, R will only output a small chunk of our data for us to see. If youd like to see it in full, go the the Environment pane and double click on the airPol data. 3.3.1 Accessing data in subfolders Note that read_csv() requires us to specify the file name, but in the above example we prefixed our file name with \"data/2018...\". This is because the .csv file we want to open is stored in the data sub-folder. By specifying this in the prefix, we tell read_csv() to first go to the data sub folder in the working directory and then search for and open the specified data file. What weve done above is called relative referencing and its a huge benefit of projects. The actual data file is stored somewhere on your computer in a folder like \"C:/User/Your_name/Documents/School/Undergrad/Second_Year/R4EnvChemTemplate/data/2018-01-01_60430_Toronto_ON.csv\". If we werent in a project, this is what youd need to type to open your file, but since were working in the project, R assumes the long part, and begins searching for files inside the project folder. Hence, why we only need \"data/2018...\". Not only is this much simpler to type, and but it makes sharing your work with colleagues, TAs, and Profs (and yourself!) much easier. In other words, if you wanted to share your code, you would send the entire project folder (code &amp; data) and the receiver could open it and run it as is. 3.4 Other data structures R has several other data structures. They arent as frequently used, but its worth being aware of their existence. Other structures include: Vectors, which contain multiple elements of the same type; either numeric, character (text), logical, or integer. Vectors are created using c(), which is short for combine. A data frame is just multiple vectors arranged into columns. Some examples of vectors are shown below. num &lt;- c(1, 2, 3, 4, 5) num ## [1] 1 2 3 4 5 char &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;) char ## [1] &quot;blue&quot; &quot;green&quot; &quot;red&quot; log &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE) log ## [1] TRUE TRUE TRUE FALSE FALSE FALSE Lists are similar to vectors in that they are one dimensional data structures which contain multiple elements. However, lists can contain multiple elements of different types, while vectors only contain a single type of data. You can create lists using list(). Some examples of lists are shown below. You can use str() to reveal the different components of a list, in a more detailed format than if you were to simply type the assigned name of the list. hi &lt;- list(&quot;Greetings&quot; = &quot;Hello&quot;, &quot;someNumbers&quot; = c(5,10,15,20), &quot;someBooleans&quot; = c(TRUE, TRUE, FALSE)) str(hi) ## List of 3 ## $ Greetings : chr &quot;Hello&quot; ## $ someNumbers : num [1:4] 5 10 15 20 ## $ someBooleans: logi [1:3] TRUE TRUE FALSE hi ## $Greetings ## [1] &quot;Hello&quot; ## ## $someNumbers ## [1] 5 10 15 20 ## ## $someBooleans ## [1] TRUE TRUE FALSE hi$Greetings ## [1] &quot;Hello&quot; There are many freely available resources online which dive more in depth into different data structures in R. If you are interested in learning more about different structures, you can check out the Data structure chapter of Advanced R by Hadley Wickham. 3.5 R packages and functions Functions are bits of code written to execute a specific task. Weve already used several functions such as library() to import packages, and read_csv() to read the air quality data as seen in Data structures above. Functions offer a convenient means to reduce the amount of typing while making code more reliable and readable. Some of these functions are built into R, such as library(), but often people write new functions to improve upon base R to help it meet the needs of its users, such as the read_csv(). A collection of functions for a similar tasks is stored in a package, such as the tidyverse suite of packages which contains functions for plotting (ggplot2), reading data read_csv() and more. Lets take a look at one of the functions youll be using the most: ggplot() from the ggplot2 package which is included in the tidyverse. 3.5.1 ggplot2 ggplot() allows you to create a variety of visualizations to explore and communicate your data and results. Like every function, ggplot() has required arguments, i.e. data and instructions you pass to the function. The required arguments for this function are the data to be plotted and the aesthetic mappings for how the plot should look. Using our loaded air quality data from above, you can copy and paste the following code in the console: ggplot(data = airPol, aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() Breaking this down: Were calling ggplot() in the first line and passing the arguments inside the parentheses Were specifying that we want to plot the loaded air pollution data with data = airPol We insert a , to separate each argument We specified the aesthetics arguments (aes()) as values stored in our airPol data: x = data.time means our x-axis will be the data in the data.time column. y = concentration means our y-axis will be the data in the concentration column. colour = pollutant means we colour each point based on the value of the pollutant column. We add a + on the second last line of code as this so we can add components to our ggplot() And finally we add geom_point() to specify what type of plot we want; in this case its a scatter plot. geoms are layers that combine data, aesthetic mappings, and other data to create a plot. There are many other geoms. Go ahead and try geom_line(). For more examples see Visualizations in Section 3. ggplot() allows us to quickly create numerous plots of our data to aid our analysis. We can pass more than geoms to ggplot to improve our graphics. We can even stack geoms! ggplot(data = airPol, aes(x = pollutant, y = concentration, colour = pollutant)) + geom_boxplot() + geom_jitter() + labs(title = &quot;Toronto 60430 Air Quality Data&quot;, subtitle = &quot;from July 1st to July 8th, 2020&quot;, x = &quot;Airborn Pollutant&quot;, y = &quot;Concentration (ppm)&quot;, caption = &quot;Data from 2018 ECCC NAPS Hourly Data&quot;) + theme(legend.position = &quot;none&quot;) This plot looks more complicated then the previous one, but its the same data plotted slightly differently and with a few bells and whistles: We specified that the pollutant column would be the x-axis, i.e. the three pollutants. We kept the y-axis and colour the same. geom_boxplot() creates a box-plot summarizing the spread of our data. geom_jitter() is overlaid so we see all the individual points in our data set; this is useful to make sure stuff isnt found in clusters. Annotated the plot using labs() including title, subtitle, x- and y-axis, and a caption. Useful for publications. Made some final aesthetic changes using theme() specifically we removed the legend using legend.position = \"none\". This covers the basics of ggplot() but there is much more you can do with ggplot(). And through the use of packages you can you extend the functionalities even further. All of this is discussed in more detail in the Visualizations chapter. 3.5.2 Calling specific functions Weve called functions like ggplot() and read_csv() from the ggplot2 and readr packages, respectively. When we did so, they were implicitly imported when we called library(tidyverse). What library does is import all of the functions within a package into the R workspace, so we can simply refer to them by name later on. Sometimes youll want to be explicit to which function you call, as you can run into conflicts where different functions from different packages have the same name. Or you might not want to import the entire package when you only need to call one function. Either way, to explicitly call a function from a specific package you type the package name, followed by ::, and the function name. I.e. We can use read_csv() without importing the tidyverse/readr packages by simply typing: readr::read_csv(). Note the package still needs to be installed on your computer for this to work. 3.5.3 Function documentation An oft unappreciated aspect of packages is that they not only contain functions we can use, but documentation. Documentation provides a description of the function (what it does), what arguments it takes, details, and working examples.Often the easiest way to learn how to use a function is to take a working example and change it bit by bit to see how it works etc. To see documentation check the help tab in the outputs window or type a question mark in front of a functions name: # Takes you to the help document for the ggplot function ?ggplot You can also write youre own functions. Please see Programming with R for additional details. 3.6 Summary In this chapter weve covered: The basics of coding in R including variables, data types, and data structures (notably data.frames). How to install r packages and make use of functions Importing data from your project folder into R Preliminary exploration of ggplot2 to visualize the aforementioned data. Now that youre familiar with navigating RStudio and some basic coding building blocks, you may have realized that working the console can get real messy, real quick. Rread on to Workflows for R Coding where well discuss R workflows to make everyones lives easier. "],["workflows-for-r-coding.html", "Chapter 4 Workflows for R Coding 4.1 Creating or opening a script 4.2 Workspace and whats real 4.3 Saving R scripts 4.4 Script formatting 4.5 Viewing data and code simultaneously 4.6 Troubleshooting error messages 4.7 Summary", " Chapter 4 Workflows for R Coding In the previous chapter we did our coding in the console, which got messy rather quickly. As a result, we code in scripts. Scripts are essentially a recipe that R reads and executes from top to bottom to execute instead of you typing and running each line of code. An important aspect of scripts is that theyre reproducible. In other words you should be able to rerun your script and get the exact same result every time. 4.1 Creating or opening a script To create a script: Go to File -&gt; New File -&gt; R Script Then save your script by going to File -&gt; Save As. a. Make sure to save your file with the .R suffix. b. Save your script in your project folder, otherwise youll run into issues. Weve also provided an example script in the R4EnvChem project template. Assuming youre currently in the template project you can open the script as follows: Go to File -&gt; Open File -&gt; open the Rscript-example.R file. This will open a new pane above the console dedicated to writting your script(s). To run code, highlight the lines you wish to run and click Run at the top of the script. Selecting all of the script will run every line. Youll see the lines selected lines appear in the console, followed by the outputs and messages if there are any. 4.2 Workspace and whats real Weve already mentioned the environment pane that displays objects present in your R session. While they are useful to work with, theyre not real. That is to say, if you closed your R session, those objects would be lost. And while RStudio allows you to save a working environment (and its associated objects), its best to embrace that only your scripts are real. You cant readily share your working environment, and even so its bad practice as you may reference a previous iteration of an object giving you erroneous results. Think back to the chemistry labs: you may jot notes down on loose leaf, but only whats written in your lab book is considered real well thats how its supposed to work anyways. The idea is everything you need can be generated from the original data and the instructions in your script Anyone should be able to take your data and your code and get the same results you got. This is paramount for the reproducibility of your work and your results. 4.3 Saving R scripts You can save an R script to a .r file by going to File-&gt;Save or by clicking the save button in the top left of your script. Code saved to a .r file is considered real. Variables, plots, or data sets that only exist in your work-space (shown in the Environment window) are not. Whenever you close RStudio, any objects in R that are not considered real will be lost in that R session. Furthermore when you need to share your code (for school or publication) youll need to share your data and your script, but never your work-space. This is to increase predictability and helps people (and you) to make sure your work is reproducible, an under appreciate hallmark of science. 4.3.1 What should I save? At this point in the chapter, two things should be clear: R scripts saved to .R files are real. Objects in your work-space/environment are not real, and will not be available to you after you close and re-open RStudio unless you re-run the code used to generate the work-space. So what is important to save in R, and how often should you save these files? It is paramount that you save the scripts you code in, and that you save them regularly. Even if youve made small notation changes to the code, it is always a good idea to save your changes to the script before closing RStudio, as there is a good chance you will not remember the minor differences upon returning. You want to make sure that even if you lose an object in your environment, your script still contains the code you used to generate that object. You also want to make sure that you generate the object before you call it in part of another command, so that when you run your scripts from top-to-bottom, the variables are generated in the work-space before they are referenced by later commands. 4.3.2 Saving objects In some cases, your code may be used to generate large datasets which require quite a bit of time to create. It can be quite tedious to re-run the code used to generate these large data sets every time you open RStudio, and you might find yourself wanting to save the data to a real file that you can simply import the next time you open the application. Also, you may be finished with your analysis and want to save the final data. You can save your the data contained in your data frame as a .csv file using write.csv(). # dummy data frame to save df &lt;- data.frame(x = c(1,2,3), y = c(&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;)) write.csv(x = df, file = &quot;testData.csv&quot;) Breaking it down: we created a dummy data frame df; in reality youll already use a data frame from your analysis. we called write.csv() and x = df specifies we want to save the data.frame df file = \"data/testData.csv\" specifies where we want the file to save (in the data sub-directory, more later), and what our file will be called (testData.csv). Its important to specify the file extension so R knows how to save it. 4.4 Script formatting You should now be familiar with how to open the Scripts window, as well as some of the advantages of typing your code into this window rather than into the console directly. Before you write your first script, lets review some basic script formatting. Before you enter any code into your script, it is good practice to fill the first few lines with text comments which indicate the scripts title, author, and creation or last edit date. You can create a comment in a script by typing # before any text. An example is given below. #Title: Ozone time series script #Author: Georgia Green #Date: January 8, 2072 Below your script header, you should include any packages that need to be loaded for the script to run. Including the necessary packages at the top of the script allows you, and anyone you share your code with, to easily see what packages they need to install. This also means that if you decide to run an entire script at once, the necessary packages will always be loaded before any subsequent code that requires those packages to work. The first few lines of your scripts should look something like the following. # Title: Example R Script for Visualizing Air Quality Data # Author: John Guy Rubberboots # Date: 24 June 2021 # 1. Packages ---- # Install tidyverse if you haven&#39;t already #install.packages(&quot;tidyverse&quot;) library(tidyverse) The rest of your script should be dedicated to executable code. It is good practice to include text comments throughout the script, and in between different chunks of code to remind yourself what the different sections of code are for (i.e., # 1. Packages ---- in the above example). This also makes it easy for anyone you share your code with to understand what youre trying to do with different sections within the script. You can also use headers and sub-headers in your scripts using #, ##, and ### before your text and --- after as shown below: # Section ---- ## Subsection ---- ### Sub-subsection ---- Headings and subheadings are picked up by RStudio and displayed in the Document Outline box. You can open the Document Outline box by clicking the button highlighted in the image below. Use of these headings allows easy navigation of long scripts, as you can navigate between sections using the Document Outline box. Example script headings, document outlines, and comments. Note the  which tells RStudio this comment is to be treated as a script heading. 4.5 Viewing data and code simultaneously Before we get into more about coding and workflows, you may find yourself wanting to be able to to view your scripts and data side-by-side. You can open a script, plot, or data set in a new window by clicking and dragging the tab in RStudio or by clicking the button highlighted in the image below. How to open an R script/plot/data set in a new window. 4.6 Troubleshooting error messages In the previous section, you were introduced to your first error message in R, and we briefly discussed how to resolve the issue. As you begin to code, many of your errors will be routine syntax error such as unmatched parenthesis (the dreaded Incomplete expression:). Fortunately, RStudio will highlight any syntax errors in your code with a red squiggly line and an x in the side bar, as shown below. You can hover over the x to see what is causing the error. Figure 3.8: RStudio highlights syntax errors in the Scripts window. In the above message, R is telling you that it is not sure what to do with b. As mentioned previously, variable assignment is done in the format name &lt;- assignment. However, in the above example, the variable assignment statement is written as name name &lt;- assignment. Since variable names cannot contain spaces, R reads a b as two separate input variable names, not as a single string. If you wanted to assign a value of 0 to both a and b, you would need to write the statement once per variable, as shown below. a &lt;- 0 b &lt;- 0 Lets look at another example. Some functions require you to write code with nested parentheses. A good example would be the aes() argument that is called inside of ggplot(), as shown below. #plot ozone concentration vs. time ggplot(data = airPol, aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() (For more detail about importing and using ggplot2, please re-visit Chapter 2, section 2.3.4, or see Chapter 11.) If you were to forget one of the parentheses in the previous line of code, RStudio would highlight it similar to below: Figure 3.9: RStudio highlights unmatched parentheses in the script window. Here R is telling you that you have an unmatched opening bracket. To resolve the error, simply add a closing bracket to match. The expected ',' after expression is a common error that you will see accompanying unmatched opening brackets. Sometimes you might get this error in the console after running code that is missing a bracket somewhere. It is good practice to check your parentheses a few times before running your code to make sure that all the commands are closed, and that R doesnt keep waiting for you to continue inputting code after youve click Run. If you notice that the &gt; in your R console has turned into a +, this is likely because youve just run a command that is missing a closing bracket, and thus, R is not aware that your code is finished. Simply input a closing bracket into the console, and the &gt; should return. While the script window is very useful for pointing out syntax errors in your code, there are many other errors that can arise in RStudio which the script window is not able to capture. These are generally errors that arise from trying to execute your code, rather than from mistakes in your syntax. The following is a prime example of such an error. q &lt;- 8 + &quot;hi&quot; ## Error in 8 + &quot;hi&quot;: non-numeric argument to binary operator Here we are trying to add a numeric value (8) to a character string (hi), then set the sum of the two to variable q. R has given us an error in return, because there is no logical way for R to add a numeric value to non-numeric text. The error indicates that we have passed a non-numeric argument to binary operator, meaning we have used a non-numeric data type for an expression which is exclusively reserved for numeric data. It is important to be aware of error codes as many functions require specific data types as their inputs. You can alwways consult the function documentation by via the Help tab of the Viewer pane or by typing a ? followed by the name of the function in the console (i.e. ?ggplot). 4.7 Summary In this chapter weve covered: R workflows in the context of projects and scripts Whats considered real when working in RStudio How to format your script for legibility (Remember youre the one whos going to be stuck rereading it!) Troubleshooting some common error messages Now that youre familiar the above, well introduce Using R Markdown, a way to combine your R code, its outputs, and your writing all in one dynamic document (like your lab reports!). "],["using-r-markdown.html", "Chapter 5 Using R Markdown 5.1 Getting started with rmarkdown 5.2 Compiling your final report 5.3 Authoring with rmarkdown 5.4 R Markdown resources", " Chapter 5 Using R Markdown In a nutshell, R Markdown allows you to analyse your data with R and write your report in the same place (this entire book was written with rmarkdown). This has loads of benefits including a reproducible workflow, and streamlined thinking. No more flipping back and forth between coding and writing to figure out whats going on. Lets run some simple code as an example: # Look at me go mom x &lt;- 2+2 x ## [1] 4 What weve done here is write a snippet of R code, ran it, and printed the results (as they would appear in the console). While the above code isnt anything special, we can extend this concept so that our R markdown document contains any data, figures or plots we generate throughout our analysis in R. For example: library(tidyverse) library(knitr) airPol &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ggplot(data = airPol, aes(x = date.time, y = concentration, colour = pollutant)) + geom_line() + theme_classic() Figure 5.1: Time series of 2018 ambient atmospheric O3, NO2, and SO2 concentrations (ppb) in downtown Toronto sumAirPol &lt;- airPol %&gt;% drop_na() %&gt;% group_by(city, naps, pollutant) %&gt;% summarize(mean = mean(concentration), sd = sd(concentration), min = min(concentration), max = max(concentration)) knitr::kable(sumAirPol, digits = 1) city naps pollutant mean sd min max Toronto 60430 NO2 20.5 11.5 7 55 Toronto 60430 O3 19.7 8.7 1 33 Toronto 60430 SO2 1.1 0.3 1 3 Pretty neat, eh? You might not think so, but lets imagine a scenario youll encounter soon enough. Youre about to submit your assignment, youve spent hours analyzing your data and beautifying your plots. Everything is good to go until you notice at the last minute you were supposed to subtract value x and not value y in your analysis. If you did all your work in Excel (tsk tsk), youll need to find the correct worksheet, apply the changes, reformat your plots, and import them into word (assuming everything is going well, which is never does with looming deadlines). Now if you did all your work in R markdown, you go to your one .rmd document, briefly apply the changes and re-compile your document. 5.1 Getting started with rmarkdown As youve already guessed, R markdown documents use R and are most easily written and assembled in RStudio. If you have not done so, revisit Chapter 1:Installing R. Once setup with R and R Studio, youll need to install the rmarkdown and tinytex packages by running the following code in the console: # These are large packages so it&#39;ll take a couple of minutes to install install.packages(&quot;rmarkdown&quot;) # downloaded from CRAN install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() # install TinyTeX The rmarkdown package is what well use to generate our documents, and the tinytex package enables compiling documents as PDFs. Theres a lot more going on behind the scenes, but you shouldnt need to worry about it. Now that everything is setup, you can create your first R Markdown document by opening up R Studio, selecting FILE -&gt; NEW FILE -&gt; Rmarkdown. A dialog box will appear asking for some basic input parameters for your R markdown document. Add your title and select PDF as your default output format (you can always change these later if you want). A new file should appear thats already populated with some basic script illustrating the key components of an R markdown document. 5.1.1 Understanding rmarkdown Your first reaction when you opened your newly created R markdown document is probably that it doesnt look anything at all like something youd show your prof. Youre right, what youre seeing is the plain text code which needs to be compiled (called knit in R Studio) to create the final document. When you create a R markdown document like this in R Studio a bunch of example code is already written. You can compile this document (see below) to see what it looks like, but lets break down the primary components. At the top of the document youll see something that looks like this: --- title: &quot;Temporal Analysis of Foot Impacts While Birling Down the White Water&quot; author: &quot;Jean Guy Rubberboots&quot; date: &quot;24/06/2021&quot; output: pdf_document --- This section is known as the preamble and its where you specify most of the document parameters. In the example we can see that the document title is Temporal Analysis of Foot Impacts While Birling Down the White Water, its written by Jean Guy Rubberboots, on the 24th of June, and the default output is a PDF document. You can modify the preamble to suit your needs. For example, if you wanted to change the title you would write title: \"Your Title Here\" in the preamble. Note that none of this is R code, rather its YAML, the syntax for the documents metadata. Apart from whats shown you shouldnt need to worry about this much, just remember that indentation in YAML matters. Reading further down the default R markdown code, youll see different blocks of text. In R markdown anything you write will be interpreted as body text (i.e. stuff like this that you want folks to read) in the knitted document. To actually run R code youll need to see the next section. 5.1.2 Running code in rmarkdown Theres two ways to write R code in markdown: Setup a code chunk. Code chunks start with three back-ticks like this: ```{r}, where r indicates youre using the R language You end a code chunk using three more backticks like this ```. Specify code chunks options in the curly braces. i.e. ```{r, fig.height = 2} sets figure height to 2 inches. See the Code Chunk Options section below for more details. Inline code expression, which starts with `r and ends with ` in the body text. Earlier we calculated x &lt;- 2 + 2, we can use inline expressions to recall that value (ex. We found that x is 4) To actually run your R code you have two options. The first is to run the individual chunks using the Run current chunk button (See figure 2). This is a great way to tinker with your code before you compile your document. The second option is to compile your entire document using the Knit document button (see Figure 2). Knitting will sequentially run all of your code chunks, generate all the text, knit the two together and output a PDF. Youll basically save this for the end. Note all the code chunks in a single markdown document work together like a normal R script. That is if you assign a value to a variable in the first chunk, you can call this variable in the second chunk; the same applies for libraries. Also note that every time you compile a markdown document, its done in a fresh R session. If youre calling a variable that exist in your working environment, but isnt explicitly created in the markdown document youll get an error. How this document, the one youre reading, appeared in RStudio; to see the final results scroll up to the first figure. Note the knit and run current chunk buttons. 5.2 Compiling your final report To create a PDF to hand in, youll need to compile, or knit, your entire markdown document as mentioned above. To knit your R markdown script, simply click the knit button in R Studio (yellow box, Figure 2). You can specify what output you would like and R Studio will (hopefully) compile your script. Remember, you can test or run individual code chunks as outlined in Running code in rmarkdown. 5.3 Authoring with rmarkdown Below is a brief summary of the major elements required to author an rmarkdown document. They shoudl address the majority of your needs, but please see the R Markdown resources for more information. 5.3.1 R markdown syntax Unlike Microsoft Word, RMarkdown utilizes a specific syntax for text formatting. Once you get used to it, it makes typing documents much easier than Words approach. The table below is how some of the most common text formatting is typed in your Rmarkdown document (syntax &amp; example column) and how itll appear in the final output. Text formatting syntax Example Example output italics *text* this is *italics* this is italics bold **text** this is **bold** this is bold subscript ~text~ this is ~subscript~ this is subscript superscript ^text^ this is ^superscript^ this is superscript monospace `text` this is `monospaced` this is monospace For a collection of other RMarkdown syntax, please see the useful (and brief) list compiled online here. This includes ordered and unordered lists, headers, code blocks, hyperlinks, and figure captions. 5.3.2 R code chunk options Your R code is run in chunks and the results will be embedded in the final output file. To each chunk you can specify options thatll effect how youre code chunk is run and displayed in the final output document. You include options in the chunk delimiters ```{r} and ```. For example the following code tells markdown youre running code written in R, that when you compile your document this code chunk should be evaluated, and that the resulting figure should have the caption Some Caption. ```{r, eval = FALSE, fig.cap = &quot;Some caption&quot;} # some code to generate a plot worth captioning. ``` The most common and useful chunk options are shown below. Note that they all have a default value. For example, eval tells R markdown whether the code within the block should be run. Its default option is TRUE, so by default any code in a chunk will be run when you knit your document. If you dont want that code to be run, but still displayed, you would set eval = FALSE. Another example would be setting echo = FALSE which allows the code to run, but the code wont be displayed on the output document (the outputs will still be displayed though); useful for creating clean documents with plots only (i.e. lab reports). option default effect eval TRUE whether to evaluate the code and include the results echo TRUE whether to display the code along with its results warning TRUE whether to display warnings error FALSE whether to display errors message TRUE whether to display messages tidy FALSE whether to reformat code in a tidy way when displaying it fig.width 7 width in inches for plots created in chunk fig.height 7 height in inches for plots created in chunk fig.cap NA include figure caption, must be in quotation makrs (\"\") 5.3.3 Inserting images Images not produced by R code can easily be inserted into your document. The markdown code isnt R code, so between paragraphs of bodytext insert the following code. ![Caption for the picture.](path/to/image.png){width=50%, height=50%} Note that in the above the use of image atributes, the {width=50%, height=50%} at the end. This is how youll adjust the size of your image. Other dimensions you can use include px, cm, mm, in, inch, and %. A final note on images: when compiling to PDF, the LaTeX call will place your image in the optimal location (as determined by LaTeX), so you might find your image isnt exactly where you though it would be. A more in-depth guide to image placement can be found here 5.3.4 Generating tables There are multiple methods to create tables in R markdown. Assuming you want to display results calculated through R code, you can use the kable() function. Or you can consult the Summarizing Data chapter for making publication ready tables. Alternatively, if you want to create simple tables manually use the following code in the main body, outside of an R code chunk. You can increase the number of rows/columns and the location of the horizontal lines. To generate more complex tables, see the kable() function and the kableExtra package. Header 1 | Header 2| Header 3 ---------|---------|---------| Row 1 | Data | Some other Data Row 2 | Data | Some other Data ---------|---------|---------| Header 1 Header 2 Header 3 Row 1 Data Some other Data Row 2 Data Some other Data 5.3.5 Spellcheck in R Markdown While writing an R markdown document in R studio, go to the Edit tab at the top of the window and select Check Spelling. You can also use the F7 key as a shortcut. The spell checker will literally go through every word it thinks youve misspelled in your document. You can add words to it so your spell checkers utility grows as you use it. Note that the spell check may also check your R code; be wary of changing words in your code chunks because you may get an error down the line. 5.3.6 Exporting Rmarkdown documents Youll most likely be exporting your rmarkdown documents as PDFs, but the beauty of rmarkdown is it dosent stop there. Your rmarkdown documents can be knitted as a HTML document, a book (or both like this book!). You can even make slideshow presentations and yes, if need be, export as a word document that you can open in Microsoft Word. You specify the output format in the document header. To specify you want your document to be outputted as a PDF your YAML header would look like this: --- title: &quot;Your title here&quot; output: pdf_document --- Here are some links to different output formation available in R Markdown and how to use them: pdf_document creates a PDF document via Latex; probably your defacto output. word_document creates a Word document. Note that the formatting options are pretty basic, so while evereything will be where you want it to be, youll need to pretty it up in Word to comply with your instructors specifications. tufte_handout for a PDF handout in the style of Edward Tufte. Check it out. ioslides_presentation, revealjs::revealjs_presentation, and powerpoint_presentation are all options to create sldieshow presentations. I personally use revealjs for my own work. It has the steepest learning curve of the bunch, but once setup, you can make incredibly slick slides with ease. Note: like word_document, powerpoint_presentations outputs are stylistically simple. Youll definitly need to pretty them up manually in Powerpoint. 5.3.7 RStudio tips and tricks To further the usefulness of rmarkdown, the latest release of RStudio has a Visual R Markdown editor which introduces many useful features for authoring documents in rmarkdown. Some of the most pertinent are: Visual editor so you can see how your document looks (top left of script pane) Combining Zotero and RStudio for easy citatiosn of your document (read more here 5.4 R Markdown resources Theres a plethora of helpful online resources to help hone your R markdown skills. Well list a couple below (the titles are links to the corresponding document): Chapter 2 of the R Markdown: The Definitive Guide by Xie, Allair &amp; Grolemund (2020). This is the simplest, most comprehensive, guide to learning R markdown and its available freely online. The R markdown cheat sheet, a great resource with the most common R markdown operations; keep on hand for quick referencing. Bookdown: Authoring Books and Technical Documents with R Markdown (2020) by Yihui Xie. Explains the bookdown package which greatly expands the capabilities of R markdown. For example, the table of contents of this document is created with bookdown. "],["r-tutorial-exercise.html", "Chapter 6 R Tutorial Exercise 6.1 Task overview 6.2 Closing thoughts", " Chapter 6 R Tutorial Exercise With the information presented in Section 1 you have the skills to start your data analysis. Weve created a brief tutorial that covers the major elements introduced. At the end of this tutorial youll have visualized a small subset of real Environment and Climate Change Canada (ECCC) National Airborne Pollution Surveillance Program (NAPS) data. More importantly, youll have a properly setup project with working code and rmarkdown documents that you can recycle and re-purpose for your upcoming course work. After all, a beautiful aspect of coding is recycling it in future work to save you hassle. Youll need to install the following packages if you havent already (you can copy the code below and run it in the console): tidyverse rmarkdown tinytex: needed to generate PDF files, more info here install.packages(&quot;tidyverse&quot;) install.packages(&quot;rmarkdown&quot;) install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() # This will take ~5 mins, so grab a coffee If you already have the R4EnvChem Project downloaded from Importing a project from the RStudio Projects chapter, then simply open the file Rmarkdown-example.Rmd and follow the steps listed there. If not, refer to [Importing the R4EnvChem Project] for download instructions. 6.1 Task overview This tutorial exercise will require you to make modifications to an existing R Markdown file in order to produce a very basic lab report template you can use for your future reports. You will draw on the skills developed over the course of Section 1 to: Modify file paths to import data and images Customize ggplot and markdown code to produce 2 different captioned plots based on that data Compile a PDF version of the report template containing your plots for submission 6.2 Closing thoughts Its taken a lot of work to get here, but once you finish this exercise and generate your own markdown file analyzing your selected dataset youll be well on your way to tackling the upcoming course labs/work. Youll also have the following resources to help you on your R journey: An RStudio project which you can copy and reuse for your future projects. An rmarkdown template, similar to the reports youd hand in during class. Sample R code showcasing the basics of ggplot. "],["intro-to-data-wrangling.html", "Chapter 7 Intro to Data Wrangling 7.1 Example Data 7.2 Further Reading", " Chapter 7 Intro to Data Wrangling This section will teach you how to use R to import and organize your data as part of a cohesive data analysis workflow. Whether it takes 10 minutes or 10 hrs, youll use this workflow for every data analysis project. By explicitly understanding the workflow steps, and how to execute them in R, youll be more than capable of expanding the limited tools learned from this book to any number of data analysis projects youll soon encounter. The explicit workflow well be teaching was originally described by Wickham and Grolemund, and consists of six key steps: Data science workflow describes by Wickham and Grolemund; image from R for Data Science, Wickham and Grolemund (2021) Import is the first step and consist of getting your data into R. Seems obvious, but doing it correctly will save you time and headaches down the line. Tidy refers to organizing your data in a tidy manner where each variable is a column, and each observation a row. This is often the least intuitive part about working with R, especially if youve only used Excel, but its critical. If you dont tidy your data, youll be fighting it every step of the way. Transform is anything you do to your data including any mathematical operations or narrowing in on a set of observations. Its often the first stage of the cycle as youll need to transform your data in some manner to obtain a desired plot. Visualize is any of the plots/graphics youll generate with R. Take advantage of R and plot often, its the easiest way to spot an error. Model is an extension of mathematical operations to help understand your data. The linear regressions needed for a calibration curve are an example of a model. Communicate is the final step and is where you share the knowledge youve squeezed out of the information in the original data. Import, Tidy, and Transformation go hand-in-hand in a process called wrangling. Wrangling is all of the steps needed to get your data ready for analysis. Its often the most tedious and frustrating, hence wrangling (its a fight), but once done make the subsequent cycle of understanding your data via transformation, visualizations, and modelling much easier and more predictable. In this section well cover data wrangling before moving onto the next section where well explore data analysis as it pertains to environmental chemistry. 7.1 Example Data Throughout this section and the next well be making use of a couple of example datasets. These datasets are all availabe in the data subfolder of the R4EnvChem Project Template. If you havent already, read Importing a project for instructions on dowloading the repository and data. 7.2 Further Reading In case it hasnt been apparent enough, this entire endeavour was inspired by the R for Data Science reference book by Hadley Wickham and Garrett Grolemund. Every step described above is explored in more detail in their book, which can be read freely online at https://r4ds.had.co.nz/. We strongly encourage you to read through the book to supplement your R data analysis skills. "],["importing-your-data-into-r.html", "Chapter 8 Importing Your Data Into R 8.1 .csv files 8.2 read_csv 8.3 Importing other data types 8.4 Saving data 8.5 Further Reading", " Chapter 8 Importing Your Data Into R Unlike Excel, you cant copy and paste your data into R (or RStudio). Instead you need to import your data into R so you can work with it. This chapter will discuss how your data is stored, and how to import it into R (with some accompanying nuances). 8.1 .csv files While there are a myriad of ways data is stored, raw instrument often record results in a proprietary vendor format, the data youre likely to encounter in an undergraduate lab will be in the form of a .csv or comma-separated values file. As the name implies, values are separated by commas (go ahead and open any .csv file in any text editor to observe this). Essentially you can think of each line as a row and commas as separating values into columns, which is exactly how R and Excel handle .csv files. 8.2 read_csv Importing a .csv file into R simply requires the read.csv or the read_csv function from tidyverse. The first variable is the most important as its the file path. Recall that R, unless specified, uses relative referencing. So in the example below were importing the ATR_plastics.csv from the data sub-folder in our project by specifying \"data/ATR_plastics.csv\" and assigning it to the variable atr_plastics. Note the inclusion of the file extension. atr_plastics &lt;- read_csv(&quot;data/ATR_plastics.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## wavenumber = col_double(), ## EPDM = col_double(), ## Polystyrene = col_double(), ## Polyethylene = col_double(), ## `Sample: Shopping bag` = col_double() ## ) A benefit of using read_csv is that it prints out the column specifications with each columns name (how youll reference it in code) and the column value type. Columns can have different data types, but a data type must be consistent within any given column. Having the columns specifications is a good way to ensure R is correctly reading your data. The most common data types are: int for integer values (-1,1, 2, 10, etc.) dbl for doubles (decimals) or real numbers (-1.20, 0.0, 1.200, 1e7, etc.) chr for character vectors or strings (A, chemical, Howdy maam, etc.) note numbers can be encoded as strings, so while you might read 1 as a number, R treats it as a character, limiting how you can use this value. lgl for logical values, either TRUE or FALSE We can also quickly inspect either through the Environment pane in RStudio or quickly with the head() function. Note the column specifications under the column name. head(atr_plastics) ## # A tibble: 6 x 5 ## wavenumber EPDM Polystyrene Polyethylene `Sample: Shopping bag` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.0746 0.000873 0.0236 ## 2 551. 0.212 0.0746 0.000834 0.0238 ## 3 551. 0.213 0.0745 0.000819 0.0239 ## 4 552. 0.213 0.0745 0.000825 0.0239 ## 5 552. 0.214 0.0745 0.000868 0.0240 ## 6 553. 0.214 0.0746 0.000949 0.0240 Note how the first line of the ATR_plastics.csv has been interpreted as columns names (or headers) by R. This is common practice, and gives you a handle by which you can manipulate your data. If you did not intend for R to interpret the first row as headers you can suppress this with the additional argument col_names = FALSE. head(read_csv(&quot;data/atr_plastics.csv&quot;, col_names = FALSE)) ## ## -- Column specification -------------------------------------------------------- ## cols( ## X1 = col_character(), ## X2 = col_character(), ## X3 = col_character(), ## X4 = col_character(), ## X5 = col_character() ## ) ## # A tibble: 6 x 5 ## X1 X2 X3 X4 X5 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 wavenumber EPDM Polystyrene Polyethylene Sample: Shopping bag ## 2 550.0952 0.2119556 0.07463058 0.000873196 0.02364882 ## 3 550.5773 0.2124079 0.07455246 0.000834192 0.02382648 ## 4 551.0594 0.2128818 0.07450471 0.000819447 0.02387163 ## 5 551.5415 0.2133267 0.07449704 0.000825491 0.02391921 ## 6 552.0236 0.2137241 0.07452058 0.000868397 0.02396947 Note in the example above that since the headers are now considered data, and are composed of a string of chatacters, the entire column is then interpreted as character values. This will happen if a single non-numeric character is introduced in the column, so beware of typos when recording data! If we wanted to skip rows (i.e. to avoid blank rows at the top of our .csv), we can use the skip = n to skip n rows: head(read_csv(&quot;data/atr_plastics.csv&quot;, col_names = FALSE, skip = 1)) ## ## -- Column specification -------------------------------------------------------- ## cols( ## X1 = col_double(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_double(), ## X5 = col_double() ## ) ## # A tibble: 6 x 5 ## X1 X2 X3 X4 X5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 550. 0.212 0.0746 0.000873 0.0236 ## 2 551. 0.212 0.0746 0.000834 0.0238 ## 3 551. 0.213 0.0745 0.000819 0.0239 ## 4 552. 0.213 0.0745 0.000825 0.0239 ## 5 552. 0.214 0.0745 0.000868 0.0240 ## 6 553. 0.214 0.0746 0.000949 0.0240 Note in the example above that we skipped our headers, so read_csv() created placeholder headers (X1, X2, etc.). 8.2.1 Tibbles vs. data frames Quick eyes will notice the first line outputted above is # A tibble: 6 x 5. tibbles are a variation of data.frames introduced in section one, but built specifically for the tidyverse family of packages. While data.frames and tibbles are often interchangeable, its important to be aware of the difference in case you do run into a rare conflict. In these situations you can readily transform a tibble into a data.frame by coercion with the as.data.frame() function, and vice-versa with the as_tibble() function. class(as.data.frame(atr_plastics)) ## [1] &quot;data.frame&quot; 8.3 Importing other data types There are other functions to import different types of tabular data which all function like read_csv, such as read_tsv for tab-separate value files (.tsv) and read_excel and read_xlsx from the readxl package to import Excel files. Note most Excel files have probably been formatted for legibility (i.e. merged columns), which can lead to errors when importing into R. If you plan on importing Excel files, its probably best to open them in Excel to remove any formatting, and then save as .csv for smoother importing into R. 8.4 Saving data As you progress with your analysis you may want to save intermediate or final datasets. This is readily accomplished ussing the write_csv() from the readr package. Similar rules apply to how we used read_csv, but now the second argument specifies the save location and file name, while the first argument is which tibble/data.frame were saving. Note that R will not create a folder this way, so if youre saving to a sub-folder youll have to make sure it exists or create it yourself. write_csv(atr_plastics, &quot;data/ATRSaveExample.csv&quot;) A benefit of write_csv is that it will always save in UTF-8 encoding and ISO8601 time format. This standardization makes it easier to share your .csv files with collaborators/yourself. 8.5 Further Reading See Chapters 10 and 11 of R for Data Science for some more details on tibbles and read_csv. "],["tidying-your-data.html", "Chapter 9 Tidying Your Data 9.1 What is tidy data? 9.2 Tools to tidy your data 9.3 Tips for recording data 9.4 Further reading 9.5 Chapter References", " Chapter 9 Tidying Your Data You might not have explicitly thought about how you store your data, whether working in Excel or elsewhere. Data is data after all. But having your data organized in a systematic manner that is conducive to your goal is paramount for working not only with R, but all of your experimental data. This chapter will introduce the concept of tidy data, and how to use some of the tools in the dplyr package to get there. Lastly well offer some tips for how you should record your data in the lab. A bit of foresight and consistency can eliminate hours of tedious work down the line. 9.1 What is tidy data? Tidy data has each variable in a column, and each observation in a row (Wickham 2014) This may seem obvious to you, but lets consider how data is often recorded in lab, as exemplified in Figure 9.1A. Here the instrument response of two chemicals (A and B) for two samples (blank and unknown) are recorded. Note how the samples are on each row and the chemical are columns. However, someone else may record the same data differently as shown in Figure 9.1B, with the samples occupying distinct columns, and the chemical rows. Either layout may work well, but analyzing both would require re-tooling your approach. This is where the concept of tidy data comes into play. By reclassifying our data into observations and variables we can restructure out data into a common format: the tidy format (Figure 9.1C). Figure 9.1: (A and B) The same data can be recorded in multiple formats. (C) The same data in the tidy format. Note how the tidy data typically has more rows, hence why its sometimes refered to as long data. In the tidy or long format, we reclassified out data into three variables (Sample, Chemical, and Reading). This makes the observations clearer as now we know we measured two chemicals (A and B) in two samples (blank and unknown) and weve explicitly declared the Reading variable for our measured instrument response, which was only implied in the original layouts. Moreover, we can read across a row to get the gist of one data point (i.e. Our blank has a reading of 0 for Chemical A). Again we havent changed any information, weve simply reorganized our data to be clearer, consistent, and compatible with the tidyverse suit of tools. This might seem pedantic now, but as you progress youll want to reuse code youve previously written. This is greatly facilitated by making every data set as consistently structured as possible, and the tidy format is an ideal starting place. 9.2 Tools to tidy your data Now one of the more laborious parts of data science is tidying your data. If you can follow the tips in the Tips for recording data section, but the truth is you often wont have control. To this end, the tidyverse offers several tools, notable dplyr (pronounces d-pliers), to help you get there. Lets revisit our spectroscopy data from the previous chapter: atr_plastics &lt;- read_csv(&quot;data/ATR_plastics.csv&quot;) # This just outputs a table you can explore within your browser DT::datatable(atr_plastics) As we can see this our ATR spectroscopy results of several plastics, as recorded for a CHM 317 lab, is structured similarly to the example in Figure 9.1A. The ATR absorbance spectra of the four plastics are recorded in separate columns. Again, this format makes intuitive sense when recording in the lab, and for working in Excel, but isnt the friendliest with R. When making plots with ggplot, we can only specify one y variable. In the example plot below its the absorbance spectrum of Polystyrene. However, if wanted to plot the other spectra for comparison, wed need to repeat our geom_point call. # Plotting Polystyrene absorbance spectra ggplot(data = atr_plastics, aes( x = wavenumber, y = Polystyrene)) + geom_point() # Plotting Polystyrene and Polyethylene absorbance spectra ggplot(data = atr_plastics, aes( x = wavenumber, y = Polystyrene)) + geom_point() + geom_point(data = atr_plastics, aes(x = wavenumber, y = Polyethylene)) 9.2.1 Making data longer While the code above works, its not particularly handy and undermines much of the utility of ggplot because the data isnt tidy. Fortunately the pivot_longer function can easily restructure our data into the long format to work with ggplot. Lets demonstrate that: atr_long &lt;- atr_plastics %&gt;% pivot_longer(cols = -wavenumber, names_to = &quot;sample&quot;, values_to = &quot;absorbance&quot;) # head() only prints the first couple of lines head(atr_long) ## # A tibble: 6 x 3 ## wavenumber sample absorbance ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 550. EPDM 0.212 ## 2 550. Polystyrene 0.0746 ## 3 550. Polyethylene 0.000873 ## 4 550. Sample: Shopping bag 0.0236 ## 5 551. EPDM 0.212 ## 6 551. Polystyrene 0.0746 Lets break down the code weve executed via the pivot_longer function: cols = -wavenumber specifies that were selecting every other column but wave number. we could have just as easily specified each column individually using cols = c(\"EPDM\",...) but its easier to use - to specify what we dont want to select. names_to = \"sample\" specifies that the column header (i.e. names) be converted into an observation under the sample column. values_to = \"absorbance\" specifies that the absorbance values under each of the selected headers be placed into the aborsbance column. Now that weve reclassified out data into the longer, we can exploit the explicitly introduced sample variable to easily plot all of our spectra: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance, colour = sample) ) + geom_point() Well talk more about ggplot in the Visualizations chapter, but for now you can see how our code could scale to accommodate any number of different samples, whereas the previous attempt to plot the wide data would require an explicit call to each column. pivot_longer has many other features that you can take advantage of. We highly recommend reading the examples listed on the pivot_longer page to get a better sense of the possibilities. For example its common to record multiple observations in a single column header, i.e. Chemical_A_0_mM. We can exploit common naming conventions like this to easily split up these observations as shown below. head(example) ## wavelength_nm Chemical_A_0_mM Chemical_A_1_mM Chemical_B_0_mM Chemical_B_1_mM ## 1 488 0 1 2 NA ## 2 572 0 5 7 20 example_long &lt;- example %&gt;% pivot_longer( cols = starts_with(&quot;Chemical&quot;), names_prefix = &quot;Chemical_&quot;, names_to = c(&quot;Chemical&quot;, &quot;Concentration&quot;, &quot;Conc_Units&quot;), names_sep = &quot;_&quot;, values_to = &quot;Absorbance&quot;, values_drop_na = TRUE ) head(example_long) ## # A tibble: 6 x 5 ## wavelength_nm Chemical Concentration Conc_Units Absorbance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 488 A 0 mM 0 ## 2 488 A 1 mM 1 ## 3 488 B 0 mM 2 ## 4 572 A 0 mM 0 ## 5 572 A 1 mM 5 ## 6 572 B 0 mM 7 9.2.2 Making data wider Sometimes packages or circumstances will require you reformat your data into a matrix or wide format (notable the matrixStats and matrixTests packages). You can accomplish this using the pivot_wider function, which operates inverse to the pivot_longer function described above. For example the input names_from is used to specify which variables are to be converted to headers. You can read up on the pivot_wider function here 9.2.3 Selection helpers As youve already seen, there are multiple ways to select columns and variables with the dplyr package. For a complete rundown of other useful helper functions please see Subset columns using their names and types. starts_with() for selecting columns from a prefix, and contains() for selecting columns that contain a string are two of the most useful. 9.2.4 Seperating columns Sometimes your data has already been recorded in a tidy-ish fashion, but there may be multiple observations recorded under one apparent variable, something like 1 mM for concentration. As it stands we cannot easily access the numerical value in the concentration recording because R will encode this as a string due to the mM. We can separate data like this using the seperate function, which operates similarly to how pivot_longer breaks up headers. # Example with multiple encoded observations sep_example ## sample reading ## 1 Toronto_O3_1 10 ## 2 Toronto_O3_2 22 ## 3 Toronto_NO2_1 30 The example above is something youll come across in the lab, most often with the sample names youll pass along to your TA where you crammed as much information as possible into that name so you and your TAs know exactly whats being analyzed. In this example, the sample name contains the location (Toronto), the chemical measured (O3 or NO2) and the replicate number (i.e. 1). Using the seperate function we can split up these three observations so we can properly group our data later on in our analysis. # Separating observations sep_example %&gt;% separate( col = sample, into = c(&quot;location&quot;, &quot;chemical&quot;, &quot;replicateNum&quot;), sep = &quot;_&quot;, remove = TRUE, convert = TRUE) ## location chemical replicateNum reading ## 1 Toronto O3 1 10 ## 2 Toronto O3 2 22 ## 3 Toronto NO2 1 30 Again, lets break down what we did with the separate function: col = sample specifies were selecting the sample column into = c(...) specifies what columns were separating our name into. sep = \"_\"1 specifies that each element is separated by an underscore (_); you can use sep = \" \" if they were separated by spaces. remove = TRUE removes the original sample column, no need for duplication; setting this to FALSE would keep the original column. convert = TRUE converts the new columns to the appropriate data format. In the original column ,the replicate number is a character value because its part of a string, convert ensures that itll be converted to a numerical value. Another example why its paramount to be consistent when recording data. 9.2.5 Uniting/combining columns The opposite of the separate function is the unite function. Youll use it far less often, but you should be aware of it as it may come in handy. You can use it for combining strings together, or prettying up tables for publication/presentations as shown in Summarizing Data. You can read more about the unite function here 9.2.6 Renaming columns/headers Sometimes a name is lengthy, or cumbersome to work with in R. While something like This_is_a_valid_header is valid and compatible with R and tidyverse functions, you may want to change it to make it easier to work with (i.e. less typing). Simply use the rename function: colnames(badHeader) ## [1] &quot;UVVis_Wave_Length_nM&quot; &quot;Absorbance&quot; colnames(rename(badHeader, wavelength_nM = UVVis_Wave_Length_nM)) ## [1] &quot;wavelength_nM&quot; &quot;Absorbance&quot; 9.3 Tips for recording data In case you havent picked up on it, tidying data in R is much easier if the data is recorded consistently. You cant always control how your data will look, but in the event that you can (i.e. your inputting the instrument readings into Excel on the bench top) here are some tips to make your life easier: Be consistent. If youre naming your samples make sure they all contain the same elements in the same order. The sample names Toronto_O3_1 and Toronto_O3_2 can easily be broken up as demonstrated in [Separating columns]; O3_Toronto_1, TorontoO32, and Toronto_1 cant be. Use as simple as possible headers. Often youll be pasting instrument readings into one .csv using Excel on whatever computer records the instrument readings. In these situations its often much easier to paste things in columns. Recall the capabilities of pivot_longer and how you can break up names as described in Making data longer. Chemical_A_1 and Chemical_B_2 are headers that are descriptive for your sample and can be easily pivoted into their own columns. Chemical A 1 ( I think?!) is a header that isnt. Make sure data types are consistent within a column. This harks back to the [Importing data into R] chapter, but a single non-numeric character can cause R to misinterpret an entire column leading to headaches down the line. Save your data in UTF-8 format. Excel and other programs often allow you to export your data in a variety of .csv encodings, but this can affect how R reads when importing your data. Make sure you select UTF-8 encoding when exporting your data. 9.4 Further reading As always, the R for Data Science book goes into more detail on all of the elements discussed above. For the material covered here you may want to read Chapter 9: Tidy Data. 9.5 Chapter References "],["transform-data-manipulation.html", "Chapter 10 Transform: Data Manipulation 10.1 Selecting by row or value 10.2 Arranging rows 10.3 Selecting column name 10.4 Adding new variables 10.5 Group and summarize data 10.6 The pipe: chaining functions together 10.7 Further reading", " Chapter 10 Transform: Data Manipulation Transformation encompasses any steps you take to manipulate, reshape, refine, or transform your data. Weve already touched upon some useful transformation functions in previous example code snippets, such as the mutate function for adding columns. This section will explore some of the most useful functionalities of the dplyr package, explicitly introduce the pipe operator %&gt;%, and showcase how you can leverage these tools to quickly manipulate your data. The essential dplyr functions are : mutate() to create new columns/variables from existing data arrange() to reorder rows filter() to refine observations by their values (in other words by row) select() to pick variables by name (in other words by column) summarize() to collapse many values down to a single summary. Well go through each of these functions, but we highly recommend you read Chapter 3: Data Transformation from R for Data Science to get a more comprehensive breakdown of these functions. Note that the information here is based on a tidyverse approach, but this is only one way of doing things. Please see the Further reading section for links to other suitable approaches to data transformation. Lets explore the functionality of dplyr using some flame absorption/emission spectroscopy (FAES) data from a CHM317 lab. This data represents the emission signal of five sodium (Na) standards measured in triplicate: # Importing using tips from Import chapter FAES &lt;- read_csv(file = &quot;data/FAESdata.csv&quot;) %&gt;% # see section on Pipe pivot_longer(cols = -std_Na_conc, names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) %&gt;% separate(col = std_Na_conc, into = c(&quot;type&quot;, &quot;conc_Na&quot;, &quot;units&quot;), sep = &quot; &quot;, convert = TRUE) DT::datatable(FAES) Note the use of convert = TRUE in the separate() call. This runs a type convert on new columns. If we didnt include this, the conc_Na column would be of type character because the numbers originated from a string. convert() ensures theyre converted to numeric. Always use convert = TRUE when you separate columns. 10.1 Selecting by row or value filter() allows up to subset our data based on observation (row) values. filter(FAES, conc_Na == 0) ## # A tibble: 3 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 2 592. ## 3 blank 0 ppm 3 581. Note how we need to pass logical operations to filter() to specify which rows we want to select. In the above code, we used filter() to get all rows where the concentration of sodium is equal to 0 (== 0). Note the presence of two equal signs (==). In R one equal sign (=) is used to pass an argument, two equal signs (==) is the logical operation is equal and is used to test equality (i.e. that both sides have the same value). A frequent mistake is to use = instead of == when testing for equality. 10.1.1 Logical oeprators filter() can use other relational and logical operators or combinations thereof. Relational operators compare values and logical operators carry out Boolean operations (TRUE or FALSE). Logical operators are used to combine multiple relational operators lets just list what they are and how we can use them: Operator Type Description &gt; relational Less than &lt; relational Greater than &lt;= relational Less than or equal to &gt;= relational Greater than or equal to == relational Equal to != relational Not equal to &amp; logical AND ! logical NOT | logical OR is.na() function Checks for missing values, TRUE if NA Selecting all signals below a threshold value filter(FAES, signal &lt; 4450) ## # A tibble: 3 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 2 592. ## 3 blank 0 ppm 3 581. Selecting signals between values filter(FAES, signal &gt;= 4450 &amp; signal &lt; 8150) ## # A tibble: 3 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 standard 0.1 ppm 1 5656. ## 2 standard 0.1 ppm 2 5654. ## 3 standard 0.1 ppm 3 5667. Selecting all other replicates other than replicate 2 filter(FAES, replicate != 2) ## # A tibble: 10 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 3 581. ## 3 standard 0.1 ppm 1 5656. ## 4 standard 0.1 ppm 3 5667. ## 5 standard 0.2 ppm 1 9393. ## 6 standard 0.2 ppm 3 9332. ## 7 standard 0.5 ppm 1 20187. ## 8 standard 0.5 ppm 3 20153. ## 9 standard 1 ppm 1 30798. ## 10 standard 1 ppm 3 30790. selecting the first standard replicate OR any of the blanks. filter(FAES, (type == &quot;standard&quot; &amp; replicate == 1) | (type == &quot;blank&quot;)) ## # A tibble: 7 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 2 592. ## 3 blank 0 ppm 3 581. ## 4 standard 0.1 ppm 1 5656. ## 5 standard 0.2 ppm 1 9393. ## 6 standard 0.5 ppm 1 20187. ## 7 standard 1 ppm 1 30798. Removing any missing values (NA) using is.na(). Note there are no missing values in our data set so nothing will be removed, if we removed the NOT operator (!) we would have selected all rows with missing values. filter(FAES, !is.na(signal)) ## # A tibble: 15 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 2 592. ## 3 blank 0 ppm 3 581. ## 4 standard 0.1 ppm 1 5656. ## 5 standard 0.1 ppm 2 5654. ## 6 standard 0.1 ppm 3 5667. ## 7 standard 0.2 ppm 1 9393. ## 8 standard 0.2 ppm 2 9363. ## 9 standard 0.2 ppm 3 9332. ## 10 standard 0.5 ppm 1 20187. ## 11 standard 0.5 ppm 2 20141. ## 12 standard 0.5 ppm 3 20153. ## 13 standard 1 ppm 1 30798. ## 14 standard 1 ppm 2 30837. ## 15 standard 1 ppm 3 30790. These are just some examples, but you can combine the logical operators in any way that works for you. Likewise, there are multiple combinations that will yield the same result, its up to you do figure out which works best for you. 10.2 Arranging rows arrange() reorders the rows based on the value you passed to it. By default it arranges the specified values into ascending order. Lets arrange our data our data by increasing order of signal value: arrange( FAES, signal) ## # A tibble: 15 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. ## 2 blank 0 ppm 3 581. ## 3 blank 0 ppm 2 592. ## 4 standard 0.1 ppm 2 5654. ## 5 standard 0.1 ppm 1 5656. ## 6 standard 0.1 ppm 3 5667. ## 7 standard 0.2 ppm 3 9332. ## 8 standard 0.2 ppm 2 9363. ## 9 standard 0.2 ppm 1 9393. ## 10 standard 0.5 ppm 2 20141. ## 11 standard 0.5 ppm 3 20153. ## 12 standard 0.5 ppm 1 20187. ## 13 standard 1 ppm 3 30790. ## 14 standard 1 ppm 1 30798. ## 15 standard 1 ppm 2 30837. Since our original FAES data is already arranged by increasing conc_Na and replicate, lets inverse that order by arranging conc_Na into descending order using the desc() function WHILE arranging the signal values in ascending order: # Note the order of precedence (left-to-right) arrange(FAES, desc(conc_Na), signal) ## # A tibble: 15 x 5 ## type conc_Na units replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 standard 1 ppm 3 30790. ## 2 standard 1 ppm 1 30798. ## 3 standard 1 ppm 2 30837. ## 4 standard 0.5 ppm 2 20141. ## 5 standard 0.5 ppm 3 20153. ## 6 standard 0.5 ppm 1 20187. ## 7 standard 0.2 ppm 3 9332. ## 8 standard 0.2 ppm 2 9363. ## 9 standard 0.2 ppm 1 9393. ## 10 standard 0.1 ppm 2 5654. ## 11 standard 0.1 ppm 1 5656. ## 12 standard 0.1 ppm 3 5667. ## 13 blank 0 ppm 1 502. ## 14 blank 0 ppm 3 581. ## 15 blank 0 ppm 2 592. Just note with arrange() that NA values will always be placed at the bottom, whether you use desc() or not. 10.3 Selecting column name select() allows you to readily select columns by name. Note however that it will always return a tibble, even if you only select one variable/column. select(FAES, signal) ## # A tibble: 15 x 1 ## signal ## &lt;dbl&gt; ## 1 502. ## 2 592. ## 3 581. ## 4 5656. ## 5 5654. ## 6 5667. ## 7 9393. ## 8 9363. ## 9 9332. ## 10 20187. ## 11 20141. ## 12 20153. ## 13 30798. ## 14 30837. ## 15 30790. You can also select multiple columns using the same operators and helper functions described in Tidying Your Data:. select(FAES, conc_Na:replicate) ## # A tibble: 15 x 3 ## conc_Na units replicate ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0 ppm 1 ## 2 0 ppm 2 ## 3 0 ppm 3 ## 4 0.1 ppm 1 ## 5 0.1 ppm 2 ## 6 0.1 ppm 3 ## 7 0.2 ppm 1 ## 8 0.2 ppm 2 ## 9 0.2 ppm 3 ## 10 0.5 ppm 1 ## 11 0.5 ppm 2 ## 12 0.5 ppm 3 ## 13 1 ppm 1 ## 14 1 ppm 2 ## 15 1 ppm 3 # Getting columns containing the character &quot;p&quot; select(FAES, contains(&quot;p&quot;)) ## # A tibble: 15 x 2 ## type replicate ## &lt;chr&gt; &lt;chr&gt; ## 1 blank 1 ## 2 blank 2 ## 3 blank 3 ## 4 standard 1 ## 5 standard 2 ## 6 standard 3 ## 7 standard 1 ## 8 standard 2 ## 9 standard 3 ## 10 standard 1 ## 11 standard 2 ## 12 standard 3 ## 13 standard 1 ## 14 standard 2 ## 15 standard 3 10.4 Adding new variables mutate() allows you to add new variable (read columns) to your existing data set. Itll probably be the workhorse function youll use during your data transformation as you can readily pass other functions and mathematical operators to it to transform your data. lets suppose that our standards were diluted by a factor of 10, we can add a new column for this: mutate(FAES, &quot;dil_fct&quot; = 10) ## # A tibble: 15 x 6 ## type conc_Na units replicate signal dil_fct ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. 10 ## 2 blank 0 ppm 2 592. 10 ## 3 blank 0 ppm 3 581. 10 ## 4 standard 0.1 ppm 1 5656. 10 ## 5 standard 0.1 ppm 2 5654. 10 ## 6 standard 0.1 ppm 3 5667. 10 ## 7 standard 0.2 ppm 1 9393. 10 ## 8 standard 0.2 ppm 2 9363. 10 ## 9 standard 0.2 ppm 3 9332. 10 ## 10 standard 0.5 ppm 1 20187. 10 ## 11 standard 0.5 ppm 2 20141. 10 ## 12 standard 0.5 ppm 3 20153. 10 ## 13 standard 1 ppm 1 30798. 10 ## 14 standard 1 ppm 2 30837. 10 ## 15 standard 1 ppm 3 30790. 10 We can also create multiple columns in the same mutate() call: mutate(FAES, dil_fct = 10, adj_signal = signal * dil_fct) ## # A tibble: 15 x 7 ## type conc_Na units replicate signal dil_fct adj_signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blank 0 ppm 1 502. 10 5023. ## 2 blank 0 ppm 2 592. 10 5918. ## 3 blank 0 ppm 3 581. 10 5815. ## 4 standard 0.1 ppm 1 5656. 10 56563. ## 5 standard 0.1 ppm 2 5654. 10 56536. ## 6 standard 0.1 ppm 3 5667. 10 56674. ## 7 standard 0.2 ppm 1 9393. 10 93934. ## 8 standard 0.2 ppm 2 9363. 10 93627. ## 9 standard 0.2 ppm 3 9332. 10 93320. ## 10 standard 0.5 ppm 1 20187. 10 201869. ## 11 standard 0.5 ppm 2 20141. 10 201405. ## 12 standard 0.5 ppm 3 20153. 10 201530. ## 13 standard 1 ppm 1 30798. 10 307977. ## 14 standard 1 ppm 2 30837. 10 308365. ## 15 standard 1 ppm 3 30790. 10 307898. Couple of things to note: The variable were creating needs to be in quotation marks, hence dil_fct for our dilution factor variable The variables were referencing do not need to be in quotation marks; hence signal because this variable already exist. Note the order of precedence: dil_fct is created first so we can reference in the second argument, we would get an error if we swapped the order. 10.4.1 Useful mutate function There are a myriad of functions you can make use of with the mutate function. Here are some of the mathematical operators available in R: Operator.Function Definition + additon - subtraction * multiplication / division ^ exponent; to the power of log() returns the specified base-log; see also log10() and log2() 10.5 Group and summarize data summarize effectively summarized your data based on functions youve passed to it. Looking at our FAES data wed probably want the mean of the triplicate signals, alongside the standard deviation. Lets see what happens when we apply the summarize function straight up: summarise(FAES, &quot;mean&quot; = mean(signal), &quot;stdDev&quot; = sd(signal)) ## # A tibble: 1 x 2 ## mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13310. 11242. This doesnt look like what we wanted. What we got was the mean and standard deviation of all of the signals, regardless of the concentration of the standard. Also note how weve lost the other columns/variables and are only left with the mean and stdDev. This is all because we need to group our observations by a variable. We can do this by using the group_by() function. groupedFAES &lt;- group_by(FAES, type, conc_Na) summarise(groupedFAES, &quot;mean&quot; = mean(signal), &quot;stdDev&quot; = sd(signal)) ## `summarise()` has grouped output by &#39;type&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 4 ## # Groups: type [2] ## type conc_Na mean stdDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blank 0 559. 48.9 ## 2 standard 0.1 5659. 7.34 ## 3 standard 0.2 9363. 30.7 ## 4 standard 0.5 20160. 24.0 ## 5 standard 1 30808. 25.0 Here weve created a new data set, groupedFAES, that we grouped by the variables type and conc_Na so we could get the mean and standard deviation of each group. Note the multiple levels of grouping. For this data set we could have omitted the type variable, but in larger datasets you may have multiple groupings (i.e. from different location), so you can group by multiple variables to get smaller groups. 10.5.1 Useful summarize functions Weve used the mean() and sd() functions above, but there are a host of other useful functions you can use in conjunction with summarize. See Useful Functions in the summarise() documentation (enter ?summarise) in the console. This is also discussed at in more depth in the Summarizing Data chapter. 10.6 The pipe: chaining functions together With the tools presented here we could do a decent job analyzing our FAES data. Lets say we wanted to subtract the mean of the blank from each standard signal and then get summarize those results. It would look something like this: blank &lt;- filter(FAES, type == &quot;blank&quot;) meanBlank &lt;- summarize(blank, mean(signal)) meanBlank &lt;- as.numeric(meanBlank) paste(&quot;The mean signal from the blank triplicate is:&quot;, meanBlank) ## [1] &quot;The mean signal from the blank triplicate is: 558.5249&quot; stds_1 &lt;- filter(FAES, type == &quot;standard&quot;) stds_2 &lt;- mutate(stds_1, &quot;cor_sig&quot; = signal - meanBlank) stds_3 &lt;- group_by(stds_2, conc_Na) stds_4 &lt;- summarize(stds_3, &quot;mean&quot; = mean(cor_sig), &quot;stdDev&quot; = sd(cor_sig)) stds_4 ## # A tibble: 4 x 3 ## conc_Na mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 5101. 7.34 ## 2 0.2 8804. 30.7 ## 3 0.5 19602. 24.0 ## 4 1 30249. 25.0 While the code above did its job, its certainly wasnt easy to type and certainly not easy to read. At every step of the way weve saved our updated data outputs to a new variable (stds_1, stds_2, etc.). However, most of these intermediates arent important, and moreover the repetitive names clutter our code. As the code above is written, weve had to pay special attending to the variable suffix to make sure were calling the correct data set as our code has progresses. An alternative would be to reassign the outputs back to the original variable name (i.e. stds_1 &lt;- mutate(stds_1, ...)), but that doesnt solve the issue of readability as theres still redundant assigning. A solution for this is the pipe operator %&gt;% ( pronounced as then), an incredibly useful tool for writing more legible and understandable code. The pipe basically changes how you read code to emphasize the functions youre working with by passing the intermediate steps to hidden processes in the background. Re-writing the code above, wed get something like: meanBlank &lt;- FAES %&gt;% filter(type ==&quot;blank&quot;) %&gt;% summarise(mean(signal)) %&gt;% as.numeric() paste(&quot;The mean signal from the blank triplicate is:&quot;, meanBlank) ## [1] &quot;The mean signal from the blank triplicate is: 558.5249&quot; Things may look a bit different, but our underlying code hasnt changed much. Whats happening is the pipe operator passes the output to the first argument of the next function. So the output of filter... is passed to the first argument of sumamrise..., and the argument we specified in summarise is actually the second argument it receives. Youre probably wondering how hiding stuff makes your code more legible, but think of %&gt;% as being equivalent to then. We can read our code as: Take the FAES dataset, then filter for type == \"blank\" then collapse the dataset to the mean signal value and then convert to numeric value then pass this final output to the new variable meanBlank. Not only is the pipe less typing, but the emphasis is on the functions so you can better understand what youre doing vs. where all the intermediates are going. Extending our piping to the second batch of code we get: stds &lt;- FAES %&gt;% filter(type == &quot;standard&quot;) %&gt;% mutate(&quot;cor_sig&quot; = signal - meanBlank) %&gt;% group_by(conc_Na) %&gt;% summarize(&quot;mean&quot; = mean(cor_sig), &quot;stdDev&quot; = sd(cor_sig)) stds ## # A tibble: 4 x 3 ## conc_Na mean stdDev ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 5101. 7.34 ## 2 0.2 8804. 30.7 ## 3 0.5 19602. 24.0 ## 4 1 30249. 25.0 Same thing. The underlying code hasnt changed much, but its much more legible and we can clearly see were subtracting the meanBlank value from each measured signal then summarizing the corrected signals. 10.6.1 Notes on piping The pipe is great and especially useful with tidyverse packages, but it does have some limitations: You cant easily extract intermediate steps. So youll need to break up your pipping chain to output any intermediate steps you can. The benefit of piping is legibility; this goes away as you increase the number of steps as you lose track of whats going on. Keep the piping short and thematically similar. Pipes are linear, if you have multiple inputs or outputs you should consider an alternative approach. 10.7 Further reading Chapter 5: Data Transformation of R for Data Science for a deeper breakdown of dplyr and its functionality. Chapter 18: Pipes of R for Data Science for more information on pipes. Syntax equivalents: base R vs Tidyverse by Hugo Taveres for a comparison of base-R solutions to tidyverse. This entire book is largely biased towards tidyverse solutions, but theres no denying that certain base-R can be more elegant. Check out this write up to get a better idea. "],["programming-with-r.html", "Chapter 11 Programming with R 11.1 Functions 11.2 Conditional arguments 11.3 When to use functions 11.4 Further reading", " Chapter 11 Programming with R With programming, like most things, learning a little goes a long way. And like most things, its easy to lose the forest for the trees. Thats why we wont focus too much on programming (after all youre chemist not computer scientist) but we will introduce a few simple yet incredibly powerful elements of programming to help you along with your data science quest. Well point to several sources for further reading on functions at the end of this chapter. 11.1 Functions Functions allow you to write general purpose code to automate common tasks. Theyre a great way to decrease repetition and make your code more legible and reproducible. By using functions youll not only reduce the number of lines of code you need to review, but youll the number of spots you need to review rectify a potential issue. To create a function in R you only need function(): funSum &lt;- function(x,y){ z &lt;- x + y paste(&quot;The sum of&quot;, x, &quot;+&quot;, y, &quot;is&quot;, z, sep =&quot; &quot;) } funSum(1, 3) ## [1] &quot;The sum of 1 + 3 is 4&quot; funSum(&quot;yes&quot;,3) ## Error in x + y: non-numeric argument to binary operator What weve done is create a function called funSum which takes two numeric inputs x and y, sums the two into z and paste an output telling us the sum. A couple of things to note: We need to explicitly state which arguments are function will take; in this example they are x and y. Whatever we pass to x or y will be carried into the function. Functions are written with an order to the inputed arguments In our case x is the first argument, so unless specified otherwise, the first argument passed to funSum() will be passed along as the x argument. You can explicitly pass values by stating the argument; i.e. funSum(y = 10, x = 2). We cant sum non-numeric values, so R returns an error in the second instance Functions create their own environment, therefore any variable created inside a function only exists inside the function. In the above example, x, y, and z only exist inside the function. R automatically returns whichever variable is on the last line of the body of the function as its output, but you can explicitly ask for an output using return() Lets take a look at a more practical function, something that you might actually use. In mass spectrometry, a common gauge of accuracy is the mass error, a measure of the difference between the observed and theoretical masses, and is reported in parts-per-million (ppm). The formula for calculating mass error is: \\[ Mass~error~(ppm) = \\frac{|mass_{theoretical} - mass_{experimental}|}{mass_{theorical}} \\times 10^6 \\] The formula is simple enough, but you may need to calculate any number of mass errors, so it behooves us to compose a quick formula to simplify our workload: ppmMS &lt;- function(theoMZ, expMZ){ ppm &lt;- abs(theoMZ - expMZ)/theoMZ * 1e6 ppm } # Theoretical mass = 1479.63 m/z # experimental mass = 1480.10 m/z ppmMS(theoMZ = 1479.63, expMZ = 1480.10) ## [1] 317.647 Pretty useful if youre manually checking something, but we can also use our functions with the pipe to help our data transformation: # Example data masses &lt;- data.frame(&quot;theo&quot; = c(1479.63, 1479.63, 1479.63), &quot;exp&quot; = c(1478.63, 1479.63, 1480.10)) masses %&gt;% mutate(massError = ppmMS(theo, exp)) ## theo exp massError ## 1 1479.63 1478.63 675.8446 ## 2 1479.63 1479.63 0.0000 ## 3 1479.63 1480.10 317.6470 This last part is critical as functions make your code more legible. We can clearly read that the code above is calculating the mass error between the theoretical and experimentally observed masses. This might not be as apparent if we put in a complex mathematical formula in the middle of our pipe. 11.2 Conditional arguments Condaitional arguments used to specify a path in a function depending on whether a statement is TRUE or FALSE. These are explored in greater detail via the links in the Further reading section, but heres a quick example of a function that uses the conditional if statement to print out which number is largest: isGreater &lt;- function(x, y){ if(x &gt; y){ return(paste(x, &quot;is greater than&quot;, y, sep = &quot; &quot;)) } else if (x &lt; y){ return(paste(x, &quot;is less than&quot;, y, sep = &quot; &quot;)) } return(paste(x, &quot;is equal to&quot;, y, sep = &quot; &quot;)) } isGreater (2, 1) ## [1] &quot;2 is greater than 1&quot; isGreater (1, 2) ## [1] &quot;1 is less than 2&quot; isGreater (1, 1) ## [1] &quot;1 is equal to 1&quot; Our simple function compares two numbers, x and y and if x &gt; y evaluate to TRUE it returns the pasted string x is greater than y. If x &lt; y evaluates to FALSE, as in y &gt; x, our function returns the pasted string x is less than y, and finally if neither x &gt; y and x &lt; y evaluate to TRUE, they must be equal! Therefore the final output is x is equal to y. This is an example of an else if statement. If youre simply evaluating two conditions (TRUE or FALSE) you only need the if() conditional, see Further reading for more details. 11.2.1 Piping conditional statements You can already see the potential for simple conditional statements in the pipe. However, to keep piping operations legible, dplyr offers the case_when function, which works similarly to the else if statements showcased above. Lets see how it works using a real world example. In mass spectrometry undetected compounds are recorded by the instrument as having an intensity of 0; but its a common practice to replace 0 with \\(\\frac{limit~of~detection}{2}\\) for subsequent analysis However, we dont want to replace every value with \\(\\frac{LOD}{2}\\), only 0s. Lets use the case_when() function to create a new values with the recorded intensities lod &lt;- 4000 # previously calculated LOD results &lt;- data.frame(&quot;mz&quot; = c(308.97, 380.81, 410.11, 445.34 ), # dummy data &quot;intensities&quot; = c(0, 1000, 5000, 10000)) results %&gt;% mutate(reportedIntensities = case_when(intensities &lt; lod ~ lod/2, TRUE ~ intensities)) ## mz intensities reportedIntensities ## 1 308.97 0 2000 ## 2 380.81 1000 2000 ## 3 410.11 5000 5000 ## 4 445.34 10000 10000 Firstly were creating a new column called reportedIntesities using mutate() and using case_when() to conditionally fill that column. The inputs weve passed to case_when() are two-sided formulas. Essentially if the conditions on the left-hand side of the tilda (~) evaluate to TRUE, case_when will execute the right-hand side. Thee first two-sided formula is intensities &lt; lod ~ lod/2 and checks if the intensities value is less than the previously calculated limit of detection. If intensitis &lt; lod evaluates to TRUE we insert half of the LOD value for that row. If intentisites &lt; lod evaluates to FALSE, we move onto the next two-side formula and reevaluate again. The second two-sided formula TRUE ~ intensities basically means for everything thats remaining (greater than LOD in our instance) just use the value from the intensities column. Some ideas to consider when working with case-when(): Theres no limits to the conditions you can pass to case_when(). However case_when() evaluates in order so put the more specific conditions before the more general. Remember that the point of case_when() and piping is legibility. If youre passing multiple conditions, consider writing a function using else if statements to keep the pipe legible. 11.3 When to use functions A good rule when coding is Dont Repeat yourself!. In practice, this means dont copy and paste blocks of code to multiple parts of your script. Its more difficult to read (more lines), and if you identify an issue with one block, youll need to hunt down all the other blocks to rectify the situation (youll always miss something!). by using functions youll reduce the number of lines of code, but youll also only need to check one spot to rectify the issues. 11.4 Further reading These chapter has been intentional succinct. Weve omitted several other aspects of programming in R such as for loops, and other iterative programming. To get a better sense of programming in R and to learn more, please see the following links: case_when(): the documentation for the case_when() function and several useful examples. Chapter 19: Functions, Chapter 20: Vectors, and Chapter 21: Iteration of R for Data Science by H. Wickham and G. Grolemund. Hands-on Programming in R by G. Grolemund for a more in-depth (but still approachable) take on programming in R. "],["introduction-to-data-analysis.html", "Chapter 12 Introduction to Data Analysis 12.1 Example Data 12.2 Further Reading", " Chapter 12 Introduction to Data Analysis This section will showcase a variety of data analysis tool you can use to better understand your data. Specifically well be focusing on the transforming, visualizing, and modelling steps of our data analysis workflow. It relies heavily on having properly organized data, so if you havent already, please review Section 2: Data Wrangling in R. Data science workflow describes by Wickham and Grolemund; image from R for Data Science, Wickham and Grolemund (2021) The Transform, Visualize, and Model cycle exists because these steps often feed into one another. For example, youll often transform your data, make a quick model, then visualize it to see how it performs. Other times, youll visualize your data to see what type of model can explain it, and if any transformations are necessary. This is the beauty of R (and coding in general). Once youve setup everything, these steps are fairly simple to execute allowing you to quickly explore your data from a number of different angles. Well also touch upon the theory (the why) behind these steps, and introduce some tools you can use to better explore your data. Note that we wont dedicate a chapter to Communication, however readers will be given plenty of advice on producing legible and publication-ready plots in addition to the information on authoring rmardown documents in Using R Markdown. 12.1 Example Data Throughout this section and the next well be making use of a couple of example datasets. These datasets are all available in the data subfolder of the R4EnvChem Project Template. If you havent already, you can download them from the repo here 12.2 Further Reading In case it hasnt been apparent enough, this entire endeavour was inspired by the R for Data Science reference book by Hadley Wickham and Garrett Grolemund. Every step described above is explored in more detail in their book, which can be read freely online at https://r4ds.had.co.nz/. We strongly encourage you to read through the book to supplement your R data analysis skills. "],["summarizing-data.html", "Chapter 13 Summarizing Data 13.1 Data to play with 13.2 Summarizing data by group 13.3 Pretty tables with flextable", " Chapter 13 Summarizing Data Sumamrizing data is what it sounds like. Youre reducing the number of rows in your dataset based on some predermined method. Taking the average of a group of numbers is summarizing the data. Many numbers have been condensed to one: the average. In this chapter well go over sumamrizing data, and some aesthetic changes we can make for publication ready tables. 13.1 Data to play with Well take a look at the 2018 hourly mean NO2 concentrations for the Atlantic provinces (New Brunswick, Prince Edward Island, Nova Scotia, and Newfoundlan). The dataset is available in the R4EnvChem Project Template repository. Also if youre keen you can download any number of atmospheric datasets from Environment and Climate Change Canadas (ECCC) National Airborn Pollution Programs (NAPS) website here Since ECCC stores their NAPS data in a matrix layout, we need to briefly tidy it up: atlNO2 &lt;- read_csv(&quot;data/2018hourlyNO2_Atl.csv&quot;, skip = 7, na =c(&quot;-999&quot;)) %&gt;% rename_with(~tolower(gsub(&quot;/.*&quot;, &quot;&quot;, .x))) %&gt;% pivot_longer(cols = starts_with(&quot;h&quot;), names_prefix = &quot;h&quot;, names_to = &quot;hour&quot;, names_transform = list(hour = as.numeric), values_to = &quot;conc&quot;, values_transform = list(conc = as.numeric), values_drop_na = TRUE) # First 50 rows of dataset DT::datatable(atlNO2[1:50, ]) Note in our dataset that both Halifax NS and Saint John NB have three NAPS stations each. It wont matter for our aggregation, but if we were xploring this data in more depth this is something we would want to take into account. 13.2 Summarizing data by group We need to combine thegroup_by() and summarize() functions. summarise() also works What this does is (1) allows us to specify which groups we want summarize, and (2) how we want it summarized. Well talk more about point (2) later on, for now, lets look at point (1) Lets look at an example where we calculate the mean hourly NO2 concentrations in the 4 provinces in our dataset: sumAtl &lt;- atlNO2 %&gt;% group_by(p) %&gt;% summarize(mean = mean(conc)) sumAtl ## # A tibble: 4 x 2 ## p mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 NB 2.86 ## 2 NL 2.30 ## 3 NS 2.36 ## 4 PE 0.975 Thats it. 186339 unique rows summarized like that. Note that summarize produces a new data frame, so youll want to double check on the outputted data types. Lets break down what our code does: Were creating a new dataframe, so were going to store it in sumAtl. We then take out atlNO2 dataset and we group our datasets by province (group_by(p)) We then summarize our groupped data by summarizing the NO2 concentration with sumaraize(mean = mean(conc)). - Note that since we're creating a new data set, we need to create new columns. This is whatmean = mean(conc)does. We're creating a column *called mean*, which contains the *numerical mean* 1-hr NO~2~ values which were calculated using themean()` function. Simple Lets dig a little deeper. The Canadian Ambient Air Quality Standards stipulate that the annual mean of 1-hour means for NO2 cannot exceed 17.0 ppb in 2020, and 12.0 ppb in 2025. Lets see if any city in our dataset violated these standards in 2018. To do this, well group by province (p) and city (city). This will retain our provinces column that we might want to use later on. sumAtl &lt;- atlNO2 %&gt;% group_by(p, city) %&gt;% summarize(mean = mean(conc)) sumAtl ## # A tibble: 19 x 3 ## # Groups: p [4] ## p city mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 NB Bathurst 1.21 ## 2 NB Edmundston 4.52 ## 3 NB Fredericton 1.82 ## 4 NB Moncton 3.37 ## 5 NB Saint John 3.02 ## 6 NL Corner Brook 2.71 ## 7 NL Grand Falls-Windsor 0.918 ## 8 NL Labrador City 2.51 ## 9 NL Marystown 0.277 ## 10 NL Mount Pearl 1.53 ## 11 NL St Johns 5.33 ## 12 NS Halifax 3.44 ## 13 NS Kentville 0.841 ## 14 NS Pictou 1.19 ## 15 NS Port Hawkesburry 2.53 ## 16 NS Sydney 2.66 ## 17 PE Charlottetown 1.85 ## 18 PE Southampton 0.512 ## 19 PE Wellington 0.455 13.2.1 Further sumarize operations There are other options we can use to sumamrize out data. A handy list is proviced in the summarize help page. The most common ones youll need are : mean() which calcualted the arithmetic mean, a.k.a. the average. median() which calcualted the sample media, the value separating the higher half from the lower half of a data sample. sd() which calcualted the sample standard deviation. min() and max() which returnt he smallest and largest value in the dataset. n() which provides the number of entires in a group. Note you dont specify a variable for n.  Lets seem them in action: sumAtl &lt;- atlNO2 %&gt;% group_by(p, city) %&gt;% summarize(mean = mean(conc), sd = sd(conc), min = min(conc), max = max(conc), n = n()) sumAtl ## # A tibble: 19 x 7 ## # Groups: p [4] ## p city mean sd min max n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 NB Bathurst 1.21 1.91 0 27 8755 ## 2 NB Edmundston 4.52 4.82 0 45 8756 ## 3 NB Fredericton 1.82 4.04 0 39 8729 ## 4 NB Moncton 3.37 4.71 0 39 8749 ## 5 NB Saint John 3.02 4.04 0 41 26051 ## 6 NL Corner Brook 2.71 2.47 0 28 8505 ## 7 NL Grand Falls-Windsor 0.918 1.26 0 27 7746 ## 8 NL Labrador City 2.51 4.05 0 46 8612 ## 9 NL Marystown 0.277 0.635 0 11 7142 ## 10 NL Mount Pearl 1.53 2.82 0 68 8522 ## 11 NL St Johns 5.33 6.14 0 49 8670 ## 12 NS Halifax 3.44 4.19 0 39 17591 ## 13 NS Kentville 0.841 1.22 0 20 8640 ## 14 NS Pictou 1.19 1.52 0 20 8515 ## 15 NS Port Hawkesburry 2.53 4.29 0 59 8601 ## 16 NS Sydney 2.66 2.93 0 37 8675 ## 17 PE Charlottetown 1.85 2.87 0 35 8690 ## 18 PE Southampton 0.512 0.683 0 16 6799 ## 19 PE Wellington 0.455 0.747 0 9 8591 Note that the functions we pass to summarize adhere to rules of missing values. That is to say, if even one value in a group is an NA, the entire group defaults to NA. Consequently, if your confident this isnt an issue, you can pass the argument na.rm = TRUE to any of the summarize functions, which would look like mean = mean(conc, na.rm = TRUE). This will ignore any NA values and return a numeric value like you probably expect. 13.3 Pretty tables with flextable While the summarize function does an excellent job of summarizing our data, the outputted dataset isnt really fit for publication. This is double so if you used summarize as the last step of your chemuical quantification and you want a nice and pretty table of means sample concentration with standard deviations. To this end well use the flextable pacakage. Please refer to flextable R package. There are other packages to make tables, but flextable is consistent between HTML and PDF outputs. library(flextable) flextable(sumAtl) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-2249d92e{border-collapse:collapse;}.cl-2240ff16{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-22412626{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-22412627{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-22417446{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-22417447{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-22417448{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-22417449{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2241744a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2241744b{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} pcitymeansdminmaxnNBBathurst1.21336381.90688650278,755NBEdmundston4.51895844.81933870458,756NBFredericton1.82437854.04449040398,729NBMoncton3.37021374.71261280398,749NBSaint John3.02264794.038227304126,051NLCorner Brook2.70840682.46963800288,505NLGrand Falls-Windsor0.91750581.26145040277,746NLLabrador City2.50627034.05408020468,612NLMarystown0.27667320.63498500117,142NLMount Pearl1.53414692.82499290688,522NLSt Johns5.32975786.13848170498,670NSHalifax3.43834924.187116403917,591NSKentville0.84108801.21860100208,640NSPictou1.19377571.51804860208,515NSPort Hawkesburry2.52540404.28565340598,601NSSydney2.66109512.93000390378,675PECharlottetown1.85132342.86694040358,690PESouthampton0.51154580.68317780166,799PEWellington0.45547670.7471108098,591 Perhaps that isnt pretty enough for you. Doubtlessly your instructor will tell you to combine the mean and sd into one value (i.e. $ mean sd$). Well do this in two steps. Step 1: Use unite() to merge the mean and sd values together rowise; values will be separated by ±. ± is a legit symbol, try Alt+241or copy and paste it from this book. Step 2: Pretty up our kable to significant digits, and perform some aesthetic changes. 13.3.1 Uniting columns Firstly, our mean and sd columns contain way too many decimal places. Well need to round them down before we use unite() to paste together the two columns into one. During our unite() call, well use sep = \" ± \" to seprate the mean from sd values (otherwise theyd be pasted as one long number). prettySumAtl &lt;- sumAtl %&gt;% mutate(mean = sprintf(&quot;%.1f&quot;, mean), sd = sprintf(&quot;%.1f&quot;, sd)) %&gt;% unite(&quot;mean ± sd&quot;, mean:sd, sep = &quot; ± &quot; ) %&gt;% select(-n) # removing n column prettySumAtl ## # A tibble: 19 x 5 ## # Groups: p [4] ## p city `mean ± sd` min max ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NB Bathurst 1.2 ± 1.9 0 27 ## 2 NB Edmundston 4.5 ± 4.8 0 45 ## 3 NB Fredericton 1.8 ± 4.0 0 39 ## 4 NB Moncton 3.4 ± 4.7 0 39 ## 5 NB Saint John 3.0 ± 4.0 0 41 ## 6 NL Corner Brook 2.7 ± 2.5 0 28 ## 7 NL Grand Falls-Windsor 0.9 ± 1.3 0 27 ## 8 NL Labrador City 2.5 ± 4.1 0 46 ## 9 NL Marystown 0.3 ± 0.6 0 11 ## 10 NL Mount Pearl 1.5 ± 2.8 0 68 ## 11 NL St Johns 5.3 ± 6.1 0 49 ## 12 NS Halifax 3.4 ± 4.2 0 39 ## 13 NS Kentville 0.8 ± 1.2 0 20 ## 14 NS Pictou 1.2 ± 1.5 0 20 ## 15 NS Port Hawkesburry 2.5 ± 4.3 0 59 ## 16 NS Sydney 2.7 ± 2.9 0 37 ## 17 PE Charlottetown 1.9 ± 2.9 0 35 ## 18 PE Southampton 0.5 ± 0.7 0 16 ## 19 PE Wellington 0.5 ± 0.7 0 9 Note to round the numbers we used sprintf() to round out numbers. This is because in the final publication its important to keep trailing zeros (i.e. 1.0 and not 1), but Rs round() will drop these. mean = sprintf(\"%.1f\", mean) takes the existing values in the mean column, rounds thems to one difit, thats what \"%.1f\" means (%.2f would be two digits and so on), and paste them back into the mean column. Same situation for the sd column. 13.3.2 Pretty kables Now well want to make a pretty table. Despite this being a book on visualziations in R, tables are often underappreciate means to convey information. Often when youre only plotting a handful of numbers, a table would better serve the reader. So dont overlook this point of your report. If youve distilled hours of your work to a handful of numbers, you best serve them up on a silver plater. ft &lt;- flextable(prettySumAtl) ft &lt;- set_header_labels(ft, p = &quot;province&quot;) ft &lt;- set_table_properties(ft, layout = &quot;autofit&quot;) ft &lt;- align(ft, j = &quot;mean ± sd&quot;, align = &quot;right&quot;, part = &quot;all&quot;) ft .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-226b273c{table-layout:auto;border-collapse:collapse;width:100%;}.cl-2263d3ce{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2263d3cf{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2263d3d0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-226421f8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-226421f9{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-226421fa{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-226421fb{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-226421fc{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-226421fd{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} provincecitymean ± sdminmaxNBBathurst1.2 ± 1.9027NBEdmundston4.5 ± 4.8045NBFredericton1.8 ± 4.0039NBMoncton3.4 ± 4.7039NBSaint John3.0 ± 4.0041NLCorner Brook2.7 ± 2.5028NLGrand Falls-Windsor0.9 ± 1.3027NLLabrador City2.5 ± 4.1046NLMarystown0.3 ± 0.6011NLMount Pearl1.5 ± 2.8068NLSt Johns5.3 ± 6.1049NSHalifax3.4 ± 4.2039NSKentville0.8 ± 1.2020NSPictou1.2 ± 1.5020NSPort Hawkesburry2.5 ± 4.3059NSSydney2.7 ± 2.9037PECharlottetown1.9 ± 2.9035PESouthampton0.5 ± 0.7016PEWellington0.5 ± 0.709 "],["visualizations.html", "Chapter 14 Visualizations 14.1 ggplot basics 14.2 Visualizations for Env Chem 14.3 Bar chart 14.4 Box Plots 14.5 Histograms 14.6 Scatter plots 14.7 Interactive plots 14.8 Plotting for publication", " Chapter 14 Visualizations Visualizations have always been an important part of data science and chemistry. Good graphics illuminate trends and patterns you may have otherwise missed and allow us to quickly inspect thousands of values. R via the ggplot2 package is one of, if not the premier, data visualization language available. This chapter will formally introduce the ggplot2 package, explain a bit of the logic undergirding its operation, and give you some quick examples of how it works. Afterwards well delve deeper into specific visualizations youll use and encounter in your studies culminating in preparing your plots for publication. 14.1 ggplot basics ggplot2 is loaded by default with the tidyverse suite of packages. Lets revisit our spectroscopy data we encountered in Tidying your data: library(tidyverse) atr_long &lt;- read_csv(&quot;data/ATR_plastics.csv&quot;) %&gt;% pivot_longer(cols = -wavenumber, names_to = &quot;sample&quot;, values_to = &quot;absorbance&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## wavenumber = col_double(), ## EPDM = col_double(), ## Polystyrene = col_double(), ## Polyethylene = col_double(), ## `Sample: Shopping bag` = col_double() ## ) # First 50 rows of data DT::datatable(atr_long[1:50, ]) 14.1.1 Building plots ups The gg in ggplot2 stands for the grammar of graphics (Wickham 2010), and its a way to break down graphics (plots) into small pieces that can be discussed (hence grammar). Well take a look at this grammar via geoms (what kind of plot), aes (aesthetic choices), etc. For now, understand that this means we need to build up graphics/plots piece-by-piece and layer-by-layer. This extends beyond code to how we code. No sense in putting lipstick on a pig. Plot often, and discard the useless ones. Take the time to pretty up your plot after youre satisfied with the underlying data. 14.1.2 Basic plotting ggplot2 uses geoms to specify what type of plot to create. Different plots are used to convey different meanings and have different strengths and weakness. Well explore these more in Section 3, but for now well focus on geom_point(), which simply plots data as points on an [x,y] coordinate. In other words, a scatter plot. Lets plot our tided atr_long data: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance)) + geom_point() Lets ignore the plot for now and look at our code down: ggplot() initializes a ggplot object, basically an empty plot. To this weve specified out data set (data = atr_long). We then specified our aesthetic mappings via aes(). Here well pass information for how we want the plot to look. 3.To our aesthetic mappings weve specified which values from atr_long are supposed to be our x-axis values (x = wavenumber) and y-axis values (y = absorbance). We then add the geom_point() layer to create a scatter plot of [x,y] points . Now lets look at our result. What we see is a point for every recorded absorbance measurements from our ATR analysis. We can clearly see the spectra of the different plastics in our data, however theyre all colours the same. This is because weve only species the x and y values. As far as ggplot() is concerned, these are the only values that matter, but we know different. Fortunately you can pass multiple variables to different aes() options to enhance our plot. For instance, we can pass the sample variable, which specifies which sample a spectrum originates from, to the colour option: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() Now we have a legend which clearly specifies which points are associated with each sample. But now the points are too large, potentially masking certain peaks. We can adjust the size of each point as follows: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point(size = 0.5) We specified size = 0.5 in the geom_point() call because its a constant. We can map size to any continuous variable, such as the absorbance: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance, colour = sample, size = absorbance)) + geom_point() Sometimes this makes sense (i.e. a bubble chart) but for our example, having the size of the points increase as the absorbance increases doesnt provide any new information (it actually clutters our plot). 14.1.3 Changing plot labels By default ggplot uses the header of the columns you passed for the x and y aes() options. Because headers are written for code theyre often poor label titles for plots. We can specify new labels and plot titles as follows: ggplot(data = atr_long, aes(x = wavenumber, y = absorbance, colour = sample)) + geom_point() + labs(title = &quot;ATR Spetra&quot;, subtitle = &quot;Courtesey of CHM317 student data&quot;, x = &quot;Wavenumber, cm^-1&quot;, y = &quot;Absorbance (arbitrary units)&quot;, caption = &quot;hi mom&quot;) 14.2 Visualizations for Env Chem Weve already encountered and produced several types of plots to visualize our data. Weve also gone over the theory and basic operations of ggplot() in the ggplot basics section. Now, well expand on these and explicitly walk through the most common data visualization methods youll encounter in the field of environmental chemistry. Additionally, well learn how to get your plots ready for publication. The plots well be covering include: bar plots box plots histograms scatter plots (with linear regression) Touch ups for publications These are only a smattering of the possible data visualizations you can perform in R. Were focusing on them because of their ubiquity in our field, but they often wont be the ideal visualizations you need to communicate your story. We highly recommend you check out the following resources. Not only are they a great source of inspiration, they provide example code to get you up and running. We consult them regularly. Data to viz which features a decision tree to help you decide on what plot would serve you best. ggplot2 extensions gallery which is the best repository to the plethora of ggplot2() extensions. If you need a specialized plot, check here. Odds are someone has a solution to your problem. Some great extensions include ggrepel for easy labelling of points; ggpmisc for statistical annotations; and ggpubr for publication ready plots, group wise comparisons, and annotation of statistical significance. The R Graph Gallery contains hundreds of charts made with R. While its not as easy to navigate as Data to viz, it does contain many more examples; it is definitely worth exploring. Discrete vs. continuous variables The type of plots available to you, and how they display, are dependent on the type of data. Namely, whether your data is discrete (i.e .can only take particular values) or continuous (is not restricted to defined separate values, but can occupy any value over a continuous range). So a variable consisting of cities would be discrete, whereas a variable like concentration of a chemical would be continuous. You can treat numeric data as categorical if you so chose. Understanding the difference between discrete and continuous data will shape how you plot your data. Prerequisites Additionally, for this section well mostly be using the atlNO2 and sumAtl datasets we created in the Summarizing data chapter. Please revisit that chapter for details on that dataset. 14.3 Bar chart Bar charts, also called column charts, represent categorical data with rectangular bars whose height/length is proportional to the values they represent. ggplot(data = sumAtl, aes(x = city, y = mean)) + geom_col() + coord_flip() # rotates plot 90 degrees Pretty boring, but its gotten the job done. Note that we used coord_flip() to rotate our plot 90\\(^\\circ\\) therefore the supplied x option of city is now plotted on the y-axis. This makes reading long categorical names (i.e. the names of cities) easier. coord_flip() doesnt change anything else except the final orientation of the plot. Also note that ggplot() includes geom_col() and geom_bar(). While both can be used to make bar charts. geom_col() is used when you want to represent values in the data (i.e. the precalculated mean as shown above), whereas geom_bar() makes the height of the bar proportional to the number of cases in each group. 14.3.1 Adding error bars Any measurement always has an associated uncertainty/variability. These values are expressed visually via error bars demarcating the minimum and maximum variability and give a general idea of how precise a measurement is. In our sumAtl dataset weve calculated the standard deviation as a measure of uncertainty. In our example, weve used the standard deviation (sd) as a measure of uncertainty of our calculated annual means. To plot error bars we use geom_errorbar() and pass the min and max values we want the error bars to be. In our case, the lowest value would be ymin = mean - sd, and the highest would be ymin = mean + sd. Our plotted error bars now indicated plus or minus one standard deviation from the mean. ggplot(data = sumAtl, aes(x = city, y = mean)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() Some of the error bars indicate we could get a negative concentration of NO2. This is physically impossible, but it does suggest we should evaluate the distribution of our data (see below). Note that since were calculating error bar ranges on the fly, weve had to specify new aesthetic arguments to geom_errorbar(). 14.3.2 Ordering bar charts Often with bar charts (and similar plots), its useful to order the bars to help tell a story or convey information. We can effectuate this using fct_reorder(): ggplot(data = sumAtl, aes(x = fct_reorder(city, mean), y = mean)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() So in our aesthetics call for geom_bar we specified the x variable should be city, but ordered based on their corresponding mean value. Doing this has helped shed some light on trends in NO2 levels. For one, despite Labrador City having lower mean [NO2], we can now easily see that it has a larger variation in [NO2] then Corner Brook. 14.3.3 Grouping bar charts Sometimes youll want to group bar charts as in the concentration of several chemicals in different locations. We can easily group bar charts in ggplot. Lets go ahead and group our mean annual [NO2] by province by simply (1) reordering based on province, and (2) colour bars based on province: ggplot(data = sumAtl, aes(x = fct_reorder(city, p), y = mean, fill = p)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() There are other ways to group your bar charts depending on the story you want to tell and the data you have. Please consult the Grouped, stacked and percent stacked barplot in ggplot2 page from the R-graph-gallery. 14.4 Box Plots Box plots give a summary of the distribution of a numeric variable through their quartiles. Youve no doubt seen them before, but theyre often misinterpreted. Lets create a box-plot using geom_boxplot() and our Atlantic hourly NO2 measurements, then well break down how to interpret it. ggplot(data = atlNO2, aes( x = city, y = conc)) + geom_boxplot() + coord_flip() Lets break down how to interpret one box before tackling the entire set. As previously mentioned, box plots describe data in their quartiles. Quartiles basically arrange the data from the lowest to highest value and split the data at three points: The first quartile (Q1) is halfway between the lowest value and the median (50%) of the data. In other words 25% of the data lies below Q1. The second quartile (Q2) is the median. 50% of the data lies below, and 50% lies above this point. The third quartile (Q3) is halfway between the median and the highest value in the data. In other words, 75% of the data lies below Q3. The box in box-plots represents the range between Q1 and Q3. This is known as the inter-quartile range (IQR) and 50% of the total data falls somewhere inside this box. You can estimate the distribution by the symmetry of the box. if Q1 to the median is smaller than the median to Q3, the data has a positive skew (right sided skew), and vice versa. Rounding it out, geom_boxplot() includes whiskers, the thin lines emanating out from the box. This is used the predict outliers and is calculated as \\(outliers = \\pm 1.5 \\times IQR\\). Anything outside the whiskers is considered an outliers or an extreme point, and is plotted individually. Putting this all together, lets look at the [NO2] for St. Johns city: Note that weve plotted the actual distribution of the data. Prior to the computers, this was incredibly difficult to do, hence the use of box plots which can be drawn knowing only five points. However, the simplicity in calculating box-plots means they can hide trends and observations of your data. On top of that, they arent very intuitive (see the score of text needed to explain them). Consequently, we strongly recommend you explore some of the Box plot alternatives unless you are explicitly asked to create box-plots. 14.4.1 Box plot alternatives The first alternative to box-plots is the violin plot which is made using geom_violin(). It is similar to the box-plot, but instead of displaying the quartiles, it plots the density within each group and is a bit more intuitive then box-plots. While the example below isnt the most convincing given the scale of the dataset, violin plots are useful for identifying underlying trends in the distribution of data. For example, in the plot below we can see that some towns such as Marystown principle has days where [NO2] = 0 ppb, whereas Grand Falls-Windsor has a large number of days with low, but measurable levels of NO2. This might be because of difference in regional ambient levels of NO2. ggplot(data = atlNO2, aes(x = city, y = conc, fill = p)) + geom_violin() + coord_flip() Another alternative is to plot the points over top of the box-plot. Youve encountered this example in R coding basics. Truth be told, there are countless way to visualize distribution. 14.4.2 Statistical comparisons between groups Often box-plots are used to show differences in distributions between two groups (i.e. population in Location A vs. Location B). How you determine this statistically is a different story, but packages such as ggpubr have many built in functionalists to display the results of these outcomes. From our NO2 data, St. Johns appears to have the highest levels of NO2. Lets apply a pairwise test against other Newfoundland communities to see if our observation is statistically significant based upon the results of a Wilcoxon test. nfld &lt;- atlNO2 %&gt;% filter(p == &quot;NL&quot;) # only nfld stations # Code from ggpubr website ggpubr::ggviolin(nfld, x = &quot;city&quot;, y = &quot;conc&quot;) + ggpubr::stat_compare_means(ref.group = &quot;St Johns&quot;, method = &quot;wilcox.test&quot;, label = &quot;p.signif&quot;) Based on the results of our test, all other stations in Newfoundland have statistically significant differences in the median NO2 values. Note the validity of this statistical approach to this particular problem is called into question based on the distribution of the data etc. Weve included it to demonstrate how to label significance on plots, rather than an explicit discussion on statistics. For more information on ggpubr, adding p-values and significance labels, and different pairwise statistical test please visit ggpubr: Publication Ready Plots. 14.5 Histograms Histograms are an approximate representation of the distributions of numerical data. Theyre an approximation because you arbitrarily bin your data into groups and then count the number of values inside that bin. The frequency, or count, in each bin is represented by the height of a rectangle whose width equals that of the bin. geom_histogram() is used to create histograms: ggplot(data = subset(atlNO2, city = &quot;St Johns&quot;), aes(x = conc)) + geom_histogram() + labs(subtitle = &quot;Distribution of St. Johns&#39; NO2 levels in 2018&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can alter the resolution of our histogram by modifying the width of the bins using the binwidth argument or by specifying the number of bins with the bins argument. The former is useful when you dont know the range of your data, whereas the latter is useful is you do (i.e. numbers between 0 and 100). ggplot(data = subset(atlNO2, city = &quot;St Johns&quot;), aes(x = conc)) + geom_histogram(binwidth = 1) + labs(subtitle = &quot;Distribution of St. Johns&#39; NO2 levels in 2018, binwidth = 1&quot;) 14.5.1 Multiple histograms While you can overlap histograms, it gets difficult to read with more than a handful of datasets. If we wanted to plot histograms of all the cities in our dataset we would have to use a small multiple via the facet_grid() or facet_wrap() arguments. facet_grid() allows you to arrange many small plots on a grid defined by variables in your dataset (i.e. columns for provinces, and rows for different pollutants). In the example below weve used facet_wrap(~city) which creates a 2D layout of histograms of each cities NO2 values. Note the tilde , ~, preceding in ~city. ggplot(data = atlNO2, aes(x = conc, fill = p)) + geom_histogram(binwidth = 1, position = &quot;identity&quot;) + facet_wrap(~city) 14.6 Scatter plots Scatter plots display values of two variables, one of which is a continuous variable. Each data point ins plotted as an individual point on.Youve already made scatter plots in the form of a time series during the Section 1 tutorial exercise. Weve already touched upon scatter plots during the [Linear Regression] chapter where we also overlaid our linear model over our concentration points. So now well touch upon some things you can do to improve your scatter plots. 14.6.1 Marginal plots You can easily combine a scatter plot with marginal plot. This is useful to summarize one dimension of our scatter plot. For example, lets revisit the time series plot we made in R coding basics. We might want to know the distribution of concentrations of the individual pollutants. using the ggExtra package and the ggMarginal() function we can get the following: torontoAir &lt;- read_csv(&quot;data/2018-01-01_60430_Toronto_ON.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## naps = col_double(), ## city = col_character(), ## p = col_character(), ## latitude = col_double(), ## longitude = col_double(), ## date.time = col_datetime(format = &quot;&quot;), ## pollutant = col_character(), ## concentration = col_double() ## ) # note we&#39;re storing our plot in the variable &#39;torPlot&#39; # and we&#39;re not plotting SO2 torPlot &lt;- ggplot(data = subset(torontoAir, pollutant != &quot;SO2&quot;), aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() + theme(legend.position = &quot;bottom&quot;) # We&#39;re passing our torPlot to the ggMarginal Function ggExtra::ggMarginal(torPlot, margins = &quot;y&quot;, groupColour = TRUE, groupFill = TRUE) We can now see the distributions of NO2 and O3 overlaid on the vertical axis. note that ggMarginal() only works with scatter plots. There are plenty of other marginal options scatted about various packages. You can see many of them in action (with beautiful examples) at Tufte in R by Lukasz Piwek. 14.7 Interactive plots Ultimately youre visualizations will be printed to a static PDF document, but in the interim having an interactive plot can be helpful for data exploration. The plotly package magically makes most ggplots interactive with a simply command. Heres an example with our Toronto air quality data: torPlot2 &lt;- ggplot(data = torontoAir, aes(x = date.time, y = concentration, colour = pollutant)) + geom_line() plotly::ggplotly(torPlot2) This is also super useful when surveying spectroscopy data, although the large number of points in those datasets can take a while to render into interactive plotly plots. 14.8 Plotting for publication Up until now we havent payed much attention to the explicit aesthetics of plots beyond what we needed for our exploratory analysis. However, many journals, publications, instructors, etc. will want plots to adhere to certain aesthetic standards. Theres scores of options to play with, so we recommend you consult the ggplot2 Cheat Sheet. 14.8.1 Plot Themes Overall themes can be applied to ggplot. The simple and minimalist theme_classic() is satisfactory for most submissions, but you can peruse the available these in ggplot here or you can explore many more themes in the ggthemes package. # generating example plot to modify p &lt;- ggplot(data = torontoAir, aes(x = date.time, y = concentration, colour = pollutant)) + geom_point() # default theme default &lt;- p + labs(subtitle = &quot;Default geom_scatter&quot;) # Classic theme classic &lt;- p + theme_classic() + labs(title = &quot;theme_classic()&quot;) # arranging into grid gridExtra::grid.arrange(default, classic, ncol = 2) 14.8.2 Legends You can specify the position of the legend under the theme() option as such: bottom &lt;- p + theme(legend.position = &quot;bottom&quot;) inside &lt;- p + theme(legend.position = c(.95, .95)) gridExtra::grid.arrange(bottom, inside, ncol = 2) Other legend positions include: none, left, right, bottom, top, or a two-element numeric vector to specify the location such as c(0.95, 0.95) for inside the top-right corner. c(0.05, 0.05) would place it inside the bottom right corner, and so on. Also note that legend.position = \"none\" will remove the legend entirely. 14.8.3 Modifying labels The labels generated for the plots are derived from the variable names passed along to the ggplot() function. Consequently, variable names that are easy to code become ugly labels on the plot. You can modify labels using the labs() function. Note in the example below that we changed the legends title by specifying what aes() option we used to create the legend; in the example below its colour. p + labs(title = &quot;Toronto Air quality&quot;, subtitle = &quot;from Jan 1st to 8th, 2018&quot;, xlab = &quot;Date&quot;, ylab = &quot;Concentration (ppb)&quot;, colour = &quot;Pollutant&quot;) 14.8.4 Modifying Axis Weve already talked about labelling axis titles in Modifying labels, and adding marginal plots in Scatter plots. So well just briefly touch upon some simple axis modifications. 14.8.4.1 Transforming axix Transformations are largely related to continuous data, and are done using scale_y_continuous() or scale_x_continuous() functions. For example to scale the y-axis of our plot wed do the following: p + scale_y_continuous(trans = &quot;log10&quot;) + labs(y = &quot;Log10(concentration)&quot;) Other useful transformations include log2 for base-2 logs, date for dates, and hms for time. The later two are useful if R hasnt correctly interpreted your dataset. The data type for the data.time column of our dataset was correctly interpreted during our initial importation using read_csv(). Hooray for doing it right the first time. 14.8.4.2 Limits The limits of plots created with ggplot() are automatically assigned, but you can override these using the lims() function. For example we can specify the limits of our example plot to show from 0 to 100 ppb: p + lims(y = c(0, 100)) #### Axis ticks/labels Sometimes when you are plotting, the length of the axis labels is unreadable. This is often the case with categorical data, such as the name of cities like weve encountered earlier. We addressed this earlier in [Bar charts] by rotating the plot 90\\(^\\circ\\) with the coord_flip() function. This is often the best solution as its how we read English. Another solution is to rotate the axis labels themselves: basePlot &lt;- ggplot(data = subset(sumAtl, p == &quot;NL&quot;), aes(x = city, y = mean)) + geom_col() default &lt;- basePlot + labs(title = &quot;default plot&quot;) flip &lt;- basePlot + coord_flip() + labs(title = &quot;coord_flip()&quot;) rotated &lt;- basePlot + theme(axis.text.x = element_text(angle = 45)) + labs(title = &quot;element_text(angle = 45)&quot;) rotatedHJust &lt;- basePlot + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;element_text(angle = 45, hjust = 1)&quot;) gridExtra::grid.arrange(default, flip, rotated, rotatedHJust, ncol = 2, nrow = 2) ## Arranging plots We talked about how facets can be used to generate multiple plots from a dataset (small multiples), but sometimes you want to combine two or more different plots together. There are a couple of ways, but weve been using grid.arrange() from the gridExtra pacakge (as demonstrated above). You can read up on gridExtra here. There is also the ggarrange function from the ggpubr package which, amongst other things, can easily create shared legends between plots. colchart &lt;- ggplot(data = sumAtl, aes(x = fct_reorder(city, mean), y = mean, fill = p)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd)) + coord_flip() boxplot &lt;- ggplot(data = atlNO2, aes( x = city, y = conc, fill = p)) + geom_boxplot() + coord_flip() boxplot ggpubr::ggarrange(colchart, boxplot, ncol = 2, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;), common.legend = TRUE, legend = &quot;bottom&quot;) 14.8.5 Anotating plots Everyplot can do with a bit of annotation. These range from providing critical information for contextualizing and understanding your plot to pointing out something you think the reader might miss but should know. These are different then captions, which is accomplished in the rmarkdown chunk header (see R code chunk options for a refresher). Lets quickly plot a map of annual mean 1-hr [NO2] in our dataset so we can visualzie them spatially. Note, the map were making here is rather basic, to make prettier maps see CHM410: Air Quality Lab. # need lat and long value for map mapNO2 &lt;- atlNO2 %&gt;% group_by(latitude, longitude, p, city) %&gt;% summarise(mean = mean(conc)) #install.packages(&quot;ggmap&quot;) library(ggmap) atlMap &lt;- get_stamenmap(bbox = make_bbox(lon = mapNO2$longitude, lat = mapNO2$latitude, f = 0.1), zoom = 6, maptype = &quot;terrain&quot;, crop = FALSE) atlMap &lt;- ggmap(atlMap) atlMap Now we want to plot our annual mean 1-hr [NO2] onto the map. Weve covered this in detail in Plotting Airbeam data spatially, but for this plot we spefically want to annotate each point with its corresponding city location. Doing this manually would take ages, so were going to use the ggrepel package. We simply need to specify which column (naps id) well use for our labels: atlMap + geom_point(data = mapNO2, aes(x=longitude, y=latitude, colour = mean, size = mean), alpha = 0.8) + scale_alpha(guide = &quot;none&quot;) + # removing legend for alpha scale_size(guide = &quot;none&quot;) + # removing legend for size ggrepel::geom_label_repel(data = mapNO2, aes(x=longitude, y=latitude, label = city), box.padding = 0.5, max.overlaps = Inf) Again, not the prettiest map, but thats up to you to fix in post. geom_text_repel() is an incredibly useful package for quickly annotating plots. If you need to label/annotate points check it out. "],["modelling-linear-regression.html", "Chapter 15 Modelling: Linear Regression 15.1 Modelling Theory 15.2 Modelling in R 15.3 Visualizing models 15.4 Calculating Concentrations 15.5 Summary 15.6 Further reading 15.7 Going deeper with modelling", " Chapter 15 Modelling: Linear Regression Modelling is basically math used to describe some type of system, and they are a forte of R, a language tailor made for statistical computing Every model has assumptions, limitations, and all around tricky bits to working. There is no shortage of modelling in a myriad of context, but in this chapter well discuss and break down the most common model youll encounter, the linear regression model, in the most common context, the linear calibration model, using the most common function, lm(). You have probably encountered the linear regression model under the pseudonym trend-lines, most likely generated by Excels add trend-line option (as in CHM135). While the models well be constructing with lm() work much the same mathematically, unlike Excel, R returns alllll of the model outputs. Correspondingly, its easy to get lost between juggling R code, the seemingly endless model outputs, and keeping yourself grounded in the real systems youre attempting to model. To this end, this chapter is broken into the following parts: Modelling theory where we briefly touch upon what model is being calculated. Modelling in R where we provide a boilerplate template for how to calculate models in R. Visualizing models where we explore our model results. Calculating Concentrations where we use our model to calculate the concentration. 15.1 Modelling Theory The linear calibration model relates the response of an instrument to the value of the measurand. The measurand is simply what were measuring, often the concentration of an analyte. So we use the measurand, which we can control via preparation of standards from reference material as the independent variable, with the instrument output being the dependent variable (as instrument response varies with concentration). Altogether were: Measuring the instrument response of standards of known concentration and samples of unknown concentration. Calculating the linear calibration model (i.e. line of best fit) through our standards. Using the measurement model to calculate the concentration in our unknown from their respective instrument response. Figure 15.1: Linear calibration model; figure modified from Hibbert and Gooding (2006). Before we can calculate concentrations, we need a measurement model. In other words, an equation that relates instrument response to sample concentration (or other factors). For simple linear calibration, we use: \\[ y = a + bx\\] Where: \\(y\\) is the instrument response \\(x\\) is the independent variable (i.e. sample concentration) \\(a\\) and \\(b\\) are the coefficients of the model; otherwise known as intercept and slope, respectively. Well gloss over some of the more technical aspects of modelling, and discuss other in more detail below. For now, know that: Were assuming our linear model is correct (i.e. the instruments actually responds linearly to concentration). All uncertainties reside in the dependant variable \\(y\\) (i.e. no errors in preparation of the standards). The values of \\(a\\) and \\(b\\) are determined by minimizing the sum of the residuals squared. The residuals are the difference between the actual measured response and where it would be if it were on the calibration line. Once we have our line of best fit, we can calculate the concentration of our unknown sample \\(i\\), from its measured response \\(y_i\\) by: \\[ x_i = \\frac{y_i ~-~b}{a}\\] There is more going on under the hood then what were describing here, but this should be enough to get you up and running. If you would like a greater breakdown of linear calibration modelling, we suggest you read Chapter 5 of Data Analysis for Chemistry by Hibbert and Gooding. An online version is accessible via the University of Torontos Library. Also there is no reason the instrument response must be linear. In fact, we spend a great deal of time arranging our experiment to that we land in the linear range. For details on non-linear modelling in R see [Non-Linear Regression] 15.2 Modelling in R Now that we have a rough understanding of what were trying to do, lets go over how to calculate linear regression models in R. Note model is a general term, in this situation well be calculating a calibration curve. All calibration curves are models, but not all models are calibration curves. For our example dataset well import a dataset consisting of four analytical standards of sodium plus a calibration blank all run in triplicate. The standards were measured via flame atomic emission spectroscopy (FAES). Lets import the FAES calibration results we saw in [Transform: dplyr and data manipulation]. As weve already seen, our data is composed of four standards and a blank analyzed in triplicate. Since were focusing on modelling, well treat the blank as a standard in our model fitting. So lets import our dataset: # Importing using tips from Import chapter FAES &lt;- read_csv(file = &quot;data/FAESdata.csv&quot;) %&gt;% pivot_longer(cols = -std_Na_conc, names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) %&gt;% separate(col = std_Na_conc, into = c(&quot;type&quot;, &quot;conc_Na&quot;, &quot;units&quot;), sep = &quot; &quot;, convert = TRUE) %&gt;% mutate(type = &quot;standard&quot;) DT::datatable(FAES) And lets quickly plot our data. You should always visualize your data before modelling, especially for linear calibration modelling. Unlike statistical modelling. Visualizing your data is the easiest way to spot trends and gross errors in your data. ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() 15.2.1 Base R Linear Model Rs base lm() function for linear regression is excellent, but its outputs have some messy quirks. Its easier to show that, so lets calculate the linear relationship between the signal as a function of conc_Na: lm_fit &lt;- lm(signal ~ conc_Na, data = FAES) lm_fit ## ## Call: ## lm(formula = signal ~ conc_Na, data = FAES) ## ## Coefficients: ## (Intercept) conc_Na ## 2615 29707 Reading the code above (recall that were reading it from right to left because its base R): Were taking the FAES data we created earlier; data = FAES Were comparing signal (the dependent, y, variable) to conc_Na (the independent, x, variable) via the tilde ~. The way to read this is: Signal depends on concentration. Were comparing these two variables using the lm() function for generalized linear models. All of the model outputs are stored in the lm_fit variable. As we can see, the model outputs are pretty brief and not much more than Excels outputs. We can use summary() to extract more information to better understand our model: summary(lm_fit) ## ## Call: ## lm(formula = signal ~ conc_Na, data = FAES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2112.78 -1528.53 70.51 821.50 2718.20 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2615.1 665.2 3.931 0.00172 ** ## conc_Na 29707.2 1304.5 22.772 7.34e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1824 on 13 degrees of freedom ## Multiple R-squared: 0.9755, Adjusted R-squared: 0.9737 ## F-statistic: 518.6 on 1 and 13 DF, p-value: 7.341e-12 Now we have a lot more information from our model (dont worry about what everything means, its discussed further in Section 3. For now, understand that its a hot mess. 15.2.2 Cleaning up model ouputs summary() provides a decent overview of our models performance, but the outputs are difficult to work with. Lets turn to the broom() package to clean up our model outputs. library(broom) calCurve &lt;- FAES %&gt;% group_by(type) %&gt;% nest() %&gt;% mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x)), tidied = map(fit, tidy), glanced = map(fit, glance), augmented = map(fit, augment) ) calCurve ## # A tibble: 1 x 6 ## # Groups: type [1] ## type data fit tidied glanced augmented ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 standard &lt;tibble [15 x ~ &lt;lm&gt; &lt;tibble [2 x ~ &lt;tibble [1 x 1~ &lt;tibble [15 x ~ Things look a bit more complicated than our earlier example, so lets break down our code line by line: Were taking the FAES dataset that we created earlier. group_by(type) groups all rows by type, in this situation we have only one type: standard. nest() collapses everything other than the type column into smaller data-frames. In this situation, all other information is stored as a tibble under the data column; this is the data used to calculate the linear model. Withing the mutate function, weve created four columns: fit, tidied, glanced, and augmented. And its these columns that contain our cleaned up model outputs. fit contains the raw output from the linear regression model for signal as a function of conc_Na using the lm() function. The output is in the form of a list, similar to what summary() gave us above. Again, this is exceptionally messy, hence why we used the tidy(), glance(), and augment() functions from the broom package . map() just means were applying the function tidy() to the individual output list created by lm() and stored in the fit column. Note that the tidy(), glanced(), and augmented outputs are tibbles. So we now have a tibble containing specific model output values (i.e. (Intercept)), lists (i.e. fit), and tibbles (tidied). This is known as **nested data*. Were no longer in Kansas anymore Well break down what each function did below. Keep in mind however that lm() is used for a variety of statistical tests, and consequently has many associated outputs. Some are essential, others are useful, and some are useless for linear calibration. There are also many ways to use these additional model outputs to calculate outliers, etc. but you shouldnt have any outliers in your calibration model. Dont rely on statistics to bail you out of poor chemical technique. 15.2.3 Glanced outputs Anyways, lets take a look at our model results. The glanced tibble contains a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire mode[^linear-regression-1] Because the data is nested, well need to use unnest() to flatten it back out into regular columns: glanced &lt;- calCurve %&gt;% unnest(glanced) # DT is to make interactive tables for the book. DT::datatable(glanced, options = list(scrollX = TRUE)) What you see here is a bit more than what youd get from Excels line-of-best fit output. In brief, : type, data, fit, tidied, and augmented are columns weve created earlier. r.squared is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable. The closer r.squared is to 1, the more variance is captured by the model. adj.r.squared is the same as r.squared in this situation. This is because r.squared will always increase if we add more exploratory variables to our model; the adj.r.squared accounts for the number of exploratory variables used in the model. In our case we only have one exploratory variable, hence theyre approximately the same. The other columns are different measurements of goodness-of-fit and hypothesis testing of the model. See Further reading. 15.2.4 Tidied outputs But what about the slope and the intercept? After all, thats what we need to calculate the concentration in our unknowns. Lets take a look at tidied from the tidy() function which constructs a tibble that summarizes the models statistical findings. This includes coefficients and p-values for each term in a regression:] # storing because we&#39;ll use it later on. tidied &lt;- calCurve %&gt;% unnest(tidied) # DT is to make interactive tables for the book. DT::datatable(tidied, options = list(scrollX = TRUE)) Again, a lot more to unpack compared to Excel. Thats because the lm() function in R calculates a generalized linear model. lm() performs a linear regression model, which we normally think of as an equation of the form \\(y= a + bx\\) as discussed earlier. But, regression models can be expanded to account for multiple variables (hence multiple linear regression) of the form: \\[y = \\beta _{0} + \\beta _{1} x_{1} + \\beta _{2} x_{2} ... \\beta _{p} x_{p}\\]] where, \\(y\\) = dependent variable \\(x\\) = exploratory variable; theres no limit how many you can input \\(\\beta _{0}\\) = y-intercept (constant term) \\(\\beta _{p}\\) = slope coefficient for each explanatory variable With our linear calibration model, we only have one input variable for our model (conc), so the above formula collapses down to \\(y = \\beta _{0} + \\beta _{1} x_{1}\\). So looking at our tidied model outputs: each row corresponds to a model coefficient (under the term column). For each modelling parameter, were provided an estimate of its numerical value: estimate. These are the values well use to calculate concentration. std.error measures how precisely the model estimates the coefficients unknown value; smaller is better. p.value is a indication of the significance of a model coefficient; the closer to zero the better. If we were to use multiple parameters in our model (eg. concentration and temperature) we could use the p.value to determine if a given coefficient was useful for our model. We can extract the value of the model coefficients for subsequent calculations as follows: # intercept a &lt;- as.numeric(tidied[1,5]) # slope b &lt;- as.numeric(tidied[2,5]) paste(&quot;The equation of our calibration curve is: y = &quot;, a, &quot; + &quot;, b,&quot;x&quot;, sep=&quot;&quot;) ## [1] &quot;The equation of our calibration curve is: y = 2615.11945030675 + 29707.1701380368x&quot; 15.2.5 Augmented outputs Finally lets take a look at the outputs of the augment function: # storing because we&#39;ll use it later on. augmented &lt;- calCurve %&gt;% unnest(augmented) # DT is to make interactive tables for the book. DT::datatable(augmented, options = list(scrollX = TRUE)) As you can see, augment() adds columns to the original data that was modelled. For our purposes were interested: signal and conc_Na, the original data used in our model. .fitted, the predicted value of the point according to our calculated model. .resid, the residuals of that point (different between measured and fitted values.) The other parameters are different measurements of the influence of each point on the model fitting. They can be used to detect outliers; see Further reading 15.2.6 Why we approach modelling this way You may be wondering why weve seemingly overcomplicated a simple enough procedure. Fair enough, weve showcased an analysis with a simple data set. However, as you progress in your studies youll be quantifying many compounds, often at the same time in the same instrument runs. If you organize your data in a tidy format, you can plot calibration curves for all of your compounds with the same block of code. Essentially you use group_by() to group your data by compound/element. Subsequently, the same code is expandable from 1 compound to as many as you can ever hope to quantify in one shot. So for upper year labs where youre analyzing tens of compounds (cough CHM410 Dust Lab) you can generate calibration curves for all your compounds at once. 15.3 Visualizing models At the top of the chapter we plotted out standards to visualize a linear trend. Visualizations is an essential component when calculating calibration curves, and indeed our standards appeared to follow a linear trend, which was corroborated by the model we calculated above. However, for publications/reports youll need to create a plot with both your standards and model with the displayed equation, so below is a bit of stock code you can use as a starting point to create these plots. Note that it requires the ggpmisc package to display the equation: ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=F) + ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE, size = 3) ## `geom_smooth()` using formula &#39;y ~ x&#39; 15.3.1 Vizualizing residuals As stated earlier, residuals are the difference between measured and fitted values. Theyre often overlooked in linear calibration and folks are hot to plot a straight line through their data. This has the unintended effect of fooling your eyes into thinking your data is linear. Consequently, it is always a good idea to plot the residuals of your model as this will magnify any trends or discrepancies of your calibration model. A good linear model will have the residuals randomly distributed about zero. Other examples of patterns in residuals are shown below: Figure 15.2: Example residual patterns; figure adapted from Hibbert and Gooding (2006). Normally distributed residuals is satisfactory for linear modelling. Note the relatively small magnitude of the residuals. Curvature throughout range results from an instrument become saturated. Consequently, the linear model will cut through the curve. This is a good indication that youll need to either breakdown your calibration curve into two or more parts or utilize a non-linear model. Heteroscedasticity means the variance of the response is proportional to the concentration. This is often the case in instrumental analysis. See Weighing below. Outliers shouldnt exist in your calibration plot, nevertheless, a plot of residuals can readily highlight an outlier point. 15.3.2 Plotting residuals To plot residuals, we use our augmented dataset from above, and simply create a plot of the independent variable vs. the residuals. Here we plot the FAES calibration model and its residuals. Note that the residuals indicate curvature throughout the range. We may have over extended our calibration range outside of the linear range of our instrument. a &lt;- ggplot(data = FAES, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=F) + ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names rr.digits = 4, # reported digits of r-squared aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE, size = 3) b &lt;- ggplot(data = augmented, aes(x = conc_Na, y = .resid)) + geom_point() ggpubr::ggarrange(a, b, ncol = 2, labels = c(&quot;A&quot;, &quot;B&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 15.3: (A) linear calibration model and (B) plot of model residuals. 15.4 Calculating Concentrations In Tidied outputs we extracted our model coefficients (slope and intercept): # intercept a &lt;- as.numeric(tidied[1,5]) # slope b &lt;- as.numeric(tidied[2,5]) paste(&quot;The equation of our calibration curve is: y = &quot;, a, &quot; + &quot;, b,&quot;x&quot;, sep=&quot;&quot;) ## [1] &quot;The equation of our calibration curve is: y = 2615.11945030675 + 29707.1701380368x&quot; Now that we have our coefficients, we can calculate the sample concentration as described in Modelling Theory. Lets import the FAES unknown dataset first: FAESsamples &lt;- read_csv(file = &quot;data/FAESUnknowns.csv&quot;) %&gt;% pivot_longer(cols = -c(sample, `dilution factor`), names_to = &quot;replicate&quot;, names_prefix = &quot;reading_&quot;, values_to = &quot;signal&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## sample = col_character(), ## `dilution factor` = col_double(), ## reading_1 = col_double(), ## reading_2 = col_double(), ## reading_3 = col_double() ## ) FAESsamples ## # A tibble: 9 x 4 ## sample `dilution factor` replicate signal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 wine 10 1 12775. ## 2 wine 10 2 12651. ## 3 wine 10 3 12746. ## 4 tap-water 10 1 8014. ## 5 tap-water 10 2 7200. ## 6 tap-water 10 3 7203. ## 7 fish-tank 100 1 12085. ## 8 fish-tank 100 2 12073. ## 9 fish-tank 100 3 12156. Now its simply a matter of calculating the concentration of the sample analyzed by the instrument, and correcting for the dilution factor to find the concentration in the parent sample: FAESsamples &lt;- FAESsamples %&gt;% mutate(&quot;instConc&quot; = (signal - a)/b, &quot;sampleConc&quot; = instConc * `dilution factor`) DT::datatable(FAESsamples, options = list(scrollX = TRUE)) And lets summarize our results using code from the Summarizing data chapter: FAESsamples %&gt;% group_by(sample) %&gt;% summarize(mean = mean(sampleConc), sd = sd(sampleConc), n = n()) ## # A tibble: 3 x 4 ## sample mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 fish-tank 31.9 0.150 3 ## 2 tap-water 1.64 0.158 3 ## 3 wine 3.40 0.0219 3 And there we go. See the Summarizing data and Visualizations chapters for assiting an making prettier tables and visualizations, respectively. 15.5 Summary In this chapter weve covered: The essential theory undergirding the linear calibration model. How to scallably model in R with a consistent workflow for one or thousands of linear calibration models. How to extract model parameters using the tidy(), glance(), and augment() functions from the broom package. A brief overview of model parameters as they pertain to linear calibration. Viasualizing models and the importance of visualizing residuals. Calculating sample concentration from model outputs. 15.6 Further reading As previously stated, we highly recommend reading Chapter 5: Calibration from Data Analysis for Chemistry by Hibbert and Gooding for a more in-depth discussion of linear calibration modelling. The book can be accessed online via the University of Torontos Library. For a greater discussion on modelling in R, see Modelling in R for Data Science. 15.7 Going deeper with modelling 15.7.1 Weighing As shown above, youll often find that your calibration data is heteroscedastic, meaning the variance increases with the concentration. This leads to leverage of your line-of-best fit, as it is pulled by one way or another by the higher concentration standards then the lower. You can assign weights (how much a point impacts the model) in R, although youll need to justify the validity of your approach. A common approach however, is to weight each standard by \\(\\frac{1}{x^2}\\). This ensures that samples with higher concentration impact the line less, and vice-versa with low-concentration standards. To utilize weight in R, we need to calculate the weigh prior to modelling, and subsequently specify the weights column # note that our blank has a concentration of 0, hence infinite weight. # we need to remove it to weight our data. FAESweighed &lt;- FAES %&gt;% filter(conc_Na &gt; 0) %&gt;% mutate(wght = 1/(conc_Na^2)) weightedCalCurve &lt;- FAESweighed %&gt;% group_by(type) %&gt;% nest() %&gt;% mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x, weights = wght)), tidied = map(fit, tidy), glanced = map(fit, glance), augmented = map(fit, augment) ) %&gt;% unnest(augmented) ggplot(data = weightedCalCurve, aes(x = conc_Na, y = signal)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=F, colour = &quot;red&quot;, label = &quot;unweighed&quot;) + geom_smooth(method = &#39;lm&#39;, se=F, colour = &quot;blue&quot;, aes(weight=`(weights)`, label = &quot;weighed&quot;)) ## Warning: Ignoring unknown parameters: label ## Warning: Ignoring unknown aesthetics: label ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["modelling-non-linear-regression.html", "Chapter 16 Modelling: Non-Linear Regression 16.1 Experimental Background 16.2 Modelling Sigmoid Curve 16.3 Summary", " Chapter 16 Modelling: Non-Linear Regression Weve touched upon the basics of modelling in R but it doesnt have to stop there. This chapter will expand upon the contents of Modelling: Linear Regression to cover non-linear regressions. Since we cant account for the myriad of models utilized throughout the field, well work through a case-study. 16.1 Experimental Background For this chapter well be using data obtained from an experiment in CHM317. In this experiment, students measure the fluorescence of the fluorescent dye acridine orange in the presence of sodium dodecyl sulfate (SDS). In, or near, the critical micellular region of SDS, there is a sharp change in absorbance and fluorescence of the solution. Tracking these changes in fluorescence, students can then estimate the CMC of SDS. Experimentally, students prepared solutions of a constant concentration of acridine orange and varying concentrations of SDS. The emission spectrum of each sample was recorded, and we want to take the maximal of each spectra as a data point to built our model. Lets go ahead and import our data: library(tidyverse) sds &lt;- read_csv(&quot;data/CHM317/fluoro_SDSCMC.csv&quot;) %&gt;% pivot_longer(cols = !`Wavelength (nm)`, # select all columns BESIDES `Wavelength (nm)` names_to = c(&quot;conc&quot;, &quot;conc.units&quot;, &quot;chemical&quot;), names_pattern = &quot;(.*) (.) (.*)&quot;, values_to = &quot;intensity&quot;, names_transform = list(conc = as.numeric) ) %&gt;% rename(wavelength = &#39;Wavelength (nm)&#39;) # renaming column, less typing later on. head(sds) ## # A tibble: 6 x 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 500 0.001 M SDS 20.0 ## 2 500 0.0016 M SDS 18.6 ## 3 500 0.004 M SDS 7.02 ## 4 500 0.0048 M SDS 1.12 ## 5 500 0.0056 M SDS 5.48 ## 6 500 0.0064 M SDS 7.72 And a quick plot to visualize our data: ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() Alright, alright, alright. Things are looking like wed expect with some well behaved data. By plotting each point individually, we can really see the noise inherent with each reading. For a more robust analysis wed typically conduct several replicates and average out the spectra for each concentration or apply some kind of model to smooth each peak. But today, were just interested in getting the maximal fluorescence emission intensity from each reading. Lets first annotate our plate to find the highest point, then go about extracting our data for analysis. 16.1.1 Annotating maximal values Annotating the maximal point on the plot will take a bit more code then actually obtaining it from the data. For this well need to use the ggpmisc package which contains miscellaneous extensions for ggplot2, and ggrepel so our labels wont overlap. library(ggpmisc) library(ggrepel) ggplot(data = sds, aes(x = wavelength, y = intensity, colour = conc)) + geom_point() + ggpmisc::stat_peaks(span = NULL, geom = &quot;text_repel&quot;, # From ggrepel mapping = aes(label = paste(..y.label.., ..x.label..)), x.label.fmt = &quot;at %.0f nm&quot;, y.label.fmt = &quot;Max intensity = %.0f&quot;, segment.colour = &quot;black&quot;, arrow = grid::arrow(length = unit(0.1, &quot;inches&quot;)), nudge_x = 60, nudge_y = 200) + facet_grid(rows = vars(conc)) By faceting the plot (i.e. arranging many smaller plots vs. one large one), we can easily see the increase in emission peak intensity as the concentration of SDS increases. Likewise, we can avoid the messy overlap of the max intensity annotations. This is only one way to plot this data, but this is sufficient because were simply inspecting our data at this point. And here we can see that the intensity all occur around a similar wavelength (~ 528 nm) 16.1.2 Extracting maximal values The plots we made above are great for inspecting our data, but what we really want is the maximal emission intensity value to calculate the CMC of SDS. We can see the maximal values on the plots, but theres no way were typing those in manually. So lets go ahead and get out maximal values from our dataset: sdsMax &lt;- sds %&gt;% group_by(chemical, conc.units, conc) %&gt;% filter(intensity == max(intensity)) %&gt;% ungroup() head(sdsMax) ## # A tibble: 6 x 5 ## wavelength conc conc.units chemical intensity ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 520 0.0056 M SDS 28.1 ## 2 524 0.0072 M SDS 116. ## 3 527 0.0064 M SDS 65.3 ## 4 527 0.008 M SDS 768. ## 5 528. 0.012 M SDS 810. ## 6 528 0.0048 M SDS 22.0 All we did was tell R to take the row with the highest emission intensity value per group. We specified chemical, conc.units, and conc, in case we had more chemicals in our dataset. Our maximum values match those we see in our plot above. Lets see how they stack up against each other: ggplot(data = sdsMax, aes(x = conc, y = intensity)) + geom_point() Figure 16.1: Plot of maximal fluoresence intensity at various concentrations of SDS. 16.2 Modelling Sigmoid Curve So we want to find the critical micellular concentration of SDS using the maximum fluorescence emission. The CMC is at the midpoint of the sinusoid curve. Which means well need to a) plot a sinusoid curve and b) extract the midpoint. The sinusoid or S-shaped curve mentioned in the lab manual is known as a logistic regression. Logistic regressions are often used to model systems with a largely binary outcome. In other words, the system starts at point A, and remains there for awhile, before quickly jumping up (or down) to level B and remain there for the remainder. Examples include saturation and dose response curves. For our CMC working data, the fluorescence intensity is low when the \\([SDS] &lt; CMC\\), as micelles are not able to form. However once \\([SDS] &gt; CMC\\), micelles form and the fluorescence intensity increases. We can see this trend in 16.1. There are different forms of logistic regression equations. The simplest form is the 1 parameter, or sigmoid, function which looks like \\(f(x) = \\frac{1}{1+e^{-x}}\\). The outputs for this function are between 0 and 1. We could apply this formula to our model if we somehow normalized our fluorescence intensity accordingly. An alternative is to use the four parameter logistic regression, which looks like: \\[f(x) = \\frac{a - d}{\\left[ 1 + \\left( \\frac{x}{c} \\right)^b \\right ]} + d\\] where: a = the theoretical response when \\(x = 0\\) b = the slope factor c = the mid-range concentration (inflection point) This is commonly referred to as the EC50 or LC50 in toxicology/pharmacology. d = the theoretical response when \\(x = \\infty\\) Why do we need such a complicated formula for our model? Well, looking again at 16.1 we see that the lower point is approximately 20, and not zero. Likewise, the upper limit appears to be around 825. The slope factor is necessary because the transition from the low to high steady state occurs over a small, but not immeasurable, concentration range. And lastly, by including the inflection point, we can calculate exactly for this value using R to get the CMC estimate. 16.2.1 Calculating Logistic Regression A strength of R is its flexibility in running various models, and logistic regression is no different. We can use a number of packages to reach these ends, specifically the drc package contains a plethora of functions for modelling dose response curves (hence drc). However, for this example well use a more generalized approach. Earlier we talked about linear regression, where we plot adjust the slope and intercept of a linear equation to best fit our data (see Calibration Curves). Recall that this optimization is based on minimizing the distance between the model and all of the experimental points (least squares). Well the stats package has a function called nls that expands upon the this to nonlinear models. Per the nls function description: [nls] determine[s] the nonlinear (weighted) least_squared estimates of the parameters of a nonlinear model. So we can create a formula in R based on the four-parameter logistic regression described above. After that, well need to produce some starting details from which the model can build off of. If we dont tell nls where to start, it cant function, as the search space is too large. Looking at @ref{fig:sdsMaxPlot}, the intensity appears to floor around 20; the intensity appears to max out around 820; the midpoint appears to be around 0.0075 M, and lets say the slope is 1. Remember, these are starting values from which nls starts to optimize from, and not the actual values used to construct the model. So, lets create our model logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 1, # slope c = 0.0075, # CMC d= 820) # max intensity ) ## Error in numericDeriv(form[[3L]], names(ind), env, central = nDcentral): Missing value or an infinity produced when evaluating the model  and we get an error message. Get used to these when modelling! Dont worry about understanding it completely, error messages are often written with programmers in mind so they can be a bit cryptic. You can often copy and paste these directly into any search engine to get some more information, but this one is simple enough: we either have a missing value or an infinity produced. Well we have six input parameters in our model: a, b, c, d, our independent variable conc, and our dependant variable intensity. Weve also supplied starting values to all of them via the list we created inside the function. Therefore, one of our starting values must be too far off from a plausible start point and is causing troubles in the nls function. They all look good except for the slope start value b = 1. The slope here is an approximation for the slope between the min value a and max value d. Looking at our data in @ref{fig:sdsMaxPlot}, that slope may be a bit shallow consider the large jump in intensity. Lets increase the value of b and try again: logisModel &lt;- nls(intensity ~ (a-d)/(1 + (conc /c)^b) + d, data = sdsMax, start = list(a = 20, # min intensity b = 10, # new slope c = 0.0075, # CMC d= 820) # max intensity ) Eh, no errors! Once you progress beyond simple linear regressions, modelling becomes more of a craft. If we were trying to apply this model to multiple datasets, we would probably want to shop around cran to find a package with self-starting models. This way we can circumvent having to supply starting parameters. Anyways, thats for another day. For now, lets take a look at our model outputs which are all stored in the logisModel variable. To this end, well use the broom package discussed in Modelling: Linear Regression. Specifically, well use tidy to get an output of our estimated model parameters (i.e. a,b,c, and d), and augment for a data frame of containing the input values, and the estimated intensity values. Lets look at our fitted values: library(broom) augment &lt;- augment(logisModel) augment ## # A tibble: 9 x 4 ## intensity conc .fitted .resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28.1 0.0056 49.4 -21.3 ## 2 116. 0.0072 116. -0.123 ## 3 65.3 0.0064 49.7 15.6 ## 4 768. 0.008 768. 0.0977 ## 5 810. 0.012 810. -0.0861 ## 6 22.0 0.0048 49.4 -27.5 ## 7 93.0 0.001 49.4 43.5 ## 8 31.7 0.004 49.4 -17.7 ## 9 57.0 0.0016 49.4 7.51 What we can see here from augment are the intensity and conc values we inputted into R. .fitted are the intensity values for a given concentration fitted to out model, and .resid is the residuals, the difference between the actual and estimated values. Lets go ahead and plot our actual and fitted values against each other. ggplot(augment, aes(x = conc, y = intensity, colour = &quot;actual&quot;)) + geom_point() + geom_line(aes(y = .fitted)) + geom_point(aes(y = .fitted, colour = &quot;fitted&quot;)) Looks pretty good, although its interesting how the baseline at lower concentrations doesnt plateau like the model values. Youll note that the line produced by geom_line will only draw a straight line between points. Theres ways to address this, but we dont need to for our needs right now. There doesnt appear to be any gross outliers in our model, so it seems to have done a good job. We can verify this by checking the residuals(see Plotting residuals): ggplot(augment, aes(x = conc, y = .resid)) + geom_point() We cant see any obvious patterns in the residuals (i.e. all are negative), so we can have further confidence in the fit of out model. 16.2.2 Extracting model parameters To extract the model parameters a, b, c, and d we can use the tidy function: library(broom) tidy &lt;- tidy(logisModel) tidy ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 49.4 11.1 4.44 0.00678 ## 2 b 49.0 9.65 5.07 0.00385 ## 3 c 0.00755 0.0000785 96.2 0.00000000230 ## 4 d 810. 27.3 29.7 0.000000808 Looking past the scientific notation, our model values are pretty similar to what we estimated. Specifically, c, our midpoint value is 0.0076 M. Not too bad from our original estimate. And recall that the midpoint of our curve corresponds to the critical micellular concentration of SDS, which weve estimated to be 0.0076M. Not too far from the literature value of 0.0081 M. 16.3 Summary In this chapter we reviewed non-linear modelling using a case study with four-parameter logistic regression. While the equation covered here might not be the one you need, the steps are identical: Tidy and visually inspect your data to see and patterns Determine which mathematical model youll be using Use the nls or other suitable package to calculate your model; you may need to tinker around with the starting values, estimate them from your data. Verify your model outputs (both fitted and residuals). Lastly, weve also touched upon labelling maximal values in a plot using the ggpmisc package. Notably useful for determining local peaks in spectroscopy data. "],["chm410-air-quality-lab.html", "Chapter 17 CHM410: Air Quality Lab 17.1 Importing Data 17.2 Plotting data spatially 17.3 Summary", " Chapter 17 CHM410: Air Quality Lab The following code is a supplement to the CHM410: Air Quality lab where students measure hyper-local air quality data using a variety of portable sensors, notably the AirBeam. Putting the experimental design and underlying data aside, there are two principle hurdles when working with this data. Firstly there are idiosyncrasies in how each sensor records data frustrating importing and tidying the data into R. Secondly, how to visualize the spatial dimension of the AirBeam data. This section will address both of these issues by providing template code for tidying data from each respective sensor, and some example code illustrating how you can quickly map your spatial data to tell a story from your air quality recordings. Please note this section works towards merging multiple datasets together, so pay attention to how data is renamed to simplify this process. 17.1 Importing Data Either your TA or you will extract the air quality measurements in the form of a .csv file. So, assuming youre familiar with project design in R (if not see the R Tutorial Exercise) and the concepts of importing and tidying data (see Section 2 if youre not) 17.1.1 Importing Airbeam 2 data The AirBeam 2 sensor records temperature, relative humidity, PM1, PM2.5, and PM10 in the same file with second resolution. Lets go ahead and try to import the file directly: library(tidyverse) airbeamRaw &lt;- read_csv(&quot;data/CHM410_lab2/Airbeam2 July 25.csv&quot;) # showing first 30 lines of data DT::datatable(airbeamRaw[1:30,], options = list(scrollX = TRUE)) Notice that this is a complete mess; heres the rub: The first 8 rows of the data consists of metadata. This is information pertaining to how each sensor works. A byproduct of read_csv() is by default itll set the first row of data as headers, and change the column names so each is unique. This is why some columns are X1 and others are Sensor_Package_Name and Sensor_Package_Name_1. The data recorded by each sensor is stored in its own column, with accompanying metadata. the AirBeam take measurements at the same time, so it cycles across the five measurements within a second, hence theres only one recorded measurement per row, thats the recorded measurement for that given time. So to make our data tidy well need to separate the metadata from the real data, and clean up both the metadata and real data before merging the two together to get one holistic dataset we can explore. 17.1.1.1 Airbeam metadata As previously stated, the AirBeam metadata is the first 8 rows of data, so lets re-import our data with only the first 8 rows. airbeamMeta &lt;- read_csv(&quot;data/CHM410_lab2/Airbeam2 July 25.csv&quot;, n_max = 8) %&gt;% select(contains(&quot;Sensor&quot;)) airbeamMeta &lt;- as.data.frame(t(airbeamMeta)) %&gt;% rownames_to_column() airbeamMeta &lt;- airbeamMeta %&gt;% rename(sensor_name = V3, measurement_type = V5, measurement_units = V7, measurement = V8) %&gt;% select(-contains(&quot;V&quot;), -rowname) airbeamMeta ## sensor_name measurement_type measurement_units ## 1 AirBeam2-F Temperature degrees Fahrenheit ## 2 AirBeam2-PM1 Particulate Matter micrograms per cubic meter ## 3 AirBeam2-PM10 Particulate Matter micrograms per cubic meter ## 4 AirBeam2-PM2.5 Particulate Matter micrograms per cubic meter ## 5 AirBeam2-RH Humidity percent ## measurement ## 1 1:Measurement_Value ## 2 2:Measurement_Value ## 3 3:Measurement_Value ## 4 4:Measurement_Value ## 5 5:Measurement_Value A Lot of the code above is just work to pretty it up. From top to bottom weve: imported the first 8 rows of data removed the empty columns using select() Transposed the data frame (i.e. rotate 90 degrees) using the transpose function t(), transformed it into a data frame and saved it as airbeamMeta Renamed the columns so theyre more descriptive and easier to work with. After all of that were left with airbeamMeta, a data frame containing information on each sensor (sensor_name), measurement types and unites. Note that we kept a column called measurement this is a byproduct of the import procedure but well make use of it in the next step. 17.1.1.2 Airbeam Data Now lets get the actual AirBeam measurements into R. We noted earlier that the resolution of the AirBeam measurement is in seconds, but the time is recorded in milliseconds and the accompanying empty columns. To make our data easier to work with well round the recorded time of each measurement to the second using floor_date from the lubridate package before tidying our data. airbeam &lt;- read_csv(&quot;data/CHM410_lab2/Airbeam2 July 25.csv&quot;, skip = 8) %&gt;% mutate(Timestamp = lubridate::floor_date(Timestamp)) %&gt;% pivot_longer(cols = `1:Measurement_Value`:`5:Measurement_Value`, names_to = &quot;measurement&quot;, values_to = &quot;measurement_value&quot;, values_drop_na = TRUE) ## ## -- Column specification -------------------------------------------------------- ## cols( ## ObjectID = col_double(), ## Session_Name = col_character(), ## Timestamp = col_datetime(format = &quot;&quot;), ## Latitude = col_double(), ## Longitude = col_double(), ## `1:Measurement_Value` = col_double(), ## `2:Measurement_Value` = col_double(), ## `3:Measurement_Value` = col_double(), ## `4:Measurement_Value` = col_double(), ## `5:Measurement_Value` = col_double() ## ) airbeam ## # A tibble: 39,201 x 7 ## ObjectID Session_Name Timestamp Latitude Longitude measurement ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 &quot;starting at 4~ 2020-07-25 11:00:58 43.7 -79.6 3:Measuremen~ ## 2 2 &quot;starting at 4~ 2020-07-25 11:00:59 43.7 -79.6 1:Measuremen~ ## 3 3 &quot;starting at 4~ 2020-07-25 11:00:59 43.7 -79.6 5:Measuremen~ ## 4 4 &quot;starting at 4~ 2020-07-25 11:00:59 43.7 -79.6 2:Measuremen~ ## 5 5 &quot;starting at 4~ 2020-07-25 11:00:59 43.7 -79.6 4:Measuremen~ ## 6 6 &quot;starting at 4~ 2020-07-25 11:00:59 43.7 -79.6 3:Measuremen~ ## 7 7 &quot;starting at 4~ 2020-07-25 11:01:00 43.7 -79.6 1:Measuremen~ ## 8 7 &quot;starting at 4~ 2020-07-25 11:01:00 43.7 -79.6 5:Measuremen~ ## 9 8 &quot;starting at 4~ 2020-07-25 11:01:00 43.7 -79.6 2:Measuremen~ ## 10 8 &quot;starting at 4~ 2020-07-25 11:01:00 43.7 -79.6 4:Measuremen~ ## # ... with 39,191 more rows, and 1 more variable: measurement_value &lt;dbl&gt; Note that Timestamp data is in the col_datetime format, which means we can easily round down to the nearest second. After that we made our data tidy using pivot_longer. One more thing before we move on. Remember the measurement column from airbeamMeta? Well that same column with the same values is present in our data (we did create it after all). What this means is we can join airbeamMeta to airbeam. We can combine information from these two tables using a join() function. This will add the columns from airbeamMeta to airbeam, essentially annotating each row of data. For example, every row in airbeam1 where the measurement value is 1:Measurement_Value will now have information on the measurement type, the measurement units, and the sensor name. Ultimately this gives us more dimensions to analyze our data. airbeam &lt;- airbeam %&gt;% inner_join(airbeamMeta, by = &quot;measurement&quot;) %&gt;% rename_all(tolower) Weve merged both datasets based on the values in the measurement column using inner_join(). What this does is merge the rows wherever theres a match between the measurement columns. There are other forms of join function, which you can read up on here. 17.1.1.3 Visualizing Airbeam Data And a quick plot to see how our data looks: airbeamTime &lt;- ggplot(data = airbeam, aes(x = timestamp, y = measurement_value, colour = sensor_name)) + geom_line() + facet_grid(rows = vars(measurement_units), scales = &quot;free&quot;) airbeamTime Rad. Note that the scale of each plot is independent, because each measurement occurs in its own range. So take care when interpreting this data. And, given the resolution of the data, and the fact were just exploring it, we can transform our static ggplot into an interactive plotly plot using the plotly package: # You can also make an interactive map with the plotly package # beware this might take a while given the large dataset # you&#39;ll need to install the plotly package before hand. plotly::ggplotly(airbeamTime) Now you can zoom around and scope out the scene to see whats up. 17.1.2 Importing CO2 and O3 data The CO2 and O3 sensors have data in a much neater format, partly because theyre only recording one reading throughout a session. Consequently, importing is a relatively simple job. Since theyre both the same format, lets import them both and merge them into one dataset: co2 &lt;- read_csv(&quot;data/CHM410_lab2/CO2 July 25.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## `Date Time` = col_character(), ## `Monitor ID` = col_double(), ## `Location ID` = col_double(), ## `CO2(ppm)` = col_double() ## ) o3 &lt;- read_csv(&quot;data/CHM410_lab2/Ozone July 25.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## `Date Time` = col_character(), ## `Monitor ID` = col_double(), ## `Location ID` = col_double(), ## `O3(ppm)` = col_double() ## ) gases &lt;- inner_join(co2, o3, by = &quot;Date Time&quot;) %&gt;% select(-contains(&quot;ID&quot;)) tibble(gases) ## # A tibble: 176 x 3 ## `Date Time` `CO2(ppm)` `O3(ppm)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 Jul 2020 10:04 1284 0 ## 2 25 Jul 2020 10:05 1296 0 ## 3 25 Jul 2020 10:06 1259 0 ## 4 25 Jul 2020 10:07 1222 0 ## 5 25 Jul 2020 10:08 1370 0 ## 6 25 Jul 2020 10:09 1586 0 ## 7 25 Jul 2020 10:10 1438 0 ## 8 25 Jul 2020 10:11 1272 0 ## 9 25 Jul 2020 10:12 1235 0 ## 10 25 Jul 2020 10:13 1216 0 ## # ... with 166 more rows The CO2 and O3 measurements are pretty self evident, so lets focus on the Date Time column. Note that its stored in a character format. So you and I read it as a date and time, but to R its simply a string of characters with no meaning. So well need to coerce it into the datatime format using parse_date_time(). gases &lt;- gases %&gt;% rename(timestamp = `Date Time`, co2 = `CO2(ppm)`, o3 = `O3(ppm)` ) %&gt;% mutate(timestamp = lubridate::parse_date_time(timestamp, orders = &quot;%d-%b-%Y-%H-%M&quot;)) %&gt;% pivot_longer(cols = co2:o3, names_to = &quot;sensor_name&quot;, values_to = &quot;measurement_value&quot;, values_drop_na = TRUE) So what weve done is: - take our gases data and rename the columns to less problematic names - Mutated the timestampe data using parse_date_time(). - Working with date &amp; time data is a pain, in this case we needed to tell parse_date_time() that the string value were converting to datetime is written as the day, abbreviated month, full year, hour, and minutes. - You can read up on working with dates and times here. - We then made our data tidy using pivot_longer(). 17.1.2.1 Visualizing Gases Data And we can look at our data: ggplot(gases, aes(x = timestamp, y = measurement_value, colour = sensor_name)) + geom_line() + facet_grid(rows = vars(sensor_name), scales = &quot;free&quot;) 17.1.3 Merging datasets Accompanying this data set is a word document explaining how both the airbeam and gases data was measured. Essentially all of the sensors were tossed into a basket and someone rode a bike around Pearson airport. What that means is we can merge all the data together to see what transpired during the entire experiment. Before we merge the two datasets together, lets quickly annotate our gases data by adding measurement_type and measurement__units. gases &lt;- gases %&gt;% mutate(measurement_type = &quot;gases&quot;, measurement_units = case_when(sensor_name == &quot;co2&quot; ~ &quot;ppm&quot;, TRUE ~ &quot;ppb&quot;)) And we can perform a full_join() so that all the columns, remember theres more columns in the airbeam data (latitude, and longitude) are preserved. Note that by not specifying the columns, full_join() will match all columns that are found in both datasets; we took advantage of this property with out consistent naming convention. airport &lt;- full_join(airbeam, gases) ## Joining, by = c(&quot;timestamp&quot;, &quot;measurement_value&quot;, &quot;sensor_name&quot;, &quot;measurement_type&quot;, &quot;measurement_units&quot;) And we can plot them all together to see how the readings of the different measurements varied over time. ggplot(data = airport, aes(x = timestamp, y = measurement_value, colour = sensor_name)) + geom_line() + facet_grid(rows = vars(measurement_units), scales = &quot;free&quot;) To understand whats going on youll need to refer to Experimental details for airport transect 1 for a specific timeline of events, but we can see that: the O3 and CO2 sensors were on for a longer time then the AirBeam. Something drastic happened at approximately 12:50 (looking at the notes, this is when the sensors were brought indoors) O3 levels change even though the sensors remained outside. In the next section well explore the data from a spatial dimension to see how measurement values changed with location. 17.2 Plotting data spatially Spatial data in this context means well be plotting our data on a map, after all it was recorded while someone biked around the city. So the first thing well need to do is get a map. This is now going to touch a bit upon GIS (geographic information systems). This is a career in and of itself, so we wont delve too deeply into it as it gets real weird real quick. That being said, the ggmap package greatly simplified our task of getting a map upon which we can plot our data. What we want is a map with points for each individual measurement value so we can see how it changes as the sensors were moved through time and space. So first thing first is we need a map. Thanks to services like Google Maps and Open Street Maps anyone can get a map of any part of the world for free. The problem for us is making sure its the right size. Too big and we wont be able to tell where we went, too small and well miss some of the picture. This takes a bit of playing around with, but the following code gives you a good framework to start from. 17.2.1 Plotting Airbeam data spatially First step is to install and load the ggmap package. Now well need to figure out which size maps will cover our data. We do so by creating a bounding box, basically a square composed of latitude and longitude data for each vertices of the bounding box. So we pass the latitude and longitude data from our airbeam data set to the make_bbox() function (alongside a fudge factor f for a bit of wiggle room around each side). #install.packages(&quot;ggmap&quot;) library(ggmap) airbeamBBox&lt;- make_bbox(lon = airbeam$longitude, lat = airbeam$latitude, f = 0.1) airbeamBBox ## left bottom right top ## -79.67197 43.63128 -79.55640 43.67764 Now that we have our bounding box, we use get_stamenmap() to get the real world map contained in the geographic coordinates of the bounding box. Note that were using get_stamentmap(), which queries Stamen Maps, a spin-off of Open Street Maps. We didnt use Google Maps* because it requires registering with their API (meaning our R code can access Google Maps), which is a bit overkill for this lab. If you plan on doing more work like this in the future, it might be worth doing so. Anyways, Stamen map provides several different aesthetic of maps. Were going to use the terrain style so we can see waterways, elevations, and geographic features which may have impacted measurements. airbeamMap &lt;- get_stamenmap(bbox = airbeamBBox, zoom = 14, maptype = &quot;terrain&quot;, crop = FALSE) transectMap &lt;- ggmap(airbeamMap) transectMap And would you look at that. We have a beautiful map of Pearson Airport and surrounding areas. You can adjust the size of the map by changing the value of the zoom argument from 3 (Continental) to 15 (individual houses). Now lets overlay our measurement values. The ggmap() function transforms the map into a ggplot layer, which means we can treat it like any other ggplot, and simply add layers to it. In this instance, well superimpose the measurements onto the map allowing us to see how concentrations of PM10 varied with location: transectMap &lt;- transectMap + geom_point(data = subset(airbeam, sensor_name == &quot;AirBeam2-PM10&quot;), aes(x = longitude, y = latitude, colour = measurement_value)) transectMap Note that weve subset our data on the fly. Since the five measurements are all taken from the same device, we cant plot them on the same map because theyll overlap. So we subsetted our data using subset() so we only plot PM10 data. You can swap our the name passed to sensor_name == ... to plot the other measurements (i.e. PM1, or PM 2.5). Because theres a lot going on under the hood (i.e. we dont know where and when measurements were started and the route taken) lets annotate the map. This is done manually because we need to decide whats worth annotating and what we want people to know about our map. Lets add the starting, turnaround and end of the trip so people know the route taken, and the general timeline of recordings. The locations used for the annotations are geographic coordinates, and are used to place the arrows/text on the map. I went used the timeline notes to figure out the start, turnaround, and end of the journey. transectMapAnnotated &lt;- transectMap + # annotations for start of journey annotate(&quot;segment&quot;, x = -79.62161, xend = -79.621614, y = 43.635, yend = 43.65395, size=1) + annotate(&quot;label&quot;, x = -79.62161, y = 43.64, label = &quot;Started biking west \\n at 11:00am&quot;, fill = &quot;white&quot;) + # annotation for turn around annotate(&quot;segment&quot;, x = -79.65, xend = -79.66234, y = 43.655, yend = 43.66672, size=1) + annotate(&quot;label&quot;, x = -79.65, y = 43.655, label = &quot;Turned around \\n to head east \\n at 11:35am&quot;, fill = &quot;white&quot;) + # annotation for end of journey annotate(&quot;segment&quot;, x = -79.56662, xend = -79.56662, y = 43.657, yend = 43.63529, size=1) + annotate(&quot;label&quot;, x = -79.56662, y = 43.66, label = &quot;Entered house \\n at 12:50pm&quot;, fill = &quot;white&quot;) + # labels for plots labs(x = &quot;longitude&quot;, y = &quot;latitude&quot;, colour = &quot;PM 10 \\n(ug/m^3)&quot;) transectMapAnnotated And thats that. We can see a story of where the sensors went, and how PM 10 doesnt really seem to vary much with location. Lets see if its any different for the gases. 17.2.2 Plotting gases data spatially In the accompanying word document, its explained that both the airbeam and gases data was measured at the same time. Essentially all of the sensors were tossed into a basket and someone rode a bike around Pearson airport. That means that although the sensors for the gases data didnt record their location, since they were in the same basket as the airbeam sensors (which did record spatial data) we take that data from the latter and put it into the former. 17.2.2.1 Getting spatial data for gases One hiccup is that the airbeam data is recorded every second whereas the gases data is recorded every minute. This will cause some headaches when trying to match the data up. So well first extract the relevant data from the airbeam dataset, and round it down to the nearest minute. spatDat &lt;- airbeam %&gt;% select(timestamp, latitude, longitude) %&gt;% mutate(timestamp = lubridate::round_date(timestamp, unit = &quot;minute&quot;)) %&gt;% distinct(timestamp, .keep_all = TRUE) spatDat ## # A tibble: 132 x 3 ## timestamp latitude longitude ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-07-25 11:01:00 43.7 -79.6 ## 2 2020-07-25 11:02:00 43.7 -79.6 ## 3 2020-07-25 11:03:00 43.7 -79.6 ## 4 2020-07-25 11:04:00 43.7 -79.6 ## 5 2020-07-25 11:05:00 43.7 -79.6 ## 6 2020-07-25 11:06:00 43.7 -79.6 ## 7 2020-07-25 11:07:00 43.7 -79.6 ## 8 2020-07-25 11:08:00 43.7 -79.6 ## 9 2020-07-25 11:09:00 43.7 -79.6 ## 10 2020-07-25 11:10:00 43.7 -79.6 ## # ... with 122 more rows Note the last line where we used distinct() to streamline our dataset. This was necessary because we rounded down the seconds data to each minute, so we have sixty rows for each minute. distinct() allows us to take one row for each minute as our spatial data. the .keep_all = TRUE argument preserves the latitude and longitude rows. And we simply join gases to spatDat so each gas measurement now has an accompanying geographic coordinate. gasesSpat &lt;- spatDat %&gt;% left_join(gases) ## Joining, by = &quot;timestamp&quot; gasesSpat ## # A tibble: 263 x 7 ## timestamp latitude longitude sensor_name measurement_value ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-07-25 11:01:00 43.7 -79.6 co2 451 ## 2 2020-07-25 11:01:00 43.7 -79.6 o3 0 ## 3 2020-07-25 11:02:00 43.7 -79.6 co2 426 ## 4 2020-07-25 11:02:00 43.7 -79.6 o3 0 ## 5 2020-07-25 11:03:00 43.7 -79.6 co2 420 ## 6 2020-07-25 11:03:00 43.7 -79.6 o3 0 ## 7 2020-07-25 11:04:00 43.7 -79.6 co2 414 ## 8 2020-07-25 11:04:00 43.7 -79.6 o3 0 ## 9 2020-07-25 11:05:00 43.7 -79.6 co2 414 ## 10 2020-07-25 11:05:00 43.7 -79.6 o3 0 ## # ... with 253 more rows, and 2 more variables: measurement_type &lt;chr&gt;, ## # measurement_units &lt;chr&gt; 17.2.2.2 Visualizing gases data Since weve already created a map for the bike ride (airbeamMap), we can recycle it and overlay our newly created gasesSpat data. # AirBeam map made earlier, see above o3Map &lt;- ggmap(airbeamMap) + geom_point(data = subset(gasesSpat, sensor_name == &quot;o3&quot;), aes(x = longitude, y = latitude, colour = measurement_value)) o3Map Weve done the same sub-setting on the fly to only ploy O3. Lets annotate our plot again based on the information in the Experimental Details: o3MapAnnotated &lt;- o3Map + # annotations for 1st plane takeoff annotate(&quot;segment&quot;, x = -79.6, xend = -79.63254, y = 43.66460, yend = 43.66460, size=1) + annotate(&quot;label&quot;, x = -79.6, y = 43.66460, label = &quot;Plane takeoff \\n 11:17am&quot;, fill = &quot;white&quot;) + # annotations for 2nd plane takeoff annotate(&quot;segment&quot;, x = -79.65385, xend = -79.65385, y = 43.66, yend = 43.67089, size=1) + annotate(&quot;label&quot;, x = -79.65385, y = 43.66, label = &quot;Plane takeoff \\n 11:40am&quot;, fill = &quot;white&quot;) + # labels for plots labs(x = &quot;longitude&quot;, y = &quot;latitude&quot;, colour = &quot;ozone (ppm)&quot;, caption = &quot;data record on July 25th, 2020&quot;) o3MapAnnotated 17.2.3 Combining maps So we now have a map of PM10 and O3, lets place them side by side so we can see how different measurements changed along the bike trip: # chunk fig.height = 7 ggpubr::ggarrange(transectMapAnnotated, o3MapAnnotated, labels = c(&quot;A&quot;, &quot;B&quot;), nrow = 2, ncol = 1) Now theres a story being told. Some things to take away from this map is: PM10 higher indoors, largely unaffected outdoors regardless of local (urban, nature-ish, airport). O3 appears to have been affected by planes taking off, and maybe by something going on at the Bloordale park. Taking this to the next level, you could try and cross-reference the days weather (wind direction/speed) to see how that would affect it. Historical weather for Toronto can be found here. 17.3 Summary In these notes weve covered: How to import and tidy data from the AirBeam2 Sensor How to join data sets to get a more complete picture. Mapping spatial data using ggmap "]]

# Modelling: Linear Regression

Modelling is basically math used to describe some type of system, and they are a forte of R, a language tailor made for statistical computing... Every model has assumptions, limitations, and all around tricky bits to working. There is no shortage of modelling in a myriad of context, but in this chapter we'll discuss and break down the most common model you'll encounter, the *linear regression model*, in the most common context, the *linear calibration model*, using the most common funciton, `lm()`. 

You have probably encountered the linear regression model under the pseudonmy "trendlines", most likely geenrated by *Excel*'s "add trendline option" (as in CHM135). While the models we'll be constructiong with `lm()` work much the same *mathematically*, unlike *Excel*, R returns *alllll* of the model outputs. Correspondingly, it's easy to get lost between juggling R code, the seemingly endless model outputs, and keeping yourself grounded in the real systeyms you're attempting to model. 

To this end, this chapter is broken into the following parts: 

  - [Modelling theory] where we briefly touch opon *how* the model is calculated.
  - [Modelling in R] where we provide a boilerplate template for *how* to calculate models in R. 
  - [Understanding and Visualizing Models] where we explore our model results. 
  - [Calculating Concentration] where we use our model to calculate the concentrion.

## Modelling Theory

The *linear calibration model* realtes the response of an instrument to the value of the *measurand*. The measurand is simply what we're measuring, often the concentration of the analyte.  So we use the measurand, which we can control via preparation of samples as the *independent* variable, with the instrument output being the *dependent* variable (as instrument resposne varies with concentration). Once we collect sufficient data, we use *linear regression* to draw a straight line through out data:

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
library(tidyverse)
df <- data.frame("conc" = c(0, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8),
                 "abs" = c(0, 0.057, 0.119, 0.261, 0.353, 0.599, 0.730))

df <- df %>%
  nest() %>%
  mutate(fit = map(data, ~lm(abs ~ conc, data = .x)),
         tidied = map(fit, broom::tidy),
         glanced = map(fit, broom::glance),
         augmented = map(fit, broom::augment)
         ) %>%
  unnest(augmented)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}



ggplot(df, 
       aes( x = conc, y = abs)) +
  geom_point() +
  
  # for residuals
  geom_smooth(method = lm, se = FALSE) +
  
  # actual measurement
  geom_segment(aes(xend = conc, yend = .fitted)) +
  
  
  # y4 measurement
  geom_segment(aes( x = 0, y = 0.261 , xend = 0.20, yend = 0.261),
               alpha = 0.6, linetype = "dashed") +
  annotate("text", x = 0.025, y = 0.281, label = expression(y[4] )) +

  # y4 fitted
  geom_segment(aes( x = 0, y = 0.20391462 , xend = 0.20, yend = 0.20391462),
               alpha = 0.6, linetype = "dashed") +
    annotate("text", x = 0.025, y = 0.23, label = expression(widehat(y)[4])) +

  # residual annotation
  annotate("text", x = 0.25, y = 0.1, label = "This is a\n residual", hjust = 1) +
  geom_curve(aes( x = 0.24, y = 0.15, xend = 0.205, yend = 0.245),
              curvature = 0.5, arrow = arrow(length = unit(0.03, "npc"))) +

  # measured point annotation
  annotate("text", x = 0.5, y = 0.3, label = "This is the \n measurement \n of a standard", hjust = 0) +
  geom_curve(aes(x =0.5, y = 0.3, xend = 0.4, yend = 0.33),
              curvature = -0.7, arrow = arrow(length = unit(0.03, "npc"))) +

  # fitted line annotation
  annotate("text", x = 0.65, y = 0.45, label = "This is the \n calibration line") +
  geom_curve(aes(x = 0.65, y = 0.5, xend = 0.65, yend = 0.6),
              curvature = 0, arrow = arrow(length = unit(0.03, "npc"))) +
  # unknown inst. response annotation
  geom_point(aes(x = 0, y = 0.35), colour = "red", shape = "square") +
  geom_curve(aes(x =0, y = 0.35, xend = 0.359, yend = 0.35),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash", colour = "red") +
  annotate("text", x = 0.05, y = 0.5, label = "Response of\n unknown", hjust = 0.25) +
  geom_curve(aes(x = 0.05, y = 0.45, xend = 0.005, yend = 0.36), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # unknown calculated conc
  geom_curve(aes(x =0.359, y = 0.35, xend = 0.359, yend = 0),
            curvature = 0, 
            arrow = arrow(length = unit(0.03, "npc")), 
            linetype = "longdash",
            colour = "red") + 
  annotate("text", x = 0.4, y = 0.1, label = "Concentration of\n unknown", hjust = 0) +
  geom_curve(aes(x = 0.4, y = 0.1, xend = 0.365, yend = 0.01), 
             curvature = -0.35, 
             arrow = arrow(length = unit(0.03, "npc"))) +
  
  # so residual lines touch y axis
  scale_x_continuous(expand = c(0,0)) +
  coord_cartesian(clip = "off") +
  labs(x = "concentration",
       y = "instrument response") +
  theme_classic() +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank())

```

There's no reason you instrument response must be linear, although for convention sakes we often arrange our experiment so that we land in the 'linear range'. For details on non-linear modelling see [Non-Linear Regression]

## Modelling in R

For our example dataset we'll import a dataset consisting of four analytical standards of sodium plus a calibration blank all run in triplicate. The standards were measured via flame atomic emission spectroscopy (FAES). Let's import the FAES calibration results we saw in [Transform: dplyr and data manipulation]. As we've already seen, our data is composed of four standards and a blank analyzed in triplicate. Since we're focusing on modelling, *we'll treat the blank as a standard in our model fitting*, we'll justify this decision later on.

So let's import our dataset:

```{r, message = FALSE}

# Importing using tips from Import chapter

FAES <- read_csv(file = "data/FAESdata.csv") %>%
  pivot_longer(cols = -std_Na_conc,
               names_to = "replicate", 
               names_prefix = "reading_",
               values_to = "signal") %>%
  separate(col = std_Na_conc,
           into = c("type", "conc_Na", "units"),
           sep = " ",
           convert = TRUE) %>%
  mutate(type = "standard")

DT::datatable(FAES)
```

And let's quickly plot our data. You should always visualize your data before modelling, especially for linear calibration modelling. Unlike *statistical* modelling, you've spent a lot of time and effort to make sure your data adheres to a straight line. 

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point()
```

Note model is a general term, in this situation we'll be calculating a **calibration curve**. All calibration curves are models, but not all models are calibration curves.

## Base R Linear Model

R's base `lm()` function for linear regression is excellent, but it's outputs have some messy quirks. It's easier to show that, so let's calculate the linear relationship between the `signal` as a function of `conc_Na`:

```{r}
lm_fit <- lm(signal ~ conc_Na, data = FAES)
lm_fit
```

Reading the code above (recall that we're reading it from *right to left* because it's base R):

1.  We're taking the FAES data we created earlier; `data = FAES`
2.  We're comparing `signal` (the dependent, y, variable) to `conc_Na` (the independent, x, variable) via the tilde `~`. The way to read this is: *"Signal depends on concentration"*.
3.  We're comparing these two variables using the `lm()` function for generalized linear models.
4.  All of the model outputs are stored in the `lm_fit` variable.

As we can see, the model outputs are pretty brief and not much more than *Excel*'s outputs. We can use `summary()` to extract more information to better understand our model:

```{r}
summary(lm_fit)
```

Now we have a lot more information from our model (don't worry about what everything means, it's discussed further in Section 3. For now, understand that it's a hot mess.

## Cleaning up model ouputs

`summary()` provides a decent overview of our model's performance, but the outputs are difficult to work with. Let's turn to the `broom()` package to clean up our model outputs.

```{r FAES-calCurves}
library(broom)

calCurve <- FAES %>%
  group_by(type) %>%
  nest() %>%
  mutate(fit = map(data, ~lm(signal ~ conc_Na, data = .x)),
         tidied = map(fit, tidy),
         glanced = map(fit, glance)
         )
calCurve
```

Things look a bit more complicated than our earlier example, so let's break down our code line by line:

1.  We're taking the `FAES` dataset that we created earlier.
2.  `group_by(type)` groups all rows by `type`, in this situation we have only one type: `standard`.
3.  `nest()` collapses everything other than the `type` column into smaller dataframes. In this situation, all other information is stored as a `tibble` under the `data` column; this is the data used to calculate the linear model.
4.  Withing the `mutate` function, we've created three columns: `fit`, `tidied` and `glanced`.

And it's the the `fit`, `tidied` and `glanced` that contains out cleaned up model outputs. `fit` contains the raw output from the linear regression model for `signal` as a function of `conc_Na` using the `lm()` function. The output is in the form of a list, similar to what `summary()` gave us above. Again, this is exceptionally messy, hence why we used the `tidy()`, and `glance()` function from the `broom` package . `map()` just means we're applying the function `tidy()` to the individual output list created by `lm()` and stored in the `fit` column. Note that the `tidy()` and `glanced()` outputs are tibbles. So we now have a tibble containing specific model output values (i.e. `(Intercept)`), lists (i.e. `fit`), and tibbles (`tidied`). This is known as \*\*nested data\*. We're no longer in Kansas anymore...

Anyways, let's take a look at our model results. The `glanced` tibble contains "...a concise one-row summary of the model. This typically contains values such as R\^2, adjusted R\^2, and residual standard error that are computed once for the entire mode"[^linear-regression-1] Because the data is nested, we'll need to use `unnest()` to flatten it back out into regular columns:

[^linear-regression-1]: From the *broom* package vignette.

```{r FAES-lm-Glanced}
calCurve %>% 
  unnest(glanced)
```

What you see here is a bit more than what you'd get from *Excel*'s 'line-of-best fit' output. See the section on *Modelling* for a better breakdown of what everything means. But for now, we can see that our `r.squared` of each calibration curve is pretty good, and the `p.value` indicates each model is significant. the `adj.r.squared` is the same as `r.squared` in this situation. This is because `r.squared` will always increase if we add more exploratory variables to our model; the `adj.r.squared` accounts for the number of exploratory variables used in the model. However, in our case we only have one exploratory variable, hence they're the same.

But what about the slope and the intercept? After all, that's what we need to calculate the concentration in our unknowns. Let's take a look at `tidied` from the `tidy()` function "...which constructs a tibble that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression..."[^linear-regression-2]

[^linear-regression-2]: From the *broom* package vignette.

```{r FAES-lm-tidied}
# storing because we'll use it later on. 

tidied <- calCurve %>% 
  unnest(tidied)

tidied

```

Again, a lot more to unpack compared to *Excel*. That's because the `lm()` function in R calculates a generalized linear model. `lm()` performs a linear regression model, which we normally think of as an equation of the form $y= mx+b$. But, regression models can be expanded to account for multiple variables (hence *multiple linear regression*) of the form

$$y = \beta _{0} + \beta _{1} x_{1} + \beta _{2} x_{2} ... \beta _{p} x_{p}$$]

where,

-   $y$ = dependent variable
-   $x$ = exploratory variable; there's no limit how many you can input
-   $\Beta _{0}$ = y-intercept (constant term)
-   $\Beta _{p}$ = slope coefficient for each explanatory variable

In our situation, we only have one input variable for our model (`conc`), so the above formula collapses down to $y = \beta _{0} + \beta _{1} x_{1}$. So looking at our results above, each row corresponds to a model parameter. For each modelling parameter, we're provided an estimate of it's numerical value (`estimate`, the values we'll use to calculate concentration). The other parameters are useful to understand but not necessary at this point (again, check out the *Modelling* section).

And we can extract these values to use in subsequent calculations:

```{r}
intercept <- as.numeric(tidied[1,5])
slope <- as.numeric(tidied[2,5])

paste("The equation of our calibration curve is: y = ", slope, "x + ", intercept, sep="")

```

### Why we approach modelling this way

You may be wondering why we've seemingly overcomplicated a simple enough procedure. Fair enough, we've showcased an analysis with a simple data set. However, as you progress in your studies you'll be quantifying *many* compounds, often at the same time in the same instrument runs. If you organize your data in a tidy format, you can plot calibration curves for *all* of your compounds with the same block of code. Essentially you use `group_by()` to group your data by compound/element. Subsequently, the same code is expandable from 1 compound to as many as you can ever hope to quantify in one shot. So for upper year labs where you're analyzing tens of compounds (*cough* CHM410 Dust Lab) you can generate calibration curves for all your compounds at once.

## Visualizing model

At the top of the chapter we plotted out standards to visualize a linear trend. Visualizations is an essential component when calculating calibration curves, and indeed our standards apepared to follow a linear trend, which was corroborated by the model we calculated above. However, for publications/reports you'll need to create a plot with both your standards *and* model with the displayed equation, so below is a bit of stock code you can use as a starting point to create these plots. Note that it requires the `ggpmisc` package to display the equation:

```{r}

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 



```

## Further reading

The theory and use of these models are explored in greater details in Section 3. Please read up on it for an understanding of the model outputs and how to use them in your analysis. As well, see the [section on modelling](https://r4ds.had.co.nz/model-intro.html) in *R for Data Science*.

<https://www.newyorker.com/magazine/2021/06/21/when-graphs-are-a-matter-of-life-and-death>

**Note this needs to be cut up and reformated to elaborate more on the theory behind linear regression. - DH**

In this chapter we'll discuss the *why* and *what* of the linear regression model we calculated in the [Modelling]. Understanding models is more important than creating models.

```{r, message = FALSE}

# same code as Modelling Chapter

FAES <- read_csv(file = "data/FAESdata.csv") %>%
  pivot_longer(cols = -std_Na_conc,
               names_to = "replicate", 
               names_prefix = "reading_",
               values_to = "signal") %>%
  separate(col = std_Na_conc,
           into = c("type", "conc_Na", "units"),
           sep = " ",
           convert = TRUE) %>%
  mutate(type = "standard")

ggplot(data = FAES,
       aes(x = conc_Na, 
           y = signal)) +
  geom_point() +
  geom_smooth(method = 'lm', se=F) +
  ggpmisc::stat_poly_eq(formula = y ~ x, # formula uses aesthetic names
                        rr.digits = 4, # reported digits of r-squared
                        aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                        parse = TRUE, size = 3) 
```

<!-- Check out tidymodel book for tips/details on modelling w/ r -->
